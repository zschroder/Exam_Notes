---
title: "Quantitative Geography Notes"
author: "Zoe Schroder"
date: "9/19/2019"
output: html_document
---

Load libraries: 
```{r}
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(ISwR))
suppressMessages(library(scatterplot3d))
suppressMessages(library(GGally))
suppressMessages(library(foreign))
suppressMessages(library(msm))
suppressMessages(library(interplot))
suppressMessages(library(sm))
```

# Variance:

Variance is the measures of how far a set of numbers are spread out from their average value.
The formula is: 
$$
\hbox{var}(x) = \frac{(x_1 - \bar x)^2 + (x_2 - \bar x)^2 + \cdots + (x_n - \bar x)^2}{n-1}
$$

# Mean: 

The mean is the average calue of a set of numbers defined as: 
$$
\bar x = \frac{x_1 + x_2 + \cdots + x_n}{n}
$$
The sample mean is a measure of the central tendency of a set of values. Typically there are more values near the mean. This is similar with the median, but the median is more resistent to outliers. The mediann is a `resistant statistic` because it is not greatly influenced by only a few values. 

# Quantiles: 

Quantiles cut a set of ordered data into equal-sized data bins. The ordering comes from rearranging the data from lowest to highest. The first, or lower, quartile corresponding to the .25 quantile (25th percentile), indicates that 25% of the data have a value less than this value. The third, or upper, quartile corresponding to the .75 quantile (75th percentile), indicates that 75% of the data have a smaller value than this value.

The difference between the first and third quartile values is called the interquartile range (IQR). Fifty percent of all values lie within the IQR.

```{r}
x <- rnorm(n = 10000, 
           mean = 10, 
           sd = 100)
quantile(x, 
         probs = c(0.05, .25, .5, .75, .95))

IQR(x)
```

Find location of min and max values in a data frame: 
```{r}
Volume = c(20.233, 19.659, 18.597, 18.948, 17.820, 16.736, 16.648, 17.068, 15.916, 14.455, 14.569)
Year = 2002:2012
Ice.df = data.frame(Year, Volume)

which.min(Ice.df$Volume)
which.max(Ice.df$Volume)
```

# Dplyr: 

Verb        | Description
-----------:|:-----------
`select()`    | selects columns; pick variables by their names
`filter()`    | filters rows; pick observations by their values
Try this on your thesis stuff lo
`arrange()`   | re-orders the rows
`mutate()`    | creates new columns; create new variables with functions of existing variables
`summarize()` | summarizes values; collapse many values down to a single summary
`group_by()`  | allows operations to be grouped

For other types of combinations, we need to use Boolean operators: `&` is "and", `|` is "or", and `!` is "not".

```{r}
filter(airquality, Month == 5 | Month == 9)

df = airquality %>%
  filter((Month == 8 | Month == 9) & Wind < 5) %>%
  select(Month, Day, Ozone, Wind, Temp)
```

# Arrange: 

Arrange the data by a specific column
```{r}
# Ascending: 
airquality %>%
  arrange((Solar.R))

# Descending
airquality %>%
  arrange(desc(Solar.R))
#### OR ####
airquality %>%
  arrange(-Solar.R)
```

# Melt: 

Sometimes you need to reshape the data. You use the `melt()` function. The ID variable is a vector of a non-measured quantity. 

```{r}
L = "http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt"
FLp = read.table(L, na.string = "-9.900", 
                 header = TRUE)

FLpL = melt(FLp, id.vars = "Year")
head(FLpL)
ggplot(FLpL, aes(x = variable, y = value)) + 
  geom_boxplot() +
  xlab("Month") + 
  ylab("Precipitation (in)")
```

# Correlation

A scatter plot is a graphical representation of the correlation. The corrlation describes the linear relationship between two variables witha  single number. The correlation computed on a set of data values a statistic. It describes whether larger- and smaller-than-average values of one variable are related to larger- or smaller-than-average values of the other variable.  If larger than average values of one variable tend to be associated with larger than average values of the other variable then the correlation is positive. These values range betweenn -1 and +1 

Pearson correlation
$$
r = \frac{1}{n-1} \sum \left(\frac{x_i - \bar x}{s_x}\right)\left(\frac{y_i - \bar y}{s_y}\right)
$$
where $n$ is the sample size, $\bar x$ and $\bar y$ are the average of the x and y variables, respectively and $s_x$ and $s_y$ are the standard deviation of the x and y variables.

```{r}
cor(FLp$Jan, FLp$Feb)
```


geom_smooth(method = lm, se = FALSE) + #linear trend line
geom_smooth(se = FALSE, color = "red") #nonlinear trend line

Spearman Rank Correlation: 

Spearman rank correlation is a nonparametric measure of the rank correlation (statistical dependence between the rankings of two variables.) This correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples (Spearman's rho is limited to the value of the rank). High correlation between two variables indicates a similar rank. 

```{r}
cor(FLp$Jan, FLp$Feb, method = "spearman")
```

# One-sample test: 

A one-sample test of whether the mean of a population has a value specified in a null hypothesis. This is a test of location: are the data consistent with being centered on a specific value?

*Example: Given a random sample of FSU student heights (to make things simple, let's assume we are considering only the guys) in feet, test the hypothesis that the mean height of the population of guys at FSU is 183 cm (~ 6 feet).*

Start by plotting the data together with the hypothesis.
```{r}
ht = c(177, 180, 179, 174, 192, 186, 165, 183)
ht.df = data.frame(Student = 1:length(ht), 
                   Height = ht)

library(ggplot2)
ggplot(ht.df, aes(x = "", y = Height)) + 
  geom_boxplot() +
  ylab("Height (cm)") +
  geom_hline(aes(yintercept = 183), color = "red") +
  theme_minimal()
mean(ht)
```

We formally test the hypothesis that the mean height in the population is 183 cm with the `t.test()` function. The first argument is the data (as a vector!) and the second argument is the suggested population mean (`mu =`). Note that the mean is a statistic so we  
```{r}
t.test(ht, mu = 183)

#The output begins with the $t$-value (-1.2239).  The $t$-value is a statistic computed as

(mean(ht) - 183)/(sd(ht)/sqrt(length(ht)))

```

The 'degrees of freedom' (dof, df) is a term that indicates the number of values in the calculation of a statistic with a known value that are 'free' to vary. Suppose you know the mean of a set of numbers (say it's 24) and how many numbers were used to calculate the mean (say there are five). What values could the five numbers have? Four of them could be any value, but the fifth one is constrained to make the mean work out to be 24. This is true for any mean. Thus the mean is a statistic with degrees of freedom equal to n - 1.

# p-values

A $p$-value is an estimate of the probability that your data, or data more extreme than observed, could have occurred by chance IF THE HULL HYPOTHESIS IS TRUE. Said slightly differently this is how unusual your data are ASSUMING THE NULL HYPOTHESIS IS TRUE. A small $p$-value tells you that your data is unusual with respect to this particular hypothesis.

The $p$-value summarizes the evidence in support of the null hypothesis. The smaller the $p$-value, the less evidence there is in support of the null hypothesis. But, the interpretation of the $p$-value is typically stated as evidence AGAINST the null hypothesis:

$p$-value        | Statement of evidence against the null
---------------- | ---------------------
less than  .01   | convincing
.01 - .05        | moderate 
.05 - .15        | suggestive, but inconclusive
greater than .15 | no

The $p$-value is the area under the tails of the $t$ distribution. The distribution describes how the $t$ statistic varies. The $t$ statistic is given by
$$
t = \frac{\bar x - \mu_o}{s/\sqrt{n}}
$$
where $s$ is the standard deviation of the sample values and $n$ is the sample size. The denominator is the standard error (standard deviation divided by square root of sample size) of the mean.

The $p$-value comes from the `pt()` function, which determines the area under the $t$ distribution curve to the left of a particular value. The curve is obtained using the `dt()` function (density function).  

The area under the curve to the left of -1.2239 is 0.13. So 13% of the area lies to the left of the first red line. The distribution is symmetric so 13% of the area lies to the right of the second red line. With a two-sided test you add these two fractions (or multiply by 2) to get the $p$-value.
```{r}
curve(dt(x, 7), from = -3, to = 3, lwd = 2)
abline(v = -1.2239, col = 'red')
abline(v = 1.2239, col = 'red')

#To check the p-value of a t statistic: t-value = -1.2239
pt(-1.2239, df = 7)
#0.13
pt(-1.2239, 7) * 2
#0.26
```

# two-sample test 

With two samples of data the hypothesis is that they both come from distributions having the same mean.

For example, data are from two groups which we assume are sampled from the normal distributions. We test the null hypothesis that the two samples have the same population mean by computing the t statistic. 

In this case, the $t$ statistic is the difference in sample means divided by the standard error of the difference in means (SEDM).

There are two ways to calculate SEDM. (1) Assume equal variance: use the pooled standard deviation (s). Under the null hypothesis, the t statistic will follow a t distribution with n1 + n2 - 2 degrees of freedom (df). (2) Don't assume equal variances (this is the default assumption). Under the null hypothesis, the $t$ statistic approximates a $t$ distribution. In this case it is called the Welch procedure. In this case the df is not an integer.

Usually the two methods give similar results unless group sizes and variances differ widely among the two samples.

Note that in this case the df is a whole number, namely 13 + 9 - 2 = 20.  The $p$-value has dropped slightly and the confidence interval is a little narrower, but the changes are slight.

Start with side-by-side boxplots.
```{r}
ggplot(tlc, aes(x = factor(sex), y = tlc)) +
  geom_boxplot() +
  xlab("Sex (1: female)") +
  ylab("Total lung capacity (liters)") +
  theme_minimal()
```

You see a difference in lung expenditure between the two groups. Is this difference statistically significant?

With this data format (long) we use model syntax for specifying the test. Model syntax is response variable ~ explanatory variable. The `~` is the tilde on your keyboard.

Thus `tlc` is your response variable and `sex` is your explanatory variable.
```{r}
t.test(tlc ~ sex, data = tlc)

# t = -3.6693, df = 29.703, p-value = 0.0009493
# alternative hypothesis: true difference in means is not equal to 0
# 95 percent confidence interval:
# -2.7691965 -0.7883035
# sample estimates:
# mean in group 1 mean in group 2 
#       5.198125        6.976875 
```

The output is a bit different this time. The small $p$-value allows you to conclude that there is convincing evidence against the null hypothesis of no difference. Therefore, they are significantly different. The wider the uncertainty (confidence) interval the more uncertain we are about the population difference. It does not contain the value 0, so we can conclude there is a statistical difference.

# F Test

To test the assumption of equal variances you use `var.test()` function.
```{r}
var.test(energy$expend ~ energy$stature)
```

The variance test is based on the assumption that the groups are independent.  Don't apply this test to paired data. if the variances are equal, the ratio of the variances will be 1.

# Wilcoxon Test aka Mann-Whitney U test

Best to use when the data is not normally distributed. 

We can avoid the distributional assumption by using a non-parametric test. The non-parametric alternative is the Wilcoxon test. Also known as the Mann-Whitney U test.

The test statistic W is the sum of the ranks in the first group minus the sum of the ranks in the second. It is invoked with the `wilcox.test()` function. Try it on the energy data.
```{r}
wilcox.test(energy$expend ~ energy$stature)
```

# Paired observations

Paired tests are used when there are two measurements on the same experimental unit. The theory is based on taking the differences in paired values thus reducing the problem to that of a one-sample test. To invoke the paired $t$ test, use the argument `paired = TRUE`.

Example: Shoe material. A manufacturer makes two different shoe materials (A and B).  A sample of 10 kids try *both* materials and recorded are the wear times (months) for each material. Compare whether or not there is any difference in shoe material wear times.

Only used when it is the same people in the test. The same 10 kids are in both A and B
```{r}
matA = c(14, 8.8, 11.2, 14.2, 11.8, 6.4, 9.8, 11.3, 9.3, 13.6)
matB = c(13.2, 8.2, 10.9, 14.3, 10.7, 6.6, 9.5, 10.8, 8.8, 13.3)
```

```{r}
t.test(matA, matB, paired = TRUE)
```

# Bayesian data analysis

The Bayesian approach to inference is now everywhere. Solving problems like predicting elections and directing self-driving cars. Problems where large uncertainty needs to be quantified and where efficient integration of many sources of information is required.

The problem is that the approach is still not taught in statistics. R is made for Bayesian data analysis. But if you google "Bayesian" you get philosophy. Subjective vs objective, frequentism vs Bayesianism, $p$-values vs subjective probabilities.

`In brief:`

The building blocks of the Bayesian approach are distributions. We've already talked about the Gaussian (normal) distribution as a model for the mean value over a set of measurements. The Gaussian distribution has two parameters $\mu$ and $\sigma$ so we call a Gaussian model a family of Gaussian distributions.

A parametric model is a family of distributions that can be described using a finite (usually small) number of parameters. Bayesian models are made up of parametric models.

Bayesian models are generative. If we know the parameters then we can generate data: Parameters ($\mu$, $\theta$, $\sigma$) --> generative model --> data (5, 3, 4, 0, 1, ...): Monte Carlo simulation. If we know the data then we can estimate the parameters: data (5, 3, 4, 0, 1, ...) --> generative model --> Parameters ($\mu$, $\theta$, $\sigma$): Bayesian data analysis

What Bayesian data analysis is not

* A category of models
* Subjective
* Not the most efficient method of fitting a model
* Anything new

Why use Bayesian data analysis? *(Lesson 10 -- Quant Geo)*

We can build more flexible models. We can use models to answer specific questions in a natural way. 

Bayes Theorem is used to determine a posterior distribution on the unknown number of fish in a lake. It was not determined analytically but rather by repeated sampling. The distribution itself was a vector of values representing possible numbers of fish in the lake.

We can build models to answer natural questions. We don't need to express our questions in terms of a null hypothesis. 

We also have greater flexibility in building models. 

Bayes Theorem. prior x likelihood / posterior

What Bayesian data analysis is not

A category of models (e.g., regression models, decision trees). Rather it is a way of thinking about and constructing models. You can do regression or machine learning with a Bayesian framework (e.g., Bayesian decision analysis).

Not subjective. All statistics require you make assumptions. Results have to be interpreted in light of those assumptions.

Nothing new. Thomas Bayes, Simon LaPlace, Ronald Fischer (first to use the term Bayesian statistics).

# Linear Regression 

Recall that a $t$ test is used to examine whether there is evidence that population means from two distinct groups are statistically different. Linear regression extends the test to an arbitrary number of groups.

The data frame contains total lung capacity (`tlc`) in liters for 32 patients waiting for a heart or lung transplant. It also contains the patients sex (`sex`) coded as 1 for female and 2 for male.We test the null hypothesis of no difference in lung capacity between males and females. 
```{r}
t.test(tlc ~ sex, 
       data = tlc, 
       var.equal = TRUE)
```

The $t$-statistic, computed as the difference in means divided by the standard error of the difference, is -3.6693. With 30 degrees of freedom (sample size = 32), the $p$-value is less than .01 so we reject the null hypothesis of no difference in lung capacities. 

The same test is done with a linear regression model. The function is `lm()`. The syntax is identical to that of the `t.test()` function, except the assumption of equal variance is the default.
```{r}
summary(lm(tlc ~ sex,
           data = tlc))
```
We see that the $p$-value (last line) is identical to that from the `t.test()` and state that we reject the null hypothesis that lung capacity is not significantly related to `sex`. In the table of coefficients we see the value of `1.7788` next to `sex` in the column labeled `Estimate`. This is the difference in average lung capacity grouped by sex. Linear regression generalizes this type of comparison across any number of groups (explicit or not).

The mean value is a model for these data. Some points are above the line and others below. No individual value fits the model precisely (no points are on the line), but the model represents the 'best' guess at what a new value might be.

The closer the points are to the line, the more precise the model is.  Closeness is defined as the distance along the y axis between the point and the line. Distances above the line indicate values that are larger than the mean, so `y - mean(y)` is positive for these values. Distances below the line indicate values that are smaller than the mean, so `y - mean(y)` is negative for these values.
```{r}
y = c(2.9, -2.1, -0.5, 2.9, 4.2)
x = 1:5
df = data.frame(x, y)

p = ggplot(df, aes(x, y)) +
      geom_point(size = 4) +
      geom_hline(yintercept = mean(y)) +
                 scale_y_continuous(limits = c(-6, 6))

p = p + geom_segment(aes(y = mean(y), yend = y, 
                         x = x, xend = x))
p
```
Instead we add the squared distances.
```{r}
sum((y - mean(y))^2)
```

The sum of the squared distances is a measure of how well the model fits the data. The smaller the sum, the better the fit. It is called the sum of the squared errors (SSE). The individual distances are called "errors." Statisticians call them "residuals." The SSE equal to 0 indicates that all points fall exactly on the line.

Keep in mind: A residual is the observed value minus the modeled value.

We see that the regression line (conditional mean) provides a more precise model since the SSE is smaller. Rule: When choosing a regression model from a set of competing models we choose the model that minimizes the SSE.

Recall that a line is uniquely determined by its slope and the value at which it intersects the vertical axis (y-intercept). Mathematically the slope and y-intercept are determined by
$$
\hat \beta_1 = \frac{\sum (x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2}\\
\hat \beta_0 = \bar y - \hat \beta_1 \bar x
$$
`beta1` ($\beta_1$) and `beta0` ($\beta_0$) are called regression model coefficients. $\beta_1$ is the slope coefficient and $\beta_0$ is the y-intercept coefficient.


The notation is `lm(y ~ x)`. The `~` (tilde) in this notation is read "is modeled by" or "is conditional on". So the model formula `y ~ x` is read "y is modeled by x". If we use `lm(y ~ x)` then we say "y is modeled by x" in a linear way.

```{r}
lm(y ~ x)
```

Example: Age is in years and the heart rate is in beats per minute. Heart Rate is response variable. Age is explanatory variable 
```{r}
Age = c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
HR = c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
heart.df = data.frame(HR = HR, Age = Age)

model <- lm(HR ~ Age, data = heart.df)

# Make predictions for these ages: 
predict(model, newdata = data.frame(Age = 50))
predict(model, newdata = data.frame(Age = seq(40, 60, 10)))
```

We interpret the model as follows: On average a person's maximum HR *decreases* by .8 bpm every year. Or more easily understood as a decrease of 8 bpm every 10 years.

```{r}
summary(model)
```

estimate / std error == t value 
null hypothesis: age does not effect heart rate 
reject the null

The `summary()` method returns quite a bit of output. The first is the code we used to create the model. The second are the summary statistics for the model residuals: observed value minus the modeled value. The most important output is the table of coefficients. The table shows the slope and intercept coefficients in the column labeled `Estimate`. The adjacent column labeled `Std. Error` lists the standard errors on these coefficients. The standard error (or margin of error) is a measure of the uncertainty surrounding the coefficient estimate. For the slope coefficient it is computed as
$$
s_{\beta_1} = \sqrt{ \frac{\frac{1}{n - 2}\sum_{i=1}^n \varepsilon_i^{\,2}} {\sum_{i=1}^n (x_i -\bar{x})^2} },
$$
where
$$
\varepsilon_i  = y_i - \beta_0 - \beta_1 x_i
$$

The best estimate for the (population) slope is -.798 with a margin of error of +/- .07. The uncertainty about the model coefficient is used to test hypotheses and compute confidence intervals. 

Interest typically centers on the null hypothesis that the slope = 0. A zero slope implies the line is horizontal and thus, in this case, that maximum heart rate is independent of how old you are.

The $t$-value ($t$-statistic) for the zero-slope hypothesis is the slope divided by its standard error. The $t$-value has a $t$ distribution with $n-2$ degrees of freedom if the slope is zero.

Here the $t$-value is -11.4 which gives a $p$-value (Pr(>|t|)) of .0000000385 (3.85e-08). It is written this way, because the $p$-value is the probability of observing a more extreme $t$-value (positive or negative) assuming the null hypothesis is true (slope is zero).

The symbols to the right indicate a category of confidence in the inference. The line below the table shows the definitions which we can interpret using our definition. Three asterisks: overwhelming, Two asterisks: convincing, One asterisk: moderate, Point: suggestive but inconclusive.

So we have overwhelming evidence that on average maximum heart rate depends on age given these data.

The next line of output from the `summary()` function gives the residual standard error. This tells us how close (on average) the observations are from the regression line. The degrees of freedom are again $n-2$.
```{r}
sqrt(sum(resid(model)^2)/13)
```

Note the `resid()` [or `residual()`] outputs the residuals from the model object.
```{r}
resid(model)
```

The next line of output gives the multiple R-squared value. Also called the coefficient of determination. And the adjusted R-squared value. The multiple R-squared is equal to the square of the Pearson correlation coefficient. 

The multiple R-squared = 1 - SSE/SSY, where SSE is the sum of the squared residuals (residual sum of squares--RSS) and SSY is the total variation about a constant mean response.

It is useful to see how these are computed. The SSE is the sum of the squared residuals. This is computed with the `deviance()` function. Smaller deviance means a better model. 
```{r}
SSE = sum(resid(model)^2)
SSE
deviance(model)
```

To compute the multiple R-squared we need SSY. SSY is the deviance from the simpler constant mean model. This simpler model is estimated using the `lm()` function as
```{r}
model0 = lm(HR ~ 1, data = heart.df)
SSY = deviance(model0)
1 - SSE/SSY
```

One minus the ratio of the explained variation to the total variation.

SSE is less than SSY so SSE/SSY will be less that 1. If SSE is much less than SSY then, SSE/SSY is close to zero so R squared is close to 1.

The R-squared multiplied by 100% is the variance of the response variable explained (statistically) by the explanatory variable.

The adjusted R-squared value is a reduction from the R-squared value. The amount of reduction depends on how many variables are in the model.
$$
1 - \frac{n - 1}{n - p} (1 - R^2 )
$$

where $n$ is the sample size and $p$ is the number of coefficients.

The adjusted R-squared is always smaller than the multiple R-squared, can decrease as new explanatory variables are added, and can even be negative for really poorly fitting models. It is important in the context of multiple regression.

The final line of output is the F-value, degrees of freedom, and associated $p$-value.
$$
F_\hbox{statistic} = \frac{(SSY - SSE)/(p - 1)}{SSE/(n - p)}
$$

Under the null hypothesis that the regression is no better than the unconditional mean as a model for the data, the $F$-statistic comes from an $F$ distribution with ($p-1$) and $n$ degrees of freedom.

```{r}
pf(130, 1, 13, lower.tail = FALSE)
```

The $p$-value is very small so we reject the null hypothesis that the mean model (`lm(HR ~ 1, data = heart.df)`) is better than the linear regression model.

To get the uncertainty intervals on the predicted values:  Use the `level =` and `interval = "confidence"` arguments.
```{r}
predict(model, newdata = data.frame(Age = c(50, 60)), 
        level = .95, interval = "confidence")
ggplot(heart.df, aes(x = Age, y = HR)) +
  geom_point() +
  geom_smooth(method = lm)
```

The lower (`lwr`) and upper (`upr`) bounds represent the 95% uncertainty interval about the location of the line for the particular value of the explanatory variable `Age`.

We state that the best prediction for average maximum heart rate for a set of random 50-yr olds is 170 bpm with a 95% uncertainty interval between 167 and 173 bpm. Thus if we repeat the sampling 100 times and make the same prediction, our CI on the prediction will cover the true predicted value 95 times.

With `interval = "prediction"` we get a 95% *prediction* interval. That interval is wider than the confidence interval as it represents two sources of uncertainty. 
The uncertainty associated with the mean value GIVEN the person's age AND the uncertainty associated with a particular maximum heart rate GIVEN the conditional mean.  
```{r}
predict(model, data.frame(Age = c(50, 60)), 
        level = .95, interval = "prediction")
```

# Model Adequacy

By definition the regression line is the best fit line through the data. However the best fit line may not be an adequate model for your data. 

Model adequacy is about the quality of the inferences you can make with the model. It says nothing about the strength of the relationship between the response and explanatory variable. Checks on model adequacy alert you to where/how the model can be improved (e.g., transform the data, use a nonlinear term, etc.).

What do I mean by an inference from the model? 

When we infer something about nature or human behavior with our linear regression model we are making four assumptions. All four assumptions imply that the residuals should 'look' a certain way, or equivalently that the distribution of Y (response variable) conditional on X (explanatory variable) should look a certain way.

* Linearity: Average values of Y in ordered intervals of X should be a straight-line function of X. Each interval creates a 'sub-population' of Y values. 
* Constant variance: Sub-populations of Y should all have the same standard deviation.
* Normality: Values from each sub-population can be described by a normal distribution.
* Independence: Each observation is independent from the other observations.

To the extent these assumptions are valid, the model is said to be *adequate* in representing our data. A model can be statistically significant, but not adequate.

The first three assumptions are best examined using graphs. Note: We never prove an assumption is valid. We check to see if there is evidence to question its validity.

Consider the `cars` data frame. The data contains two columns `speed` and `dist` giving the speed (mph) and the corresponding distance (ft) needed to stop. The data were recorded in the 1920s.
```{r}
cor(cars$speed, cars$dist)
```

The correlation tells us there is a strong relationship between the two variables. The faster the car is moving, the more distance it needs to come to a stop.

We are interested in the breaking distance given the forward speed so `dist` is our response variable. Next we make a scatter plot.
```{r}
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Speed (mph)") + ylab("Break Distance (ft)")
```

The scatter of points shows an apparent linear relationship between the breaking distance and the forward speed of the car for this sample of data.

Fit a regression model to the data. Since distance is our response variable, we state that we regress stopping distance onto forward speed.
```{r}
model1 = lm(dist ~ speed, data = cars)
summary(model1)
```

ALWAYS Y ON TO X: Observed - fitted = residual

The model indicates a statistically significant relationship ($p$-value is less than .01) between breaking distance and speed. In fact, 65% of the variation in average breaking distance is associated with differences in speed.  

The statistically significant effect of speed on breaking distance suggests it is unlikely in the population of cars that there is no relationship between speed and breaking distance.

The model looks pretty good, so we write up our results and we are done. Not so fast, let's take a closer look.

There tends to be more values of breaking distance below the line than above the line over the range of speeds between 10 & 20 mph. Also, the spread of the residuals appears to get larger as speed increases. There is more variation in the response for larger values of speed.

*Assumption of Linearity and Constant Variance: *
One way to see this is to divide the explanatory variable into non-overlapping intervals and display the set of corresponding response values as a box plot. The variable is divided using the `cut()` function. Here we cut the `speed` into 5 intervals.
```{r}
cut(cars$speed, breaks = 5)
cars$speed[1]
```

The first value of `speed` is 4 mph and it falls in the interval (3.98, 8.2]. Greater than 3.98 and less than or equal to 8.2. And so on.

We can use the output of the `cut()` function as an argument in the `aes()` function to draw side by side box plots.
```{r}
ggplot(cars, aes(x = cut(speed, breaks = 5), y = dist)) + 
  geom_boxplot() +
  xlab("Speed (mph)") + 
  ylab("Break Distance (ft)")
```

In effect we are creating sub-populations. The population of response values within a given interval of the explanatory variable. The number of intervals is the number of breaks specified by the `cut()` function.

Here we see `evidence against the assumption of linearity`. Further we see that the box size is increasing. That is the IQR range of breaking distance is larger for faster moving cars. This creates `doubt on the assumption of constant variance`.`

*Assumption of Normality: *
What about the assumption of normality? Although the normality assumption about the residuals is that the *conditional* distribution of the residuals at each $x_i$ is adequately described by a normal distribution, with sample data there usually are not enough observations at each value of $X$ to check.

In practice it's common to examine the residuals all together. The residuals are obtained by using the `resid()` (extractor) function.
```{r}
res = resid(model1)
res
```

There are other extractor functions (like `coef()`) that output information from the model as vectors or matrices.

The `fortify()` function from the **ggplot2** package makes a data frame from information contained within the model object using several of the extractor functions.
```{r}
model.df = fortify(model1)
head(model.df)
```

Among other information, the data frame contains a column labeled `.resid` containing the vector of residuals. The residuals are obtained by substracting the observed distance from the fitted distance (`dist` column minus `.fitted` column).

A histogram and density of the model's residuals is obtained by typing
```{r}
ggplot(model.df, aes(.resid)) +
  geom_histogram(bins = 8, color = "white") +
  geom_freqpoly(bins = 8)
```

We see that the histogram is not symmetric. There are more values to the right of the central set of values than to the left. The validity of the normality assumption is under question.

Since departures from normality can occur simply because of sampling variation, the question arises as to whether that apparent skewness we see in this set of residuals is significantly larger than expected by chance.

One approach to visualizing the expected variation from a reference distribution is to superimpose an uncertainty band on the density plot.  

The `sm.density()` function from the **sm** package provides a way to plot the uncertainty band. The first argument is a vector of residuals and the argument `model = "Normal"` draws a band around a normal distribution centered on zero with a variance equal to the variance of the residuals.
```{r}
#install.packages("sm")
library(sm)
sm.density(res, model = "Normal")
```

The black curve representing the model residuals is shifted left relative to a normal distribution and goes outside the blue band in the right tail indicating that the residuals may not be adequately described by a normal distribution although deviation from normality is not large.

In summary, our linear regression model may not be adequate. The assumptions of linearity, equal variance, and normally distributed residuals are not entirely reasonable for these data.

*The Model in Inadequate!*

# Adjust the model: 
What should we do? The relationship appears to be non-linear. So we use the square root of the breaking distance as the response variable instead.
```{r}
ggplot(cars, aes(x = speed, y = sqrt(dist))) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Speed (mph)") + 
  ylab(expression(sqrt("Break Distance (ft)")))
```

Let's fit another model. First make a copy of the original data frame and add a column called `distSR`. Check the assumption of linearity.
```{r}
cars2 = cars
cars2$distSR = sqrt(cars$dist)

ggplot(cars2, aes(x = cut(speed, breaks = 5), y = distSR)) + 
  geom_boxplot() +
  xlab("Speed (mph)") + 
  ylab("Square Root of Break Distance (ft)")
```

Looks good.

Fit the new model and extract and make a histogram of the residuals.
```{r}
model2 = lm(distSR ~ speed, data = cars2)
res2 = resid(model2)

model2.df = fortify(model2)
ggplot(model2.df, aes(.resid)) +
  geom_histogram(bins = 8, color = "white") +
  geom_freqpoly(bins = 8)
```

Distribution of residuals is still skewed but it is better.
```{r}
sm.density(res2, model = "Normal")
```
Another check is to use the quantile-quantile plot. A quantile-quantile plot (or Q-Q plot) is a graph of the quantiles of one distribution against the quantiles of another distribution.  If the distributions have similar shapes, the points on the plot fall roughly along the straight line.

To check the normality assumption of a regression model you want to compare the quantiles of the residuals against the quantiles of a normal distribution. We do that with the `qqnorm()` function.
```{r}
qqnorm(res2)
qqline(res2)
```

Departures from normality are seen as a systematic departure of the points from a straight line on the Q-Q plot.Interpreting Q-Q plots is somewhat visceral. Here are the common situations.

Description: interpretation

* All but a few points fall on a line: outliers in the data
* Left end of pattern is below the line; right end of pattern is above the line: long tails at both ends of the distribution
* Left end of pattern is above the line; right end of pattern is below the line: short tails at both ends of the data distribution
* Curved pattern with slope increasing from left to right: data is skewed to the right
* Curved pattern with slope decreasing from left to right: data is skewed to the left
* Staircase pattern (plateaus and gaps): data have been rounded or are discrete

# Model Transformations

Square root
  * Limitations
    + Cannot be applied to negative numbers
    + Transforms numbers < 1 and > 1 in different ways
    
Logarithm
  * Based 10 or based $e$ (natural logarithm)
  * Appropriate
    + When the SD of the residuals is directly proportional to the fitted values (and not to some power of the fitted values)
    + When the relationship is close to exponential
  * Limitations
    + How to transform zero values?
      - Add a constant such as 1 or 0.00001
      - Remove zero values from analysis
      
Others
  *Box-Cox Transformation
  (http://stackoverflow.com/questions/26617587/finding-optimal-lambda-for-box-cox-transform-in-r)

Be careful transforming count data. Use a different type of regression.

# Regression models make a series of assumptions.

* Before accepting a model you need to examine those assumptions to make sure they are tenable.
* The model fit may be excellent, but you can't be sure your conclusions are correct unless you evaluate the tenability of the assumptions.
* The key to evaluating the regression assumptions are the model residuals.

# Regression modeling is statistical control.

* You often want to do more than just summarize the relationship between variables.
* Regression provides a strategy to control for effects of an explanatory variable to see what is left over.
* These left-overs or residuals are interpreted as "controlled observations" (e.g. percent income controlling for percent graduates).

# Outliers can distort regression results or can be interesting on their own.

* Inspect scatter plots and plots of residuals to determine whether there are any unusual values that might unduly influence the regression line.
* If you find outliers, re-fit the regression model sans those observations and compare results.
* Regardless of how you decide to handle the outliers, let your readers know about them.

# Multiple Regression 


We regress the response variable onto the explanatory variables. And when we describe what we did we don't say we performed a regression of my variables or we regressed y AND x. Or we regressed y with respect to x. These are ambiguous at best.

Examples: Regression of ozone concentration on air temperature, wind speed, and radiation; regression of voting preference on income, education and location; regression of exam scores on homework grade and seat location; regression of mammal brain size on body weight and gestation period; regression of mortality rates on family income, race, and location.

Everything from bi-variate regression carries over to multiple regression. Each explanatory variable contributes a term to the model. There is a slope coefficient for each explanatory variable. That slope coefficient multiplied by the corresponding explanatory variable is called a term. 

With one explanatory variable the regression model is described by a straight line. With two explanatory variables it is described by a flat plane.  

To see this we graph a three dimensional scatter plot with the `scatterplot3d()` from the **scatterplot3d** package and add the regression plane. The example comes from the `trees` data frame. We start with a bi-variate regression. We regress lumber volume (`Volume`) on tree girth (`Girth`) and add the model as a line on the scatter plot.

```{r}
model1 = lm(Volume ~ Girth, data = trees)
plot(trees$Volume ~ trees$Girth, pch = 16, 
     xlab = "Tree diameter (in)", 
     ylab = "Timber volume (cubic ft)")
abline(model1)
```

The `plot()` method and `abline()` are base R graphics functions. Next we regress lumber volume (`Volume`) on girth (`Girth`) and height (`Height`). Use the `scatterplot3d` function to plot the scatter of points in pseudo-3d. The resulting `plane3d` function takes the regression model and adds the plane (with a grid).
```{r}
model2 = lm(Volume ~ Girth + Height, data = trees)
s3d = scatterplot3d(trees, angle = 55, scale.y = .7, pch = 16, 
                    xlab = "Tree diameter (in)", 
                    zlab = "Timber volume (cubic ft)",
                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

We can see that lumber volume increases with tree diameter and tree height. By changing the view angle (`angle =` argument) we can see that some observations are above the plane and some are below.
```{r, eval=FALSE}
s3d = scatterplot3d(trees, angle = 32, scale.y = .7, pch = 16, 
                    xlab = "Tree diameter (in)", 
                    zlab = "Timber volume (cubic ft)",
                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

The distance along the vertical axis from the observation to the model plane is the residual.

With more than two explanatory variables the model is described by a hyper plane but the math is the same. The multiple regression model is given by: 

$$
y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + \cdots + \beta_p \cdot x_{ip} + \varepsilon_i
$$

The explanatory variables (there are $p$ of them) are written with a double subscript to indicate the observation number (1st subscript) and the variable number (2nd subscript).Each explanatory variable gets a coefficient, so there are $p+1$ of them when we include $\beta_0$. The $_i$ on the explanatory variables indicates the change in the observations (i.e. $i^th$ row in the dataset).There is a single response variable $y_i$ and a single set of residuals $\varepsilon_i$. The residuals are assumed to be described by a set of normal distributions each centered on zero and having the same variance ($\sigma^2$).

Fit a multiple regression model:

```{r}
PC.df = read.table("http://myweb.fsu.edu/jelsner/temp/data/PetrolConsumption.txt", header = TRUE)
head(PC.df)
```

```{r}
ggpairs(PC.df)
```


We fit a multiple regression model to the petrol data as follows:
```{r}
model1 = lm(Petrol.Consumption ~ Prop.DL + Pavement + 
            Avg.Inc + Petrol.Tax, data = PC.df)
model1
```

Gasoline consumption increases with the proportion of drivers and decreases with the amount of pavement, average income and gas tax. Why might gas consumption decrease with increasing income?

The equation for the model is: Average petrol consumption [millions of gallons] = 377.3 + 1336 * Prop.DL – 0.0024 * Pavement – 0.06659 * Avg.Inc – 34.79 * Petrol.Tax

The statistically significant variables in explaining petrol consumption are found by looking at the table of coefficients output from the `summary()` function.
```{r}
summary(model1)
```
The larger the p value the less important it is for the model
estimate: same units as responnse divided by explanatory


We see from the table of coefficients that `Pavement` is not significant.

The null hypothesis is that the variable is NOT important to the model. The $p$-value is evidence in support of the null hypothesis. The larger the $p$-value the more evidence you have that the variable is not important to the model.  

Magnitudes of the coefficient estimates (effect sizes) cannot be compared directly because they have different units. A regression coefficient has units of the response variable divided by the units of the explanatory variable.

However the magnitude of the $t$-values are comparable. They are coefficients that have been *standardized* by dividing by the corresponding standard error. Accordingly, `Avg.Inc` is more more important than Petrol.Tax even though .0666 is much smaller than 34.79.

The model says that for every 1 percentage point increase in the proportion of drivers (to overall population), the mean petrol consumption increases by 1.34 billion gallons assuming `Pavement`, `Avg.Inc` and `Petrol.Tax` are constant. 

For every 1 cent/gallon increase in taxes, mean petrol consumption decreases by 34.8 million gallons assuming the three other variables are held constant.

The order of the explanatory variables does not change the magnitude or the sign of the slope coefficients.
```{r}
summary(lm(Petrol.Consumption ~ Pavement + Petrol.Tax + 
           Avg.Inc + Prop.DL, data = PC.df))
```

`Simplify the model`

Can the model be simplified. We try a simpler model by removing `Pavement` (the variable not statistically significant).
```{r}
model2 = lm(Petrol.Consumption ~ Prop.DL + Avg.Inc + 
              Petrol.Tax, data = PC.df)
summary(model2)
```

The remaining explanatory variables in the model are all significant. The values are slightly different as the variance in the gas consumption attributed to `Pavement` is now spread across the remaining variables. Removing an explanatory variable changes the values of the parameters remaining in the model.

The proportion of the population with licenses is the most important variable as can be seen by having the largest $t$ value (in absolute value).

By removing a variable the R-squared statistic DECREASES to .675. This is always the case with a smaller model (fewer explanatory variables). Thus the R-squared statistic should not be used to compare models unless the models have the same number of explanatory variables.  

**NOTE: R squared always decreases when removing variables even if not important so adjusted R squared is much better because it increases only if the variable removed improves the model**

Model assumptions

We need to check the model assumptions. Equal variance. We saw how to check this assumption by using the `cut()` function and creating side-by-side box plots. Another way to check this assumption is to plot the standardized residuals on the vertical axis and the fitted values along the horizontal axis. Adding a smoothed curve and the y equal zero line makes it easy to see whether there is a pattern to the residuals.
```{r}
model2.df = fortify(model2)
ggplot(model2.df, aes(x = .fitted, y = .stdresid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0)
```

In this case: no.

Normality. First we use the `sm.density()` function.
```{r}
res = residuals(model2)
sm.density(res, xlab = "Model Residuals", model = "Normal")
```

We also examine a quantile-normal plot.
```{r}
qqnorm(model2$residuals)
qqline(model2$residuals)
```

With either of these plots we see some evidence against normality and constant variance. The fact that the residuals do not exactly follow a normal distribution lowers the confidence we can place on our inferences.

To improve model adequacy we might transform the response variable or use a weighted regression model. We will talk about transforming a response variable on Thursday and weighted regression is considered later in the context of geographic regression.

# Interaction between two continuous variables in a Multiple Regression

```{r}
scores.df = read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
```

The data includes the following measurements on 200 students: 

* Standardized reading test score 
* Standardized writing test score
* Standardized social economic status (socst)  

We regress reading score onto writing score and socio-economic status.
```{r}
model1 = lm(read ~ write + socst, 
            data = scores.df)
summary(model1)
```

We find writing skill helps reading skill as does social economic status.

But maybe the story is more complicated. Reading and writing scores depend on school quality, which is a function of social economic status. Thus it would be surprising if the RELATIONSHIP between reading and writing scores is independent of social economic status.

To check this possibility, first create a new categorical variable and then make a conditional scatter plot.
```{r}
scores.df2 = scores.df %>%
  mutate(socst2 = cut(socst, 2))
ggplot(scores.df2, aes(y = read, x = write, color = factor(socst2))) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Writing Score") +
  ylab("Reading Score")
```

The slopes cross so we should check for an interaction.

Adding an interaction term tells another story.
```{r}
model2 = lm(read ~ write + socst + write:socst, 
            data = scores.df)
summary(model2)
```

The interaction term is significant. How do we interpret it? As social economic status increases the relationship between writing and reading skill increases (larger slope). Mathematically
$$
y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 \cdot x_2) + \varepsilon_i
$$

Let $x_2 = c$ (let social economic status be constant), then the effect of $x_1$ is
$$
y_i = \beta_0 + \beta_1 x_1 + \beta_2 \cdot c + \beta_3 (x_1 \cdot c) + \varepsilon_i \\
y_i = (\beta_0 + c \beta_2) + (\beta_1 + c \cdot \beta_3) x_1 + \varepsilon_i
$$

avg reading = 49.8 - .362 * write - .398 * socst + .015 * write * socst + e

* When socst = 26, a one unit increase in writing produces a (-.362 + 26 * .015) = .028 unit increase in avg reading.
* When socst = 71, a one unit increase in writing produces a (-.362 + 71 * .015) = .703 unit increase in avg reading.

A plot of the conditional coefficient is made with the `interplot()` function from the **interplot** package. The model object is given along with the names of the interacting variables as character strings. The function uses the grammar of graphics structure so we can add layers as before.
```{r}
interplot(m = model2, var1 = "socst", var2 = "write") +
    geom_hline(yintercept = 0, linetype = "dashed") +
    xlab("Social Economic Status") +
    ylab("Coefficient on Writing Score") +
    theme_bw() 
```
If a non interactive model it would be a flat line. Therefore, we have a significant interaction.


The plot is a visualization of the interaction. There is a significant upward trend on the writing score coefficient as a function of social economic status. The 95% uncertainty band on the interaction is given in gray.

The coefficient of writing is clearly conditioned by social economic status. Note: This also means that the coefficient of social economic status is conditioned by writing in a symmetric way.

# Collinearity

This is an indication of collinearity. The large between-variable correlation leads to a model that may not make physical sense. The rule is that when the correlation between two explanatory variables exceeds .6, collinearity can be a problem.  

When explanatory variables have large correlation then estimates of the model parameters are not precise. A model with imprecise parameter estimates is not useful. The best way to proceed in this situation is to reduce the set of explanatory variables. 

# Akaike Information Criterion

Another useful statistic in this regard is called AIC (Akaike Information Criterion). AIC is more general and can be used with models that are fit with methods other than least-squares minimization.

AIC is rooted in information theory. Values of AIC provide a means for model selection in a relative sense. If all candidate models fit poorly the AIC is not much help.

AIC = $2 k - 2 \log(L)$, where k is the number of model parameters (1 + number of explanatory variables) and L is the highest value from the likelihood function. The likelihood function (likelihood) is the probability of your data given the model.

AIC rewards goodness of fit, but includes a penalty that is an increasing function of the number of estimated parameters. Given a set of candidate models for our data, we should choose the one with the lowest AIC. 

use the `drop1()` function, which takes a regression model object and returns a table showing what happens when each variable is removed from the model.
```{r}
modelF = lm(Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax, 
            data = PC.df)
drop1(modelF)
```

The first row also gives the AIC value for the full model. Here 407.37. The model deviance has units of the response variable. The AIC has no units.

The next row tells us what happens to the RSS if the explanatory variable Prop.DL is removed from the full model. If we drop the `Prop.DL` variable from the full model, the RSS increases to 401405 (from 189050) and we gain one degree of freedom (less bias).

Said another way, the value of 212355 in the `Sum of Sq` column indicates how much the variable `Prop.DL` is worth to the model. By removing it the RSS increases from 189050 to 401405; a difference of 212355.

Apparently that is too much of an increase in RSS for the gain of only 1 degree of freedom as the AIC increases from 407 to 442. Said another way, the AIC is 407 for the full model. In comparison, the AIC is 442 for a model that is otherwise the same but missing `Prop.DL`.

The next row gives the same information about the `Pavement` variable. That is, what happens to the RSS from the full model when the variable `Pavement` is removed.

Here the RSS increases by 2252 to 191302 (the residuals on average get a bit larger), but this increase is worth it as it only costs one degree of freedom. This is indicated by a reduction in the AIC to so 405.94. 

The AIC column keeps score. The AIC for the model having all the explanatory variables is 407.37. A model without `Prop.DL` has an AIC of 441.51, which is LARGER than the AIC for the full model so we keep `Prop.DL` in the model.

On the other hand, a model without `Pavement` has an AIC of 405.94, which is SMALLER than the AIC for the full model so we remove `Pavement` from the model.

So we simply compare the AIC values for each variable against the AIC value for the full model. If the AIC value for a variable is less than the AIC for the full model then the trade-off is in favor of removing it.









Facet_wrap: 
```{r}
ggplot(kid.weights, aes(x = height, y = weight)) + 
  geom_point() + 
  facet_grid(~ gender)
```


Make a fully detailed figure: 
```{r}

```


Lesson 7 -- Quantitative Geography for boxplots by month. 























