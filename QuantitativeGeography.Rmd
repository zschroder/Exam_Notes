---
title: "Quantitative Geography Notes"
author: "Zoe Schroder"
date: "9/19/2019"
output: html_document
---

Load libraries: 
```{r}
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(ISwR))
suppressMessages(library(scatterplot3d))
suppressMessages(library(GGally))
suppressMessages(library(foreign))
suppressMessages(library(msm))
suppressMessages(library(interplot))
suppressMessages(library(sm))
suppressMessages(library(pROC))
suppressMessages(library(spgwr))
suppressMessages(library(spdep))
suppressMessages(library(tidyverse))
suppressMessages(library(spdep))
suppressMessages(library(quantreg))
suppressMessages(library(MASS))
suppressMessages(library(rgdal))
suppressMessages(library(RColorBrewer))
```

# Variance:

Variance is the measures of how far a set of numbers are spread out from their average value.
The formula is: 
$$
\hbox{var}(x) = \frac{(x_1 - \bar x)^2 + (x_2 - \bar x)^2 + \cdots + (x_n - \bar x)^2}{n-1}
$$

# Mean: 

The mean is the average calue of a set of numbers defined as: 
$$
\bar x = \frac{x_1 + x_2 + \cdots + x_n}{n}
$$
The sample mean is a measure of the central tendency of a set of values. Typically there are more values near the mean. This is similar with the median, but the median is more resistent to outliers. The mediann is a `resistant statistic` because it is not greatly influenced by only a few values. 

# Quantiles: 

Quantiles cut a set of ordered data into equal-sized data bins. The ordering comes from rearranging the data from lowest to highest. The first, or lower, quartile corresponding to the .25 quantile (25th percentile), indicates that 25% of the data have a value less than this value. The third, or upper, quartile corresponding to the .75 quantile (75th percentile), indicates that 75% of the data have a smaller value than this value.

The difference between the first and third quartile values is called the interquartile range (IQR). Fifty percent of all values lie within the IQR.

```{r}
x <- rnorm(n = 10000, 
           mean = 10, 
           sd = 100)
quantile(x, 
         probs = c(0.05, .25, .5, .75, .95))

IQR(x)
```

Find location of min and max values in a data frame: 
```{r}
Volume = c(20.233, 19.659, 18.597, 18.948, 17.820, 16.736, 16.648, 17.068, 15.916, 14.455, 14.569)
Year = 2002:2012
Ice.df = data.frame(Year, Volume)

which.min(Ice.df$Volume)
which.max(Ice.df$Volume)
```

# Dplyr: 

Verb        | Description
-----------:|:-----------
`select()`    | selects columns; pick variables by their names
`filter()`    | filters rows; pick observations by their values
Try this on your thesis stuff lo
`arrange()`   | re-orders the rows
`mutate()`    | creates new columns; create new variables with functions of existing variables
`summarize()` | summarizes values; collapse many values down to a single summary
`group_by()`  | allows operations to be grouped

For other types of combinations, we need to use Boolean operators: `&` is "and", `|` is "or", and `!` is "not".

```{r}
filter(airquality, Month == 5 | Month == 9)

df = airquality %>%
  filter((Month == 8 | Month == 9) & Wind < 5) %>%
  dplyr::select(Month, Day, Ozone, Wind, Temp)
```

# Arrange: 

Arrange the data by a specific column
```{r}
# Ascending: 
airquality %>%
  arrange((Solar.R))

# Descending
airquality %>%
  arrange(desc(Solar.R))
#### OR ####
airquality %>%
  arrange(-Solar.R)
```

# Melt: 

Sometimes you need to reshape the data. You use the `melt()` function. The ID variable is a vector of a non-measured quantity. 

```{r}
L = "http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt"
FLp = read.table(L, na.string = "-9.900", 
                 header = TRUE)

FLpL = melt(FLp, id.vars = "Year")
head(FLpL)
ggplot(FLpL, aes(x = variable, y = value)) + 
  geom_boxplot() +
  xlab("Month") + 
  ylab("Precipitation (in)")
```

# Correlation

A scatter plot is a graphical representation of the correlation. The corrlation describes the linear relationship between two variables witha  single number. The correlation computed on a set of data values a statistic. It describes whether larger- and smaller-than-average values of one variable are related to larger- or smaller-than-average values of the other variable.  If larger than average values of one variable tend to be associated with larger than average values of the other variable then the correlation is positive. These values range betweenn -1 and +1 

Pearson correlation
$$
r = \frac{1}{n-1} \sum \left(\frac{x_i - \bar x}{s_x}\right)\left(\frac{y_i - \bar y}{s_y}\right)
$$
where $n$ is the sample size, $\bar x$ and $\bar y$ are the average of the x and y variables, respectively and $s_x$ and $s_y$ are the standard deviation of the x and y variables.

```{r}
cor(FLp$Jan, FLp$Feb)
```


geom_smooth(method = lm, se = FALSE) + #linear trend line
geom_smooth(se = FALSE, color = "red") #nonlinear trend line

Spearman Rank Correlation: 

Spearman rank correlation is a nonparametric measure of the rank correlation (statistical dependence between the rankings of two variables.) This correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples (Spearman's rho is limited to the value of the rank). High correlation between two variables indicates a similar rank. 

```{r}
cor(FLp$Jan, FLp$Feb, method = "spearman")
```

# One-sample test: 

A one-sample test of whether the mean of a population has a value specified in a null hypothesis. This is a test of location: are the data consistent with being centered on a specific value?

*Example: Given a random sample of FSU student heights (to make things simple, let's assume we are considering only the guys) in feet, test the hypothesis that the mean height of the population of guys at FSU is 183 cm (~ 6 feet).*

Start by plotting the data together with the hypothesis.
```{r}
ht = c(177, 180, 179, 174, 192, 186, 165, 183)
ht.df = data.frame(Student = 1:length(ht), 
                   Height = ht)

ggplot(ht.df, aes(x = "", y = Height)) + 
  geom_boxplot() +
  ylab("Height (cm)") +
  geom_hline(aes(yintercept = 183), color = "red") +
  theme_minimal()
mean(ht)
```

We formally test the hypothesis that the mean height in the population is 183 cm with the `t.test()` function. The first argument is the data (as a vector!) and the second argument is the suggested population mean (`mu =`). Note that the mean is a statistic so we  
```{r}
t.test(ht, mu = 183)

#The output begins with the $t$-value (-1.2239).  The $t$-value is a statistic computed as

(mean(ht) - 183)/(sd(ht)/sqrt(length(ht)))

```

The 'degrees of freedom' (dof, df) is a term that indicates the number of values in the calculation of a statistic with a known value that are 'free' to vary. Suppose you know the mean of a set of numbers (say it's 24) and how many numbers were used to calculate the mean (say there are five). What values could the five numbers have? Four of them could be any value, but the fifth one is constrained to make the mean work out to be 24. This is true for any mean. Thus the mean is a statistic with degrees of freedom equal to n - 1.

# p-values

A $p$-value is an estimate of the probability that your data, or data more extreme than observed, could have occurred by chance IF THE HULL HYPOTHESIS IS TRUE. Said slightly differently this is how unusual your data are ASSUMING THE NULL HYPOTHESIS IS TRUE. A small $p$-value tells you that your data is unusual with respect to this particular hypothesis.

The $p$-value summarizes the evidence in support of the null hypothesis. The smaller the $p$-value, the less evidence there is in support of the null hypothesis. But, the interpretation of the $p$-value is typically stated as evidence AGAINST the null hypothesis:

$p$-value        | Statement of evidence against the null
---------------- | ---------------------
less than  .01   | convincing
.01 - .05        | moderate 
.05 - .15        | suggestive, but inconclusive
greater than .15 | no

The $p$-value is the area under the tails of the $t$ distribution. The distribution describes how the $t$ statistic varies. The $t$ statistic is given by
$$
t = \frac{\bar x - \mu_o}{s/\sqrt{n}}
$$
where $s$ is the standard deviation of the sample values and $n$ is the sample size. The denominator is the standard error (standard deviation divided by square root of sample size) of the mean.

The $p$-value comes from the `pt()` function, which determines the area under the $t$ distribution curve to the left of a particular value. The curve is obtained using the `dt()` function (density function).  

The area under the curve to the left of -1.2239 is 0.13. So 13% of the area lies to the left of the first red line. The distribution is symmetric so 13% of the area lies to the right of the second red line. With a two-sided test you add these two fractions (or multiply by 2) to get the $p$-value.
```{r}
curve(dt(x, 7), from = -3, to = 3, lwd = 2)
abline(v = -1.2239, col = 'red')
abline(v = 1.2239, col = 'red')

#To check the p-value of a t statistic: t-value = -1.2239
pt(-1.2239, df = 7)
#0.13
pt(-1.2239, 7) * 2
#0.26
```

# two-sample test 

With two samples of data the hypothesis is that they both come from distributions having the same mean.

For example, data are from two groups which we assume are sampled from the normal distributions. We test the null hypothesis that the two samples have the same population mean by computing the t statistic. 

In this case, the $t$ statistic is the difference in sample means divided by the standard error of the difference in means (SEDM).

There are two ways to calculate SEDM. (1) Assume equal variance: use the pooled standard deviation (s). Under the null hypothesis, the t statistic will follow a t distribution with n1 + n2 - 2 degrees of freedom (df). (2) Don't assume equal variances (this is the default assumption). Under the null hypothesis, the $t$ statistic approximates a $t$ distribution. In this case it is called the Welch procedure. In this case the df is not an integer.

Usually the two methods give similar results unless group sizes and variances differ widely among the two samples.

Note that in this case the df is a whole number, namely 13 + 9 - 2 = 20.  The $p$-value has dropped slightly and the confidence interval is a little narrower, but the changes are slight.

Start with side-by-side boxplots.
```{r}
ggplot(tlc, aes(x = factor(sex), y = tlc)) +
  geom_boxplot() +
  xlab("Sex (1: female)") +
  ylab("Total lung capacity (liters)") +
  theme_minimal()
```

You see a difference in lung expenditure between the two groups. Is this difference statistically significant?

With this data format (long) we use model syntax for specifying the test. Model syntax is response variable ~ explanatory variable. The `~` is the tilde on your keyboard.

Thus `tlc` is your response variable and `sex` is your explanatory variable.
```{r}
t.test(tlc ~ sex, data = tlc)

# t = -3.6693, df = 29.703, p-value = 0.0009493
# alternative hypothesis: true difference in means is not equal to 0
# 95 percent confidence interval:
# -2.7691965 -0.7883035
# sample estimates:
# mean in group 1 mean in group 2 
#       5.198125        6.976875 
```

The output is a bit different this time. The small $p$-value allows you to conclude that there is convincing evidence against the null hypothesis of no difference. Therefore, they are significantly different. The wider the uncertainty (confidence) interval the more uncertain we are about the population difference. It does not contain the value 0, so we can conclude there is a statistical difference.

# F Test

To test the assumption of equal variances you use `var.test()` function.
```{r}
var.test(energy$expend ~ energy$stature)
```

The variance test is based on the assumption that the groups are independent.  Don't apply this test to paired data. if the variances are equal, the ratio of the variances will be 1.

# Wilcoxon Test aka Mann-Whitney U test

Best to use when the data is not normally distributed. 

We can avoid the distributional assumption by using a non-parametric test. The non-parametric alternative is the Wilcoxon test. Also known as the Mann-Whitney U test.

The test statistic W is the sum of the ranks in the first group minus the sum of the ranks in the second. It is invoked with the `wilcox.test()` function. Try it on the energy data.
```{r}
wilcox.test(energy$expend ~ energy$stature)
```

# Paired observations

Paired tests are used when there are two measurements on the same experimental unit. The theory is based on taking the differences in paired values thus reducing the problem to that of a one-sample test. To invoke the paired $t$ test, use the argument `paired = TRUE`.

Example: Shoe material. A manufacturer makes two different shoe materials (A and B).  A sample of 10 kids try *both* materials and recorded are the wear times (months) for each material. Compare whether or not there is any difference in shoe material wear times.

Only used when it is the same people in the test. The same 10 kids are in both A and B
```{r}
matA = c(14, 8.8, 11.2, 14.2, 11.8, 6.4, 9.8, 11.3, 9.3, 13.6)
matB = c(13.2, 8.2, 10.9, 14.3, 10.7, 6.6, 9.5, 10.8, 8.8, 13.3)
```

```{r}
t.test(matA, matB, paired = TRUE)
```

# Bayesian data analysis

The Bayesian approach to inference is now everywhere. Solving problems like predicting elections and directing self-driving cars. Problems where large uncertainty needs to be quantified and where efficient integration of many sources of information is required.

The problem is that the approach is still not taught in statistics. R is made for Bayesian data analysis. But if you google "Bayesian" you get philosophy. Subjective vs objective, frequentism vs Bayesianism, $p$-values vs subjective probabilities.

`In brief:`

The building blocks of the Bayesian approach are distributions. We've already talked about the Gaussian (normal) distribution as a model for the mean value over a set of measurements. The Gaussian distribution has two parameters $\mu$ and $\sigma$ so we call a Gaussian model a family of Gaussian distributions.

A parametric model is a family of distributions that can be described using a finite (usually small) number of parameters. Bayesian models are made up of parametric models.

Bayesian models are generative. If we know the parameters then we can generate data: Parameters ($\mu$, $\theta$, $\sigma$) --> generative model --> data (5, 3, 4, 0, 1, ...): Monte Carlo simulation. If we know the data then we can estimate the parameters: data (5, 3, 4, 0, 1, ...) --> generative model --> Parameters ($\mu$, $\theta$, $\sigma$): Bayesian data analysis

What Bayesian data analysis is not

* A category of models
* Subjective
* Not the most efficient method of fitting a model
* Anything new

Why use Bayesian data analysis? *(Lesson 10 -- Quant Geo)*

We can build more flexible models. We can use models to answer specific questions in a natural way. 

Bayes Theorem is used to determine a posterior distribution on the unknown number of fish in a lake. It was not determined analytically but rather by repeated sampling. The distribution itself was a vector of values representing possible numbers of fish in the lake.

We can build models to answer natural questions. We don't need to express our questions in terms of a null hypothesis. 

We also have greater flexibility in building models. 

Bayes Theorem. prior x likelihood / posterior

What Bayesian data analysis is not

A category of models (e.g., regression models, decision trees). Rather it is a way of thinking about and constructing models. You can do regression or machine learning with a Bayesian framework (e.g., Bayesian decision analysis).

Not subjective. All statistics require you make assumptions. Results have to be interpreted in light of those assumptions.

Nothing new. Thomas Bayes, Simon LaPlace, Ronald Fischer (first to use the term Bayesian statistics).

# Linear Regression 

Recall that a $t$ test is used to examine whether there is evidence that population means from two distinct groups are statistically different. Linear regression extends the test to an arbitrary number of groups.

The data frame contains total lung capacity (`tlc`) in liters for 32 patients waiting for a heart or lung transplant. It also contains the patients sex (`sex`) coded as 1 for female and 2 for male.We test the null hypothesis of no difference in lung capacity between males and females. 
```{r}
t.test(tlc ~ sex, 
       data = tlc, 
       var.equal = TRUE)
```

The $t$-statistic, computed as the difference in means divided by the standard error of the difference, is -3.6693. With 30 degrees of freedom (sample size = 32), the $p$-value is less than .01 so we reject the null hypothesis of no difference in lung capacities. 

The same test is done with a linear regression model. The function is `lm()`. The syntax is identical to that of the `t.test()` function, except the assumption of equal variance is the default.
```{r}
summary(lm(tlc ~ sex,
           data = tlc))
```
We see that the $p$-value (last line) is identical to that from the `t.test()` and state that we reject the null hypothesis that lung capacity is not significantly related to `sex`. In the table of coefficients we see the value of `1.7788` next to `sex` in the column labeled `Estimate`. This is the difference in average lung capacity grouped by sex. Linear regression generalizes this type of comparison across any number of groups (explicit or not).

The mean value is a model for these data. Some points are above the line and others below. No individual value fits the model precisely (no points are on the line), but the model represents the 'best' guess at what a new value might be.

The closer the points are to the line, the more precise the model is.  Closeness is defined as the distance along the y axis between the point and the line. Distances above the line indicate values that are larger than the mean, so `y - mean(y)` is positive for these values. Distances below the line indicate values that are smaller than the mean, so `y - mean(y)` is negative for these values.
```{r}
y = c(2.9, -2.1, -0.5, 2.9, 4.2)
x = 1:5
df = data.frame(x, y)

p = ggplot(df, aes(x, y)) +
      geom_point(size = 4) +
      geom_hline(yintercept = mean(y)) +
                 scale_y_continuous(limits = c(-6, 6))

p = p + geom_segment(aes(y = mean(y), yend = y, 
                         x = x, xend = x))
p
```
Instead we add the squared distances.
```{r}
sum((y - mean(y))^2)
```

The sum of the squared distances is a measure of how well the model fits the data. The smaller the sum, the better the fit. It is called the sum of the squared errors (SSE). The individual distances are called "errors." Statisticians call them "residuals." The SSE equal to 0 indicates that all points fall exactly on the line.

Keep in mind: A residual is the observed value minus the modeled value.

We see that the regression line (conditional mean) provides a more precise model since the SSE is smaller. Rule: When choosing a regression model from a set of competing models we choose the model that minimizes the SSE.

Recall that a line is uniquely determined by its slope and the value at which it intersects the vertical axis (y-intercept). Mathematically the slope and y-intercept are determined by
$$
\hat \beta_1 = \frac{\sum (x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2}\\
\hat \beta_0 = \bar y - \hat \beta_1 \bar x
$$
`beta1` ($\beta_1$) and `beta0` ($\beta_0$) are called regression model coefficients. $\beta_1$ is the slope coefficient and $\beta_0$ is the y-intercept coefficient.


The notation is `lm(y ~ x)`. The `~` (tilde) in this notation is read "is modeled by" or "is conditional on". So the model formula `y ~ x` is read "y is modeled by x". If we use `lm(y ~ x)` then we say "y is modeled by x" in a linear way.

```{r}
lm(y ~ x)
```

Example: Age is in years and the heart rate is in beats per minute. Heart Rate is response variable. Age is explanatory variable 
```{r}
Age = c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
HR = c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
heart.df = data.frame(HR = HR, Age = Age)

model <- lm(HR ~ Age, data = heart.df)

# Make predictions for these ages: 
predict(model, newdata = data.frame(Age = 50))
predict(model, newdata = data.frame(Age = seq(40, 60, 10)))
```

We interpret the model as follows: On average a person's maximum HR *decreases* by .8 bpm every year. Or more easily understood as a decrease of 8 bpm every 10 years.

```{r}
summary(model)
```

estimate / std error == t value 
null hypothesis: age does not effect heart rate 
reject the null

The `summary()` method returns quite a bit of output. The first is the code we used to create the model. The second are the summary statistics for the model residuals: observed value minus the modeled value. The most important output is the table of coefficients. The table shows the slope and intercept coefficients in the column labeled `Estimate`. The adjacent column labeled `Std. Error` lists the standard errors on these coefficients. The standard error (or margin of error) is a measure of the uncertainty surrounding the coefficient estimate. For the slope coefficient it is computed as
$$
s_{\beta_1} = \sqrt{ \frac{\frac{1}{n - 2}\sum_{i=1}^n \varepsilon_i^{\,2}} {\sum_{i=1}^n (x_i -\bar{x})^2} },
$$
where
$$
\varepsilon_i  = y_i - \beta_0 - \beta_1 x_i
$$

The best estimate for the (population) slope is -.798 with a margin of error of +/- .07. The uncertainty about the model coefficient is used to test hypotheses and compute confidence intervals. 

Interest typically centers on the null hypothesis that the slope = 0. A zero slope implies the line is horizontal and thus, in this case, that maximum heart rate is independent of how old you are.

The $t$-value ($t$-statistic) for the zero-slope hypothesis is the slope divided by its standard error. The $t$-value has a $t$ distribution with $n-2$ degrees of freedom if the slope is zero.

Here the $t$-value is -11.4 which gives a $p$-value (Pr(>|t|)) of .0000000385 (3.85e-08). It is written this way, because the $p$-value is the probability of observing a more extreme $t$-value (positive or negative) assuming the null hypothesis is true (slope is zero).

The symbols to the right indicate a category of confidence in the inference. The line below the table shows the definitions which we can interpret using our definition. Three asterisks: overwhelming, Two asterisks: convincing, One asterisk: moderate, Point: suggestive but inconclusive.

So we have overwhelming evidence that on average maximum heart rate depends on age given these data.

The next line of output from the `summary()` function gives the residual standard error. This tells us how close (on average) the observations are from the regression line. The degrees of freedom are again $n-2$.
```{r}
sqrt(sum(resid(model)^2)/13)
```

Note the `resid()` [or `residual()`] outputs the residuals from the model object.
```{r}
resid(model)
```

The next line of output gives the multiple R-squared value. Also called the coefficient of determination. And the adjusted R-squared value. The multiple R-squared is equal to the square of the Pearson correlation coefficient. 

The multiple R-squared = 1 - SSE/SSY, where SSE is the sum of the squared residuals (residual sum of squares--RSS) and SSY is the total variation about a constant mean response.

It is useful to see how these are computed. The SSE is the sum of the squared residuals. This is computed with the `deviance()` function. Smaller deviance means a better model. 
```{r}
SSE = sum(resid(model)^2)
SSE
deviance(model)
```

To compute the multiple R-squared we need SSY. SSY is the deviance from the simpler constant mean model. This simpler model is estimated using the `lm()` function as
```{r}
model0 = lm(HR ~ 1, data = heart.df)
SSY = deviance(model0)
1 - SSE/SSY
```

One minus the ratio of the explained variation to the total variation.

SSE is less than SSY so SSE/SSY will be less that 1. If SSE is much less than SSY then, SSE/SSY is close to zero so R squared is close to 1.

The R-squared multiplied by 100% is the variance of the response variable explained (statistically) by the explanatory variable.

The adjusted R-squared value is a reduction from the R-squared value. The amount of reduction depends on how many variables are in the model.
$$
1 - \frac{n - 1}{n - p} (1 - R^2 )
$$

where $n$ is the sample size and $p$ is the number of coefficients.

The adjusted R-squared is always smaller than the multiple R-squared, can decrease as new explanatory variables are added, and can even be negative for really poorly fitting models. It is important in the context of multiple regression.

The final line of output is the F-value, degrees of freedom, and associated $p$-value.
$$
F_\hbox{statistic} = \frac{(SSY - SSE)/(p - 1)}{SSE/(n - p)}
$$

Under the null hypothesis that the regression is no better than the unconditional mean as a model for the data, the $F$-statistic comes from an $F$ distribution with ($p-1$) and $n$ degrees of freedom.

```{r}
pf(130, 1, 13, lower.tail = FALSE)
```

The $p$-value is very small so we reject the null hypothesis that the mean model (`lm(HR ~ 1, data = heart.df)`) is better than the linear regression model.

To get the uncertainty intervals on the predicted values:  Use the `level =` and `interval = "confidence"` arguments.
```{r}
predict(model, newdata = data.frame(Age = c(50, 60)), 
        level = .95, interval = "confidence")
ggplot(heart.df, aes(x = Age, y = HR)) +
  geom_point() +
  geom_smooth(method = lm)
```

The lower (`lwr`) and upper (`upr`) bounds represent the 95% uncertainty interval about the location of the line for the particular value of the explanatory variable `Age`.

We state that the best prediction for average maximum heart rate for a set of random 50-yr olds is 170 bpm with a 95% uncertainty interval between 167 and 173 bpm. Thus if we repeat the sampling 100 times and make the same prediction, our CI on the prediction will cover the true predicted value 95 times.

With `interval = "prediction"` we get a 95% *prediction* interval. That interval is wider than the confidence interval as it represents two sources of uncertainty. 
The uncertainty associated with the mean value GIVEN the person's age AND the uncertainty associated with a particular maximum heart rate GIVEN the conditional mean.  
```{r}
predict(model, data.frame(Age = c(50, 60)), 
        level = .95, interval = "prediction")
```

# Model Adequacy

By definition the regression line is the best fit line through the data. However the best fit line may not be an adequate model for your data. 

Model adequacy is about the quality of the inferences you can make with the model. It says nothing about the strength of the relationship between the response and explanatory variable. Checks on model adequacy alert you to where/how the model can be improved (e.g., transform the data, use a nonlinear term, etc.).

What do I mean by an inference from the model? 

When we infer something about nature or human behavior with our linear regression model we are making four assumptions. All four assumptions imply that the residuals should 'look' a certain way, or equivalently that the distribution of Y (response variable) conditional on X (explanatory variable) should look a certain way.

* Linearity: Average values of Y in ordered intervals of X should be a straight-line function of X. Each interval creates a 'sub-population' of Y values. 
* Constant variance: Sub-populations of Y should all have the same standard deviation.
* Normality: Values from each sub-population can be described by a normal distribution.
* Independence: Each observation is independent from the other observations.

To the extent these assumptions are valid, the model is said to be *adequate* in representing our data. A model can be statistically significant, but not adequate.

The first three assumptions are best examined using graphs. Note: We never prove an assumption is valid. We check to see if there is evidence to question its validity.

Consider the `cars` data frame. The data contains two columns `speed` and `dist` giving the speed (mph) and the corresponding distance (ft) needed to stop. The data were recorded in the 1920s.
```{r}
cor(cars$speed, cars$dist)
```

The correlation tells us there is a strong relationship between the two variables. The faster the car is moving, the more distance it needs to come to a stop.

We are interested in the breaking distance given the forward speed so `dist` is our response variable. Next we make a scatter plot.
```{r}
ggplot(cars, aes(x = speed, y = dist)) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Speed (mph)") + ylab("Break Distance (ft)")
```

The scatter of points shows an apparent linear relationship between the breaking distance and the forward speed of the car for this sample of data.

Fit a regression model to the data. Since distance is our response variable, we state that we regress stopping distance onto forward speed.
```{r}
model1 = lm(dist ~ speed, data = cars)
summary(model1)
```

ALWAYS Y ON TO X: Observed - fitted = residual

The model indicates a statistically significant relationship ($p$-value is less than .01) between breaking distance and speed. In fact, 65% of the variation in average breaking distance is associated with differences in speed.  

The statistically significant effect of speed on breaking distance suggests it is unlikely in the population of cars that there is no relationship between speed and breaking distance.

The model looks pretty good, so we write up our results and we are done. Not so fast, let's take a closer look.

There tends to be more values of breaking distance below the line than above the line over the range of speeds between 10 & 20 mph. Also, the spread of the residuals appears to get larger as speed increases. There is more variation in the response for larger values of speed.

*Assumption of Linearity and Constant Variance: *
One way to see this is to divide the explanatory variable into non-overlapping intervals and display the set of corresponding response values as a box plot. The variable is divided using the `cut()` function. Here we cut the `speed` into 5 intervals.
```{r}
cut(cars$speed, breaks = 5)
cars$speed[1]
```

The first value of `speed` is 4 mph and it falls in the interval (3.98, 8.2]. Greater than 3.98 and less than or equal to 8.2. And so on.

We can use the output of the `cut()` function as an argument in the `aes()` function to draw side by side box plots.
```{r}
ggplot(cars, aes(x = cut(speed, breaks = 5), y = dist)) + 
  geom_boxplot() +
  xlab("Speed (mph)") + 
  ylab("Break Distance (ft)")
```

In effect we are creating sub-populations. The population of response values within a given interval of the explanatory variable. The number of intervals is the number of breaks specified by the `cut()` function.

Here we see `evidence against the assumption of linearity`. Further we see that the box size is increasing. That is the IQR range of breaking distance is larger for faster moving cars. This creates `doubt on the assumption of constant variance`.`

*Assumption of Normality: *
What about the assumption of normality? Although the normality assumption about the residuals is that the *conditional* distribution of the residuals at each $x_i$ is adequately described by a normal distribution, with sample data there usually are not enough observations at each value of $X$ to check.

In practice it's common to examine the residuals all together. The residuals are obtained by using the `resid()` (extractor) function.
```{r}
res = resid(model1)
res
```

There are other extractor functions (like `coef()`) that output information from the model as vectors or matrices.

The `fortify()` function from the **ggplot2** package makes a data frame from information contained within the model object using several of the extractor functions.
```{r}
model.df = fortify(model1)
head(model.df)
```

Among other information, the data frame contains a column labeled `.resid` containing the vector of residuals. The residuals are obtained by substracting the observed distance from the fitted distance (`dist` column minus `.fitted` column).

A histogram and density of the model's residuals is obtained by typing
```{r}
ggplot(model.df, aes(.resid)) +
  geom_histogram(bins = 8, color = "white") +
  geom_freqpoly(bins = 8)
```

We see that the histogram is not symmetric. There are more values to the right of the central set of values than to the left. The validity of the normality assumption is under question.

Since departures from normality can occur simply because of sampling variation, the question arises as to whether that apparent skewness we see in this set of residuals is significantly larger than expected by chance.

One approach to visualizing the expected variation from a reference distribution is to superimpose an uncertainty band on the density plot.  

The `sm.density()` function from the **sm** package provides a way to plot the uncertainty band. The first argument is a vector of residuals and the argument `model = "Normal"` draws a band around a normal distribution centered on zero with a variance equal to the variance of the residuals.
```{r}
sm.density(res, model = "Normal")
```

The black curve representing the model residuals is shifted left relative to a normal distribution and goes outside the blue band in the right tail indicating that the residuals may not be adequately described by a normal distribution although deviation from normality is not large.

In summary, our linear regression model may not be adequate. The assumptions of linearity, equal variance, and normally distributed residuals are not entirely reasonable for these data.

*The Model in Inadequate!*

# Adjust the model: 
What should we do? The relationship appears to be non-linear. So we use the square root of the breaking distance as the response variable instead.
```{r}
ggplot(cars, aes(x = speed, y = sqrt(dist))) + 
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Speed (mph)") + 
  ylab(expression(sqrt("Break Distance (ft)")))
```

Let's fit another model. First make a copy of the original data frame and add a column called `distSR`. Check the assumption of linearity.
```{r}
cars2 = cars
cars2$distSR = sqrt(cars$dist)

ggplot(cars2, aes(x = cut(speed, breaks = 5), y = distSR)) + 
  geom_boxplot() +
  xlab("Speed (mph)") + 
  ylab("Square Root of Break Distance (ft)")
```

Looks good.

Fit the new model and extract and make a histogram of the residuals.
```{r}
model2 = lm(distSR ~ speed, data = cars2)
res2 = resid(model2)

model2.df = fortify(model2)
ggplot(model2.df, aes(.resid)) +
  geom_histogram(bins = 8, color = "white") +
  geom_freqpoly(bins = 8)
```

Distribution of residuals is still skewed but it is better.
```{r}
sm.density(res2, model = "Normal")
```
Another check is to use the quantile-quantile plot. A quantile-quantile plot (or Q-Q plot) is a graph of the quantiles of one distribution against the quantiles of another distribution.  If the distributions have similar shapes, the points on the plot fall roughly along the straight line.

To check the normality assumption of a regression model you want to compare the quantiles of the residuals against the quantiles of a normal distribution. We do that with the `qqnorm()` function.
```{r}
qqnorm(res2)
qqline(res2)
```

Departures from normality are seen as a systematic departure of the points from a straight line on the Q-Q plot.Interpreting Q-Q plots is somewhat visceral. Here are the common situations.

Description: interpretation

* All but a few points fall on a line: outliers in the data
* Left end of pattern is below the line; right end of pattern is above the line: long tails at both ends of the distribution
* Left end of pattern is above the line; right end of pattern is below the line: short tails at both ends of the data distribution
* Curved pattern with slope increasing from left to right: data is skewed to the right
* Curved pattern with slope decreasing from left to right: data is skewed to the left
* Staircase pattern (plateaus and gaps): data have been rounded or are discrete

# Model Transformations

Square root
  * Limitations
    + Cannot be applied to negative numbers
    + Transforms numbers < 1 and > 1 in different ways
    
Logarithm
  * Based 10 or based $e$ (natural logarithm)
  * Appropriate
    + When the SD of the residuals is directly proportional to the fitted values (and not to some power of the fitted values)
    + When the relationship is close to exponential
  * Limitations
    + How to transform zero values?
      - Add a constant such as 1 or 0.00001
      - Remove zero values from analysis
      
Others
  *Box-Cox Transformation
  (http://stackoverflow.com/questions/26617587/finding-optimal-lambda-for-box-cox-transform-in-r)

Be careful transforming count data. Use a different type of regression.

# Regression models make a series of assumptions.

* Before accepting a model you need to examine those assumptions to make sure they are tenable.
* The model fit may be excellent, but you can't be sure your conclusions are correct unless you evaluate the tenability of the assumptions.
* The key to evaluating the regression assumptions are the model residuals.

# Regression modeling is statistical control.

* You often want to do more than just summarize the relationship between variables.
* Regression provides a strategy to control for effects of an explanatory variable to see what is left over.
* These left-overs or residuals are interpreted as "controlled observations" (e.g. percent income controlling for percent graduates).

# Outliers can distort regression results or can be interesting on their own.

* Inspect scatter plots and plots of residuals to determine whether there are any unusual values that might unduly influence the regression line.
* If you find outliers, re-fit the regression model sans those observations and compare results.
* Regardless of how you decide to handle the outliers, let your readers know about them.

# Multiple Regression 


We regress the response variable onto the explanatory variables. And when we describe what we did we don't say we performed a regression of my variables or we regressed y AND x. Or we regressed y with respect to x. These are ambiguous at best.

Examples: Regression of ozone concentration on air temperature, wind speed, and radiation; regression of voting preference on income, education and location; regression of exam scores on homework grade and seat location; regression of mammal brain size on body weight and gestation period; regression of mortality rates on family income, race, and location.

Everything from bi-variate regression carries over to multiple regression. Each explanatory variable contributes a term to the model. There is a slope coefficient for each explanatory variable. That slope coefficient multiplied by the corresponding explanatory variable is called a term. 

With one explanatory variable the regression model is described by a straight line. With two explanatory variables it is described by a flat plane.  

To see this we graph a three dimensional scatter plot with the `scatterplot3d()` from the **scatterplot3d** package and add the regression plane. The example comes from the `trees` data frame. We start with a bi-variate regression. We regress lumber volume (`Volume`) on tree girth (`Girth`) and add the model as a line on the scatter plot.

```{r}
model1 = lm(Volume ~ Girth, data = trees)
plot(trees$Volume ~ trees$Girth, pch = 16, 
     xlab = "Tree diameter (in)", 
     ylab = "Timber volume (cubic ft)")
abline(model1)
```

The `plot()` method and `abline()` are base R graphics functions. Next we regress lumber volume (`Volume`) on girth (`Girth`) and height (`Height`). Use the `scatterplot3d` function to plot the scatter of points in pseudo-3d. The resulting `plane3d` function takes the regression model and adds the plane (with a grid).
```{r}
model2 = lm(Volume ~ Girth + Height, data = trees)
s3d = scatterplot3d(trees, angle = 55, scale.y = .7, pch = 16, 
                    xlab = "Tree diameter (in)", 
                    zlab = "Timber volume (cubic ft)",
                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

We can see that lumber volume increases with tree diameter and tree height. By changing the view angle (`angle =` argument) we can see that some observations are above the plane and some are below.
```{r, eval=FALSE}
s3d = scatterplot3d(trees, angle = 32, scale.y = .7, pch = 16, 
                    xlab = "Tree diameter (in)", 
                    zlab = "Timber volume (cubic ft)",
                    ylab = "Tree height (ft)")
s3d$plane3d(model2)
```

The distance along the vertical axis from the observation to the model plane is the residual.

With more than two explanatory variables the model is described by a hyper plane but the math is the same. The multiple regression model is given by: 

$$
y_i = \beta_0 + \beta_1 \cdot x_{i1} + \beta_2 \cdot x_{i2} + \cdots + \beta_p \cdot x_{ip} + \varepsilon_i
$$

The explanatory variables (there are $p$ of them) are written with a double subscript to indicate the observation number (1st subscript) and the variable number (2nd subscript).Each explanatory variable gets a coefficient, so there are $p+1$ of them when we include $\beta_0$. The $_i$ on the explanatory variables indicates the change in the observations (i.e. $i^th$ row in the dataset).There is a single response variable $y_i$ and a single set of residuals $\varepsilon_i$. The residuals are assumed to be described by a set of normal distributions each centered on zero and having the same variance ($\sigma^2$).

Fit a multiple regression model:

```{r}
PC.df = read.table("http://myweb.fsu.edu/jelsner/temp/data/PetrolConsumption.txt", header = TRUE)
head(PC.df)
```

```{r}
ggpairs(PC.df)
```


We fit a multiple regression model to the petrol data as follows:
```{r}
model1 = lm(Petrol.Consumption ~ Prop.DL + Pavement + 
            Avg.Inc + Petrol.Tax, data = PC.df)
model1
```

Gasoline consumption increases with the proportion of drivers and decreases with the amount of pavement, average income and gas tax. Why might gas consumption decrease with increasing income?

The equation for the model is: Average petrol consumption [millions of gallons] = 377.3 + 1336 * Prop.DL – 0.0024 * Pavement – 0.06659 * Avg.Inc – 34.79 * Petrol.Tax

The statistically significant variables in explaining petrol consumption are found by looking at the table of coefficients output from the `summary()` function.
```{r}
summary(model1)
```
The larger the p value the less important it is for the model
estimate: same units as responnse divided by explanatory


We see from the table of coefficients that `Pavement` is not significant.

The null hypothesis is that the variable is NOT important to the model. The $p$-value is evidence in support of the null hypothesis. The larger the $p$-value the more evidence you have that the variable is not important to the model.  

Magnitudes of the coefficient estimates (effect sizes) cannot be compared directly because they have different units. A regression coefficient has units of the response variable divided by the units of the explanatory variable.

However the magnitude of the $t$-values are comparable. They are coefficients that have been *standardized* by dividing by the corresponding standard error. Accordingly, `Avg.Inc` is more more important than Petrol.Tax even though .0666 is much smaller than 34.79.

The model says that for every 1 percentage point increase in the proportion of drivers (to overall population), the mean petrol consumption increases by 1.34 billion gallons assuming `Pavement`, `Avg.Inc` and `Petrol.Tax` are constant. 

For every 1 cent/gallon increase in taxes, mean petrol consumption decreases by 34.8 million gallons assuming the three other variables are held constant.

The order of the explanatory variables does not change the magnitude or the sign of the slope coefficients.
```{r}
summary(lm(Petrol.Consumption ~ Pavement + Petrol.Tax + 
           Avg.Inc + Prop.DL, data = PC.df))
```

`Simplify the model`

Can the model be simplified. We try a simpler model by removing `Pavement` (the variable not statistically significant).
```{r}
model2 = lm(Petrol.Consumption ~ Prop.DL + Avg.Inc + 
              Petrol.Tax, data = PC.df)
summary(model2)
```

The remaining explanatory variables in the model are all significant. The values are slightly different as the variance in the gas consumption attributed to `Pavement` is now spread across the remaining variables. Removing an explanatory variable changes the values of the parameters remaining in the model.

The proportion of the population with licenses is the most important variable as can be seen by having the largest $t$ value (in absolute value).

By removing a variable the R-squared statistic DECREASES to .675. This is always the case with a smaller model (fewer explanatory variables). Thus the R-squared statistic should not be used to compare models unless the models have the same number of explanatory variables.  

**NOTE: R squared always decreases when removing variables even if not important so adjusted R squared is much better because it increases only if the variable removed improves the model**

Model assumptions

We need to check the model assumptions. Equal variance. We saw how to check this assumption by using the `cut()` function and creating side-by-side box plots. Another way to check this assumption is to plot the standardized residuals on the vertical axis and the fitted values along the horizontal axis. Adding a smoothed curve and the y equal zero line makes it easy to see whether there is a pattern to the residuals.
```{r}
model2.df = fortify(model2)
ggplot(model2.df, aes(x = .fitted, y = .stdresid)) +
  geom_point() +
  geom_smooth() +
  geom_hline(yintercept = 0)
```

In this case: no.

Normality. First we use the `sm.density()` function.
```{r}
res = residuals(model2)
sm.density(res, xlab = "Model Residuals", model = "Normal")
```

We also examine a quantile-normal plot.
```{r}
qqnorm(model2$residuals)
qqline(model2$residuals)
```

With either of these plots we see some evidence against normality and constant variance. The fact that the residuals do not exactly follow a normal distribution lowers the confidence we can place on our inferences.

To improve model adequacy we might transform the response variable or use a weighted regression model. We will talk about transforming a response variable on Thursday and weighted regression is considered later in the context of geographic regression.

# Interaction between two continuous variables in a Multiple Regression

```{r}
scores.df = read.dta("https://stats.idre.ucla.edu/stat/data/hsbdemo.dta")
```

The data includes the following measurements on 200 students: 

* Standardized reading test score 
* Standardized writing test score
* Standardized social economic status (socst)  

We regress reading score onto writing score and socio-economic status.
```{r}
model1 = lm(read ~ write + socst, 
            data = scores.df)
summary(model1)
```

We find writing skill helps reading skill as does social economic status.

But maybe the story is more complicated. Reading and writing scores depend on school quality, which is a function of social economic status. Thus it would be surprising if the RELATIONSHIP between reading and writing scores is independent of social economic status.

To check this possibility, first create a new categorical variable and then make a conditional scatter plot.
```{r}
scores.df2 = scores.df %>%
  mutate(socst2 = cut(socst, 2))
ggplot(scores.df2, aes(y = read, x = write, color = factor(socst2))) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  xlab("Writing Score") +
  ylab("Reading Score")
```

The slopes cross so we should check for an interaction.

Adding an interaction term tells another story.
```{r}
model2 = lm(read ~ write + socst + write:socst, 
            data = scores.df)
summary(model2)
```

The interaction term is significant. How do we interpret it? As social economic status increases the relationship between writing and reading skill increases (larger slope). Mathematically
$$
y_i = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 (x_1 \cdot x_2) + \varepsilon_i
$$

Let $x_2 = c$ (let social economic status be constant), then the effect of $x_1$ is
$$
y_i = \beta_0 + \beta_1 x_1 + \beta_2 \cdot c + \beta_3 (x_1 \cdot c) + \varepsilon_i \\
y_i = (\beta_0 + c \beta_2) + (\beta_1 + c \cdot \beta_3) x_1 + \varepsilon_i
$$

avg reading = 49.8 - .362 * write - .398 * socst + .015 * write * socst + e

* When socst = 26, a one unit increase in writing produces a (-.362 + 26 * .015) = .028 unit increase in avg reading.
* When socst = 71, a one unit increase in writing produces a (-.362 + 71 * .015) = .703 unit increase in avg reading.

A plot of the conditional coefficient is made with the `interplot()` function from the **interplot** package. The model object is given along with the names of the interacting variables as character strings. The function uses the grammar of graphics structure so we can add layers as before.
```{r}
interplot(m = model2, var1 = "socst", var2 = "write") +
    geom_hline(yintercept = 0, linetype = "dashed") +
    xlab("Social Economic Status") +
    ylab("Coefficient on Writing Score") +
    theme_bw() 
```
If a non interactive model it would be a flat line. Therefore, we have a significant interaction.


The plot is a visualization of the interaction. There is a significant upward trend on the writing score coefficient as a function of social economic status. The 95% uncertainty band on the interaction is given in gray.

The coefficient of writing is clearly conditioned by social economic status. Note: This also means that the coefficient of social economic status is conditioned by writing in a symmetric way.

# Collinearity

This is an indication of collinearity. The large between-variable correlation leads to a model that may not make physical sense. The rule is that when the correlation between two explanatory variables exceeds .6, collinearity can be a problem.  

When explanatory variables have large correlation then estimates of the model parameters are not precise. A model with imprecise parameter estimates is not useful. The best way to proceed in this situation is to reduce the set of explanatory variables. 

Use the `cor()` function. 


# Akaike Information Criterion

Another useful statistic in this regard is called AIC (Akaike Information Criterion). AIC is more general and can be used with models that are fit with methods other than least-squares minimization.

AIC is rooted in information theory. Values of AIC provide a means for model selection in a relative sense. If all candidate models fit poorly the AIC is not much help.

AIC = $2 k - 2 \log(L)$, where k is the number of model parameters (1 + number of explanatory variables) and L is the highest value from the likelihood function. The likelihood function (likelihood) is the probability of your data given the model.

AIC rewards goodness of fit, but includes a penalty that is an increasing function of the number of estimated parameters. Given a set of candidate models for our data, we should choose the one with the lowest AIC. 

use the `drop1()` function, which takes a regression model object and returns a table showing what happens when each variable is removed from the model.
```{r}
modelF = lm(Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax, 
            data = PC.df)
drop1(modelF)
```

The first row also gives the AIC value for the full model. Here 407.37. The model deviance has units of the response variable. The AIC has no units.

The next row tells us what happens to the RSS if the explanatory variable Prop.DL is removed from the full model. If we drop the `Prop.DL` variable from the full model, the RSS increases to 401405 (from 189050) and we gain one degree of freedom (less bias).

Said another way, the value of 212355 in the `Sum of Sq` column indicates how much the variable `Prop.DL` is worth to the model. By removing it the RSS increases from 189050 to 401405; a difference of 212355.

Apparently that is too much of an increase in RSS for the gain of only 1 degree of freedom as the AIC increases from 407 to 442. Said another way, the AIC is 407 for the full model. In comparison, the AIC is 442 for a model that is otherwise the same but missing `Prop.DL`.

The next row gives the same information about the `Pavement` variable. That is, what happens to the RSS from the full model when the variable `Pavement` is removed.

Here the RSS increases by 2252 to 191302 (the residuals on average get a bit larger), but this increase is worth it as it only costs one degree of freedom. This is indicated by a reduction in the AIC to so 405.94. 

The AIC column keeps score. The AIC for the model having all the explanatory variables is 407.37. A model without `Prop.DL` has an AIC of 441.51, which is LARGER than the AIC for the full model so we keep `Prop.DL` in the model.

On the other hand, a model without `Pavement` has an AIC of 405.94, which is SMALLER than the AIC for the full model so we remove `Pavement` from the model.

So we simply compare the AIC values for each variable against the AIC value for the full model. If the AIC value for a variable is less than the AIC for the full model then the trade-off is in favor of removing it.


# Removing Variables from a model: 

When we have all variables in the model the R squared value is maximized. The cost of each variable is model bias. The model is biased toward the particular set of data. That is, it may not generalize to another sample of data. Therefore it is less useful for inference (e.g., prediction).

By dropping a variable we pick up one degree of freedom thus making the model less biased. But by dropping a variable we decrease the variance explained (R squared value gets smaller).

Simply, we favor a model that has the largest adjusted R squared. If the adjusted R squared gets smaller after we remove a variable, then we put it back in the model.

Another useful statistic is called AIC (Akaike Information Criterion). AIC is more general and can be used with models that are fit with methods other than least-squares minimization.

AIC is rooted in information theory. Values of AIC provide a means for model selection in a relative sense. If all candidate models fit poorly the AIC is not much help.

AIC = $2 k - 2 \log(L)$, where k is the number of model parameters (1 + number of explanatory variables) and L is the highest value from the likelihood function. The likelihood function (likelihood) is the probability of your data given the model.

AIC rewards goodness of fit with low values for AIC, but includes a penalty that is an increasing function of the number of explanatory variables. Given a set of candidate models for our data, we should choose the one with the lowest AIC. 

```{r}
PC.df = read.table("http://myweb.fsu.edu/jelsner/temp/data/PetrolConsumption.txt", 
                   header = TRUE)

modelFull = lm(Petrol.Consumption ~ Prop.DL + Pavement + Avg.Inc + Petrol.Tax, 
               data = PC.df)

drop1(modelFull)
```

The first row also gives the AIC value for the full model. Here 407.37. Model deviance has units of the response variable. The AIC has no units (dimensionless).The next row tells us what happens to the RSS if the explanatory variable `Prop.DL` is removed from the full model. If we drop the `Prop.DL` variable from the full model, the RSS increases to 401405 (from 189050) and we gain one degree of freedom (less bias).Said another way, the value of 212355 in the `Sum of Sq` column in the `Prop.DL` row indicates how much the variable `Prop.DL` is worth to the model. By removing it, RSS increases from 189050 to 401405; a difference of 212355.Apparently that is too much of an increase in RSS for the gain of only one degree of freedom as the AIC increases from 407 to 442. Said another way, the AIC is 407.37 for the full model. In comparison, the AIC is 441.51 for a model that is otherwise the same but missing `Prop.DL`.
    ** if you remove prop dl the aic goes up. not good. **

The next row gives the same information about the `Pavement` variable. That is, what happens to the RSS from the full model when the variable `Pavement` is removed.
  ** remove pavement from the model**

Here the RSS increases by 2252 to 191302 (the residuals on average get a bit larger), but this increase is worth it as it only costs one degree of freedom. This is indicated by a reduction in the AIC to so 405.94. The AIC column keeps score. The AIC for the model having all the explanatory variables is 407.37. A model without `Prop.DL` has an AIC of 441.51, which is LARGER than the AIC for the full model, so we keep `Prop.DL` in the model.On the other hand, a model without `Pavement` has an AIC of 405.94, which is SMALLER than the AIC for the full model, so we remove `Pavement` from the model. So we simply compare the AIC values for each variable against the AIC value for the full model. If the AIC value for a variable is less than the AIC for the full model then the trade-off is in favor of removing it.

# Stepwise Regression

Stepwise regression is a procedure (not a model) to automate this procedure of selecting a final model. It is method for finding the best model from a set of candidate models when the models are nested. It is useful for sifting through a large number of explanatory variables.

Stepwise regression can be done by backward deletion of variables or by forward selection of the variables. Backward deletion amounts to automating the `drop1()` function and forward selection amounts to automating the `add1()` function.

Both `drop1()` and `add1()` use the AIC as a criterion for choosing. To see it work, return to your full model explaining gasoline consumption.
```{r}
step(modelFull)
```

Another example: 

We are interested in regressing stack loss onto air flow, water temperature, and acid concentration.
```{r}
modelFull = lm(stack.loss ~ ., 
               data = stackloss)
```

The period after the tilde will include all the columns (remaining) in the data frame as explanatory variables.

```{r }
step(modelFull)
```

Backward deletion of variables is the default in the `step()` function. That is a successive application of the `drop1()` function. In the case of having a large number of explanatory variables it's a good idea to also try forward selection to see if the results are the same.

# Transforming a Response Variable: 

Example: Let's return to the pollution dataset. The data are in the file *pollute.txt*.
```{r}
pollute.df = read.table("http://myweb.fsu.edu/jelsner/temp/data/pollute.txt", 
                     header = TRUE)
head(pollute.df)
```

The data contain pollution levels for 41 cities. Interest is in a model that can predict pollution using the variables average temperature, the amount of industry, the population, the average wind speed, the amount of rain, and the number of wet days.

We start with a histogram and density plot of the response variable `Pollution`.
```{r}
ggplot(pollute.df, aes(Pollution)) + 
  geom_histogram(binwidth = 10)
```

```{r}
sm.density(pollute.df$Pollution, 
           model = "Normal")
```

The response variable `Pollution` (SO2 concentration in ppm) is not well described by a normal distribution. We can still use linear regression, but your inferences might not be strictly valid. One option is to transform the variable to make the distribution more normal. 

Raising a variable $y$ to some power is most common. Box and Cox (1964) suggested a family of transformations, called the power transformations, to make the data better conform to a normal distribution.
$$
y_\lambda' =  \frac{y^\lambda-1}{\lambda} 
$$

The particular transformation depends on the value of $\lambda$. The choice of which value to use is determined using the `boxcox()` function from the **MASS** package. The first argument is a `lm()` formula. Also needed are the data and a sequence of potential value for $\lambda$ (`lambda`). Here we are interested in the response variable `Pollution` without any explanatory variables so the model formula is `Pollution ~ 1`.
```{r}
boxcox(Pollution ~ 1, 
       data = pollute.df,
       lambda = seq(-1, 1, length = 10))
```
We want to look at pollution independent of the other variables. 


The output is a plot of the likelihood as a function of possible values for $lambda$. The likelihood represents the chance of observing the data given the tranformation and the normal distribution as the model. Choose the `lambda` that maximizes the likelihood. 

$\lambda$ is a random variable and thus the 95% uncertainty interval provides a range of values. Here the log-likelihood is maximized between -.75 and .2.

Create a new variable (`Pollution2`) that is the transformed `Pollution` variable (with lambda = -.25 in the above formula).
```{r}
Pollution2 = pollute.df$Pollution^(-.25)
sm.density(Pollution2, model = "Normal")
```

The variable `Pollution2` can be described by a normal distribution. 

More precisely the concern is the distribution of the model residuals. It doesn't really matter the distribution of the response. The normality assumption undergriding accurate inferences made with the model is that the distribution of residuals are described by a normal distribution.

To determine the appropriate Box-Cox transformation on your response variable so that the model residuals can be described as normal use the model formula.
```{r}
boxcox(Pollution ~ Temp + Industry, 
       data = pollute.df,
       lambda = seq(-1, 1, length = 10))
```
** What is the best fittinng model where assumptions of normality are the best? boxcoxx. What is the power distribution of the dataset? the other one **

Create a transformed response variable using the Box-Cox transformation with `lambda = -.2`. Then refit the model. The transformed variable improves the assumption of linearity as well.

# Regression Trees and Random Forrests (L14)

Linear regression is a model for the conditional mean. A regression tree is an alternative way to model the conditional mean. Output from a regression tree is easier to interpret. If the response variable is categorical then the regression tree is called a classification tree (read more in Lesson 15). Regression trees are also called decision trees.A regression (decision) tree is a series of decisions with each leading to a mean response or to another decision.

A random forest algorithm addresses predictive uncertainty by making predictions over many trees (a forest). It creates a sample from the set of all years and grows a tree using data only from the sampled years. It then repeats the sampling and grows another tree. Each tree gives a prediction and the mean is taken. 

# Cross Validation (L15)

Cross-validation compares predictive skill across different statistical models. It removes noise specific to each data point and estimates how well the prediction algorithm (e.g., random forest) finds prediction rules when this coincident information is unavailable.

# Logistic Regression (L15)

Linear regression is the workhorse model for quantitative geographic research. Often however the response variable is binary (only two outcomes). 

For instance: an event occurs or it doesn't. Many research questions, especially in social and medical science involve predicting the probability that something will happen. Probability that Trump will be impeached before the end of his term. Chance that a person will contract Zika. Or the questions involve predicting proportions or percentages: Proportion of people unemployed. Percentage of lakes in the state that contain toxins.

These situations involve binary outcomes. Logistic regression statistically describes how a binary response variable is associated with a set of explanatory variables. Applications of logistic regression involve the idea of predicting a chance (or probability/percentage).

The mechanics of fitting the model to a set of data is the same as before. However, the coefficients are determined using the method of maximum likelihood. The method of maximum likelihood answers the question, what is the probability of observing the data, given the model.

The average over a set of binary responses is a number between 0 and 1. For example, generate a set of random binary responses.
```{r}
rbinom(n = 100, size = 1, prob = .2)
```

The size and probability arguments are the number of trials and the probability of success on each trial. `size = 1` is equivalent to flipping a coin.  `prob = .5` is equivalent to a fair coin. `n` is the number of responses. The average taken over all responses is the probability of success.
```{r}
mean(rbinom(n = 100, size = 1, prob = .5))
mean(rbinom(n = 100, size = 1, prob = .9))
```

A regression model predicts the mean response. A logistic regression model predicts the probability. For example, the probability of lung cancer given smoking habit and age.

Limitation of linear regression when the response variable is binary

Probabilities are bounded between 0 and 1, but a linear regression line is not.

Example: Consider the data collected from the recovered solid booster rockets of the 23 previous shuttle flights (before the Challenger Space Shuttle disaster in 1986). The temperature in F at the time of liftoff and whether there was (1) or was not (0) damage to the O-rings on the booster.
```{r}
Temperature = c(66, 70, 69, 68, 67, 72, 73, 70, 57, 63, 70, 78, 67, 53, 67, 75, 70, 81, 76, 79, 75, 76, 58)
Damage = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1)
```

In this setting, which variable is the response?

A scatter plot of damage and temperature with the best fit line is obtained by typing:
```{r}
df = data.frame(Damage, Temperature)
ggplot(df, aes(x = Temperature, y = Damage)) +
         geom_point() +
         geom_smooth(method = lm)
```

The plot shows lower chance of O-ring damage when temperatures are higher, but a straight line does not capture the relationship very well.

Regress damage on temperature. Is the model significant?
```{r}
lrm = lm(Damage ~ Temperature, data = df)
summary(lrm)
```

Yes, temperature is significant in explaining O-ring damage. Mean Damage = 2.905 – .037 x Temp. For every one degree F decrease in temperature the chance of damage increases by about 4 percentage points.

The mean damage can be interpreted as the probability of damage for the given temperature. What is the predicted chance of damage if the temperature is 77F, 65F, and 51F?
```{r}
predict(lrm, data.frame(Temperature = c(77, 65, 51))) * 100
```

Although this appears to be a reasonable prediction equation, it leads to nonsensical forecasts. For example, what is the predicted chance of damage when the temperature is 85F and when it is 45F?
```{r}
predict(lrm, data.frame(Temperature = c(85, 45))) * 100
```

The bounded nature of probabilities is not captured by a linear regression model.

The inadequacy of the linear regression model for these data as shown in the pattern of the residuals.
```{r}
lrm.df = fortify(lrm)
ggplot(lrm.df, aes(x = .fitted, y = .stdresid)) +
  geom_point() +
  geom_hline(yintercept = 0)
```

Rather than exhibiting a random scatter of points, the plot of residuals versus temperature shows a pattern of points along two slanted lines. We need to consider a more appropriate model for binary data.

### A generalized linear model

A regression model has a response function and a regression structure. The response function is specified on the left side of the equation and the regression structure on the right.

With linear regression the response function is the *identity* function of the mean of y given x ($\mu(y|x)$) and the regression structure is a weighted sum of the explanatory variables plus an intercept. The weights are the regression coefficients.
$$
\mu(y|x) = \hat \beta_0 + \hat \beta_1  x
$$

Logistic regression generalizes the linear regression model by changing the response function to be the *logit* function of the mean of y given x ($\pi(y|x)$).  

The logit function is the logarithm of the mean divided by one minus the mean.
$$
\hbox{logit}(\pi) = \log\left(\frac{\pi}{1 - \pi}\right).
$$
Since the mean of a set of binary outcomes is a probability the symbol $pi$ (rather than $\mu$) is used for the mean. So the logit is the logarithm of the ratio of two probabilities: probability it will happen divided by the probability it won't happen. 

The logit function is the logarithm of the odds when the mean is a probability. Odds are expressed as the probability that it will happen divided by the probability that it won't happen. So we write odds as for:against. If there is a 2:1 odds of something happening, then the probability it will happen is 2/(1 + 2) or 2/3 (67%).

Conversely if there is a 67% chance that something will happen then the odds of it happening are .67/(1-.67) = 2 and expressed as 2 to 1 or 2:1. The inverse of the logit function is called the logistic function. With logit($\pi$) = $z$, 
$$
\pi = \frac{\exp(z)}{1 + \exp(z)} = \frac{1}{1 + \exp(-z)}.
$$

To see it graphically, type
```{r}
curve(1/(1 + exp(-x)), 
      from = -6, to = 6, 
      col = "red", ylab = "f(z)", xlab = "z")
abline(h = 1, lty = 2)
abline(h = 0, lty = 2)
```

The "input" is z and the "output" is f(z). The logistic function takes as input any value from negative infinity to positive infinity and returns a value between 0 and 1.

The variable z can be thought of as the exposure to some set of risk factors, while f(z) represents the chance of a particular outcome given the exposure.

The variable z is a measure of the total contribution (weighted sum) of all the risk factors and is known as the logit. It is defined as
$$
z = \beta_0 + \beta_1  x_1 + \beta_2  x_2 + \ldots + \beta_p x_p
$$
where the parameters are $\beta_0$,..., $\beta_p$ and the risk factors are $x_1$,..., $x_p$. The intercept $\beta_0$ is the value of $z$ when there is no risk (base rate).

Each regression coefficient describes the size of the contribution of that risk factor. A positive regression coefficient means that that risk factor increases the probability of the outcome. A negative regression coefficient means that that risk factor decreases the probability of that outcome.

A large regression coefficient means that that risk factor strongly influences the probability of that outcome; while a near-zero regression coefficient means that that risk factor has little influence on the probability of that outcome.

Logistic regression is the quantitative way to describe the relationship between one or more risk factors and a binary outcome (such as death).

Example: The Donner Party

The Donner and Reed families from Springfield, Illinois headed to California in 1846. The group, which became know as the 'Donner Party', was stranded in the Sierra Nevada mountains by heavy late October snows. By the time the last survivor was rescued in April 1847, 40 of the 87 members had died from famine and exposure to the cold.
 
The data file (*DonnerParty.txt*) contains a list of individuals in the Donner Party. The list contains the `Name`, `Sex`, `Age`, and `Survival` status of all individuals 15 years or older.

For any given age, were the odds of survival greater for women?

Get the data.
```{r}
DP.df = read.table("http://myweb.fsu.edu/jelsner/temp/data/DonnerParty.txt", 
                   header = TRUE)
head(DP.df)
```

Look at the data. How many men and how many women were in the party?
```{r}
table(DP.df$Sex)
```

How many survived?
```{r}
table(DP.df$Survival)
```

What was the average age in the party?
```{r}
mean(DP.df$Age)
```

Examine bi-variate relationships.
```{r}
ggplot(DP.df, aes(x = Survival, y = Age)) + 
  geom_boxplot()
```

Age appears to be a factor in explaining survivability. Younger members of the party appear to have had a better chance of survival. What about sex as an explanatory variable?
```{r}
table(DP.df$Sex, DP.df$Survival)
```

Of the 15 women, 5 perished and 10 survived. This compares with 20 deaths and 10 survivors among the men. It appears that sex is also important. Relatively more women survived than men. Logistic regression quantifies these associations (sex and age on survivability) in terms of probabilities.

To fit a logistic model to the data type:
```{r}
logrm = glm(Survival ~ Age + Sex, data = DP.df, 
            family = binomial)
summary(logrm)
```

family = normal would be annomral regression
glm is a genneralization...for logistic regression. 

The relative small $p$-values (< 0.05) on the regression coefficients confirm that both `Age` and `Sex` are important in explaining survivability. 

Caution: the estimate on the binary explanatory variable gets suffixed with the label used as the 1. Here `male` is used as 1 and `female` 0 (alphabetical).

Whether or not the logistic model is statistically significant is based on the deviance. The smaller the residual deviance, the better the logistic model fits the data. The summary output shows a null deviance of 61.827 on 44 degrees of freedom. This is the deviance of a model with a fixed survival probability (44%; 20 out of 45 survived) independent of explanatory variables.

The residual deviance is 51.256 on 42 degrees of freedom. This is the deviance for the model involving `Age` and `Sex` as explanatory variables (full model). The change in deviance from the null to the full model is:

61.827 - 51.256 = 10.571 on 44 - 42 = 2 degrees of freedom. Comparing this drop in deviance to a chi-squared value.
```{r}
pchisq(10.571, 2, lower.tail = FALSE)
```

This is the $p$-value for the model as a whole. Since it is less than .01 so we can state that there is convincing evidence that the model is significant in explaining survivability.

We write the logistic regression model, where $\pi$ represents survival probability, as
$$
\hbox{logit}(\pi) = 3.23 - .0782 \times \hbox{Age} - 1.597 \times \hbox{Sex}
$$
In words: The logarithm of the odds of survival equals 3.23 minus .078 times Age - 1.59 times Sex.

We get confidence intervals on the parameter values using an extractor function.
```{r}
confint(logrm)
```

*Interpretion*

To compare the survivability (odds of surviving) of a party member who is 50 with survivability of a member who is only 20, the model tells us that the odds ratio of a 50-year old surviving relative to a 20-year old is
$$
\exp(-.0782 \times (50 - 20))
$$
Or

exp(-.0782*(50 - 20)) = 0.0958 ~ 0.1

So, the odds of a 50 year-old surviving were about 1/10 the odds of a 20 year-old. The model makes sense and is quantitative. The model tells us that the odds of a 20-year old surviving is 10 times the odds of a 50-year old surviving regardless of whether it is a man or woman.

If an explanatory variable increases by 1 unit while the other explanatory variables remain constant, the odds will change by a multiplicative factor given by the exponent of the coefficient.

The odds of a woman (`Sex = 0`) surviving relative to the odds of a man (`Sex = 1`) of the same age is

exp(-1.597*(0 - 1)) = 4.94

Thus, the survival odds of a women are about 5x the survival odds of a man of the same age.

*Maximizing the likelihood*

Consider the following alternative logistic model for the Donner party survivability. Here by alternative, I mean that the parameters are a bit different.

logit(pi) = 3.5 - .08 * Age - 1.25 * Sex

The model is the same except with different parameter values: (3.5, -.08, -1.25) instead of (3.23, -.078, -1.60).

Now, Antoine gets coded as `Age1 = 23`, `Sex1 = 1`. So we plug his attributes into the model and get

logit(pi1) = 3.50 - (.08 * 23) - (1.25 * 1) =  0.41

And, pi1 = exp(.41)/[1 + exp(.41)] = 0.601

Thus the model tells us that Antoine had a 60.1% chance of surviving given his age and sex.

Continuing down the list, we find that Mary Breen, a 40-yr-old women (Age2 = 40, Sex2 = 0) has a survival probability of 0.574 according to the model.

In this way we can find the probability of survival for all party members, given the model.  

Then the probability over a combination of outcomes is calculated by multiplying individual probabilities under the assumption the probabilities are independent (stories of cannibalism notwithstanding).

For example, the probability that everyone survives is
0.601 * 0.574 * ... * 0.679 = 6.678826e-24

Similarly we can find the probability over any combination of people surviving and dying.  One of these combinations is the probability over all members surviving who actually did survive and all members perishing who actually did perish.

(1-0.601) * (0.574) * ... * (0.679) = 4.383823e-12

This number is very small, but not nearly as small as before.

These probabilities are based on the particular model (that is the particular values of the parameters). The probability of the known outcome calculated as 4.38e-12 for parameters having a value of (3.50, -0.08, -1.25) is called the model's likelihood.

The same procedure can be used to calculate likelihoods of the observed outcome under different models. One set of parameter values is more likely than another set if it gives the observed outcome a higher probability of occurrence.

In fact, one set of parameter values will give the maximum likelihood. As a guide to selecting the parameters of the model, the likelihood principle says: "choose the parameter values that assign the highest probability to the observed outcome."  Math involving iterative computations provides a direct way to find the maximum likelihood estimates.

To see the probability of survival assigned to each individual in the Donner party from the logistic regression model type
```{r}
logrm$fitted
```

Thus the values of (3.23, -.078, -1.60) are the maximum likelihood estimates (MLE).

*Prediction*

To predict the survival probability of a 50-year old man, type
```{r}
predict(logrm, data.frame(Age = 50, Sex = factor("male")), 
        type = "response")
```

Since the variable `Sex` is a factor, it must be specified as a factor in the `predict()` function.

Also, with a generalized linear model we need to specify what we want the predicted value to be. By default it will be the logarithm of the odds ratio (logit). To turn this value into a probability (apply the logistic function) we specify `type = "response"`.

We see that based on the data AND the model, we would predict a survival probability of 9.3% for a 50-year old man. What is the survival probability for a 50-year old woman? A 25-year old woman?

Although we can't get a 95% confidence interval on the predicted values, we can tell the predict function to output the standard error on the prediction by including the argument `se.fit = TRUE`.

*Model adequacy*

Recall the diagnostics used linear regression for assessing model adequacy involved plots of the model residuals. Similar diagnostics are available for logistic regression, but they are often more difficult to interpret.

Linearity in log-odds: A scatter plot of the dependent variable versus the independent variable is not informative since the value of the dependent variable is 0 or 1.

A "smoothed" scatter plot can average across adjacent values. Smoothing can be accomplished using a locally weighted scatter plot smoothing function. This procedure regresses the dependent variable on the independent variable using only a small amount of data near the point. This procedure is repeated to generate a smooth curve.

Using the Donnor Party, information we will first convert survival from a factor to a numeric value.  Second, we will fit a loess curve to the relationship between age and survival.  Third, we will logistically transform the probabilities.
```{r}
DP.df$Survival2 = as.integer(DP.df$Survival) - 1
lfit = loess(DP.df$Survival2 ~ DP.df$Age)

DP.df$lgpred = log(predict(lfit)/(1 - predict(lfit))) 

ggplot(DP.df, aes(x = Age, y = lgpred)) + 
  geom_point() + 
  geom_smooth(method = lm)
```

Residual deviance: Returning to the output from summary(logrm), we find that the residual deviance is 51.256 on 42 degrees of freedom. A p-value from a chi-squared distribution with this quantile value and this dof is evidence in favor of the null hypothesis that the model is adequate.  A small $p$-value is evidence AGAINST model adequacy.

Type
```{r}
pchisq(51.256, 42, lower.tail = FALSE)
```

Since the $p$-value exceeds 0.15 we fail to reject the null hypothesis that the `model is adequate` and report that there is no compelling evidence that the model can be improved.

# Influential observations (L16)

Cook's distance indicates how important an observation is to the model. Specifically it measures the amount of change in the regression coefficients when the observation is excluded from the model. 

If the amount of change is large when an observation is removed, then the observation is said to be influential to the model and the observation will have a large Cook's distance.

Recall the data on the Space Shuttle O-ring damage and launch time temperature. The recovered booster rockets were checked for damage to the rings. A damage rocket booster is coded as 1.
```{r}
Temperature = c(66, 70, 69, 68, 67, 72, 73, 70, 57, 63, 70, 78, 67, 53, 67,
         75, 70, 81, 76, 79, 75, 76, 58)
Damage = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 
           0, 1, 0, 1)
```

A logistic regression model is used to predict the probability of damage given the temperature.

The setup reminds me of survivorship bias. During World War II, the statistician Abraham Wald took survivorship bias into his calculations when considering how to minimize bomber losses to enemy fire. Researchers conducted a study of the damage done to aircraft returning from missions, and recommended that armor be added to the areas showing the most damage. 

Wald noted that the study only considered the aircraft that had survived their missions--the bombers shot down were not present for the damage assessment. The holes in the returning aircraft represented areas where a bomber could take damage and still return home safely. Wald proposed that the Navy instead reinforce the areas where the returning aircraft were unscathed, since those were the areas that, if hit, would cause the plane to be lost. 

Lesson: the most important part of a study might be what data where not available.

Fit a logistic regression model to the data.
```{r}
logrm = glm(Damage ~ Temperature, 
            family = binomial)
summary(logrm)
```

We interpret the coefficient on the temperature term to mean that damage is 2.5 times more likely on average at 65F than at 69F.
```{r}
round(exp(-.2322 * (65 - 69)), 1)
```

With generalized linear models, model adequacy is determined by the residual deviance. No plots are needed. 

The residual deviance is 20.315 on 21 degrees of freedom. A $p$-value from a $\chi^2$ distribution with this quantile value and this many degrees of freedom is evidence in favor of the null hypothesis that the model IS adequate.
```{r}
pchisq(20.315, 21, lower.tail = FALSE)
```

The large $p$-value gives you no reason to question model adequacy.

To see what observation is most influential to the regression model use the `fortify()` function (**ggplot2** package) on the model object then list the rows of the resulting data frame in descending order of Cook's distance (`.cooksd`).
```{r}
fortify(logrm) %>%
    arrange(desc(.cooksd))
```

# Receiver operating characterstic (ROC) curve

There is no goodness-of-fit statistic in a logistic regression. Some packages compute a pseudo $R^2$ value but it does not indicate goodness-of-fit.

Instead the receiver operating characteristic (ROC) curve is used. It measures how well the logistic regression model discriminates the binary outcomes. It monitors the trade-off between sensitivity and specificity.

*Sensitivity* is the probability the model predicts a response as 'positive' given that it is positive (Y = 1). In words, the sensitivity is the proportion of true positives. Conversely the *specificity* is the probability the model predicts a response as 'negative' given that it is negative (Y = 0). A perfect model has a 100% sensitivity and 100% specificity. 

Use the logistic regression model to calculate fitted probabilities for each observation. Then select a cut-point c to classify the proportion of responses with a fitted probability above c as positive and those at or below it as negative. 

Increasing the value of the cut-point c means fewer responses will be predicted as positive and more will be predicted as negative. There is a trade-off between sensitivity and specificity. The ROC curve is a plot of the values of sensitivity against one minus specificity, as the value of the cut-point c is increased from 0 through to 1.

To see this create an ROC curve for the O-ring damage on launch temperature logistic regression object. First extract the fitted O-ring probabilities at the observed temperatures.
```{r}
predpr = predict(logrm, 
                 type = "response")
Temperature = c(66, 70, 69, 68, 67, 72, 73, 70, 57, 63, 70, 78, 67, 53, 67,
         75, 70, 81, 76, 79, 75, 76, 58)
Damage = c(0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 
           0, 1, 0, 1)
```

The `roc()` function from the **pROC** package generate a roc object. The syntax is to specify a regression type equation with the response y on the left hand side and the object containing the fitted probabilities on the right hand side.
```{r}
roccurve = roc(Damage ~ predpr)
plot(roccurve)
```

The sensitivity increases upward and specificity increases leftward. A curve that gets close to the upper-left corner of the plot is better at discriminating outcomes. A curve that lies along the diagonal has no discrimination ability.

A popular way of summarizing the ability of a model to discriminate is the area under the ROC curve. The area ranges from 1, corresponding to perfect discrimination, to .5 corresponding to a model with no discrimination ability.
```{r}
auc(roccurve)
```

In this case .78 indicates an acceptable level of discrimination ability.

# Poisson Regression (L16)

A Poisson distribution describes counts. Counts are things like the number of hurricanes each year. A count is how many times something happened over a given interval (time and/or space). 

The daily number of lightning strikes over Tallahassee during summer. The number of fish of a given species found in a particular river.

The Poisson distribution has the property that the variance is equal to the mean.

The density function of the Poisson distribution is
$$
p(x) = \frac{\exp(-\lambda) \times \lambda^x}{x!}
$$
where $p$ is the probability of observing $x$ counts when the average is lambda. x! is x factorial, where for example 4! = 4 * 3 * 2 * 1.

For example, over a set of 10 years the number of hurricanes affecting the coast is 
```{r}
y = c(0, 1, 5, 2, 2, 0, 1, 1, 2, 3)
mean(y)
```

Thus lambda is 1.7.

Assume the counts are described by a Poisson distribution then the probability of obtaining zero events is obtained by setting x to 0.
$$
p(0) = \exp(-\lambda)
$$

Thus the probability of zero hurricanes affecting the U.S. next year, given a mean rate of 1.7 hurricanes/year is 18%.
```{r}
exp(-1.7)
```

Note that there were 2 out of the 10 years without hurricanes (20%).

The probability of four hurricanes given a mean of 1.7 is
```{r}
exp(-1.7) * 1.7^4/(4 * 3 * 2 * 1)
```

Or 6%. Note that none of the 10 years had 4 hurricanes underscoring the importance of distributions.

The `dpois()` function evaluates the above equation for any count of interest. The probability of observing exactly one hurricane when the rate is 1.7 hurricanes per year is
```{r}
dpois(4, lambda = 1.7)
```

Or the probability of five hurricanes when the rate is 1.7 expressed in percent.
```{r}
dpois(5, lambda = 1.7) * 100
```

To answer the question, what is the probability of one or fewer hurricanes use the cumulative probability function `ppois()`.
```{r}
ppois(1, lambda = 1.7)
```

The probability of more than one hurricane is
```{r}
ppois(1, lambda = 1.7, lower.tail = FALSE)
```

To simulate 100 seasons of hurricane strikes on the U.S. coast with a rate that is commensurate to the long-term rate of 1.7 hurricanes per year, type
```{r}
rpois(100, lambda = 1.7)
```

To see the distribution
```{r}
table(rpois(100, lambda = 1.7))
barplot(table(rpois(100, lambda = 1.7)))
```

# Poisson regression model

What if the rate of hurricanes depends on climate variables? Note the way the question is worded. Interest is on the rate of hurricanes. Given the rate the counts follow a Poisson distribution. 

We do not model counts directly. Poisson regression is not least squares regression on the logarithm of counts. It is a regression using the logarithm of the rate. 

It's nonlinear in the response function, but linear in regression structure and the model coefficients are determined by the method of maximum likelihoods. This is similar to logistic regression where the response function is the logit.

Our interest here is the relationship between the climate variables and hurricanes. We do this with a regression model that specifies that the logarithm of the annual rate is linearly related to the explanatory variables.

Consider the data from *UShur.txt* which contains annual values of US hurricane counts and values of 4 explanatory variables including NAO, SOI, SST, and SSN.

```{r}
UShur = read.table("http://myweb.fsu.edu/jelsner/temp/data/UShur.txt",
                   header = TRUE)
names(UShur)
```

Let's first examine the relationships using plots. Scatter plots are not ideal. In fact, what we want is a mean count (rate) as a function of the explanatory variables.

To do this let's first break the variables into quantiles and graph a series of hurricane count box plots as a function of successive quantiles.
```{r}
NAOq = cut(UShur$NAOmj, quantile(UShur$NAOmj, seq(0, 1, .1)))
boxplot(UShur$US ~ NAOq, xlab = "NAO deciles (sd)",
        ylab = "US hurricane counts")
```

Here we see that the boxes tend to be lower with increasing quantiles of the NAO. To help visually, we can plot regression lines on various statistics from the box plot.
```{r}
xx = 1:10
x = boxplot(UShur$US ~ NAOq)
abline(lm(x$stats[5, ] ~ xx), col = "red")
abline(lm(x$stats[4, ] ~ xx), col = "blue")
abline(lm(x$stats[3, ] ~ xx), col = "green")
```

Similar plots can be done for the other explanatory variables.

### A Poisson model for hurricanes

To regress the annual hurricane count onto NAO, SOI, SST, and SSN using a Poisson regression, type
```{r}
prm = glm(US ~ NAOmj + SOIao + SSTao + SSNao, 
          data = UShur, family = "poisson")
summary(prm)
```

The table of model coefficients indicates that all 4 explanatory variables are significant ($p$-values less than .15). The most important variable is the SOI, followed by the NAO.

The value of -.197 indicates that for a one s.d. change in the NAO, the difference in the logarithm of expected counts is -.197 holding the other explanatory variables constant.  

The difference in logs is equal to the log of the quotient and therefore we interpret the coefficient as the log of the ratio of expected counts.
```{r}
exp(-.197)
```

For a one s.d. increase in the NAO the hurricane rate decreases by 1 - .82 or 18%.

The drop in deviance (deviance of the null model minus the deviance of the full model) is used to test whether the model is significant against the null hypothesis that none of the variables are useful.  

Since the model contains significant explanatory variables the model itself is significant. To see this, type:
```{r}
pchisq(189.78 - 161.15, 140 - 136, lower.tail = FALSE)
```

The residual deviance is 161.15 on 136 degrees of freedom. A p-value from a chi-squared distribution with this quantile value and this dof is evidence in favor of the null hypothesis that the model is adequate.
```{r}
pchisq(161.15, 136, lower.tail = FALSE)
```

A somewhat small $p$-value is evidence against model adequacy. One reason for an inadequate  Poisson regression model is over dispersion. Over dispersion occurs when variance is larger than the mean. Under dispersion occurs when the variance is less than the mean. Alternative: Negative binomial regression. 

# Quantile regression

Linear regression is a model for the mean of the response variable. Quantile regression is a model for the quantiles of the response variable. 

Quantiles are points taken at regular intervals from the cumulative distribution function of your data. Quantiles mark a set of ordered data into equal-sized data subsets.
```{r}
quantile(rnorm(100), prob = seq(.1, .9, .1))
```

Consider the set of per-storm maximum wind speeds.
```{r}
L = "http://myweb.fsu.edu/jelsner/temp/data/LMI.txt"
LMI.df = read.table(L, header = TRUE)
round(head(LMI.df)[c(1, 5:9, 12, 16)], 1)
```

Here `WmaxS` is the estimated fastest wind speed for each hurricane occurring in the North Atlantic Ocean (including Gulf of Mexico and Caribbean Sea).

First convert the wind speeds from the operational units of knots to the SI units of meters per second (m/s).
```{r}
LMI.df$WmaxS = LMI.df$WmaxS * .5144
```

### Quantiles and distributions

The quartiles (.25 and .75 quantiles) of the wind speed distribution are obtained by
```{r}
quantile(LMI.df$WmaxS, prob = c(.25, .75))
```

We see that 25% of the storms have a wind speeds less than 25 m/s and 75% have a wind speeds less than 46 m/s so that 50% of all storms have wind speeds between 25 and 46 m/s (interquartile range).

To see if there is a trend over time you use the linear regression. However, we note that the distribution of per cyclone maximum wind speed is not normal so it is best to transform the wind speeds. In this case a logarithmic transform is a good choice.

To examine the distributions of the raw and log-transformed wind speeds, type
```{r}
par(mfrow = c(1, 2))
boxplot(LMI.df$WmaxS)
boxplot(log(LMI.df$WmaxS))
```

To see if there is a significant trend in wind speeds over time we regress `WmaxS` onto time. Here time is year in the column labeled `Yr`.
```{r}
summary(lm(log(WmaxS) ~ Yr, data = LMI.df))
```

What do we conclude?

### Conditional quantiles

Linear regression is a model for the conditional mean (here, conditional on year). What about higher wind speeds? The potential intensity of a hurricane is a theoretical limit to how strong it can get given how warm the ocean is. Heat engine theory of tropical cyclone intensity.

The upper quartile wind speeds represent a subset of all hurricanes that should be on average closest to their potential intensity. So if you want to detect a trend in the set of strongest hurricanes you need a model for the conditional quantiles. 

Here we restrict the observations to the years since satellite data (1967).

Create side-by-side box plots as a function of year.
```{r}
par(mfrow = c(1, 1))
attach(LMI.df)
x = boxplot(WmaxS ~ factor(Yr), plot = FALSE)
boxplot(WmaxS ~ factor(Yr), 
        ylim = c(18, 90), 
        xlab = "Year", 
        ylab = "Wind Speed (m/s)")
xx = 1:44
abline(lm(x$stats[5, ] ~ xx), col = "red")
abline(lm(x$stats[4, ] ~ xx), col = "blue")
abline(lm(x$stats[3, ] ~ xx), col = "green")
detach(LMI.df)
```

The graph verifies the lack of trend in LMI for average cyclones. It also shows a tendency for the strongest storms to get stronger over time. To quantify this trend and determine significance we turn to a quantile regression model.

### Quantile regression

The functionality is available in the **quantreg** package.
```{r}
citation("quantreg")
```

We begin with the "median" regression of wind speed on year.
```{r}
summary(rq(WmaxS ~ Yr, tau = .5, 
           data = LMI.df),
           se = "iid")
```

Insignificant trend. Similar to the linear regression.

How about trends in the upper quantiles?
```{r}
summary(rq(WmaxS ~ Yr, data=LMI.df, tau=c(.75, .9, .95)), se="iid")
```

At the 75th and 95th percentiles, there is suggestive, but inconclusive evidence of a trend, but at the 90th percentile there is convincing evidence of an increase in the intensity of the strongest hurricanes in accord with MPI theory.

To show the results on a plot, type:
```{r}
model = rq(WmaxS ~ Yr, data=LMI.df, tau=seq(.1, .9, .1))
plot(summary(model, alpha=.05, se="boot"), parm = 2, pch = 19, cex=1.2,
     mar=c(5, 5, 4, 2), ylab = "Trend (m/s/yr)", xlab="Quantile",
     main = "North Atlantic Hurricanes")
```

The dots with lines indicate the trend for quantiles in the range between .1 and .9 by increments of .1.  The gray shading indicates the 95% uncertainty interval on the slope estimate. The red solid line is the least squares regression slope and the dotted lines are the 95% uncertainty interval about that estimate.

In summary, quantile regression provides a more complete picture of how the response variable is conditioned on the values of the explanatory variables.

# Log-log regression (L17)

The number of tornado casualties is related to the power of the wind and how many people are in harms way. The power of the wind is the energy dissipation over the tornado's path.
$$
E = \frac{1}{2}A_p \rho \sum_{j=0}^{J} w_j v_j^{3},
$$
where $A_p$ is the path area, $\rho$ is air density, $v_j$ is the midpoint wind speed for each damage rating $j$, and $w_j$ is the corresponding fraction of path area. 

The model is written as 
$$
C \sim  E^{\beta_E} \cdot P^{\beta_P}
$$

where $C$ is the number of casualties, $E$ is the power in watts, and $P$ is the population density in persons per square km.

Get the data.
```{r}
L = "http://myweb.fsu.edu/jelsner/temp/data/TornadoCasualties.txt"
df = read.table(L, header = TRUE)

df %>%
  filter(cas > 0) %>%
  summarize(mean(cas),
            var(cas))
```

For the set of tornadoes with at least one casualty, the mean and variance of the counts are 14.9 and 5850, respectively suggesting a negative binomial model for counts expressed as
$$
C \sim \hbox{NegBin}(\mu, n)
$$
where NegBin($\mu$, $n$) indicates that the casualty counts are described by negative binomial distributions with mean (rate) $\mu$ and size $n$.
$$
\log(\mu) = \beta_0 + \beta_P\,\log(P) + \beta_E\,\log(E)
$$
The logarithm of casualty rate (given at least one casualty) is linearly related to the logarithm of the population density ($P$) and the logarithm of the energy dissipation ($E$). 

The coefficient $\beta_P$ is the population elasticity and the coefficient $\beta_E$ is the energy elasticity and $\beta_0$ is the intercept. 
Population elasticity of tornado casualties measures the change in the casualty amount in response to a change in population for an average energy dissipation. A 1% increase in $P$ leads to an $\beta_E$% increase in the rate of casualties.

To fit the negative binomial regression we use the `glm.nb()` function from the **MASS** package.
```{r}
df2 = df %>%
         filter(cas > 0)
model = glm.nb(cas ~ log(popD) + log(AMT), 
               data = df2, link = log)
summary(model)
```

The coefficient on the population density term is .35 indicating that a doubling of the population leads to a (2^.35 - 1) * 100% [27%] increase in the number of casualties. The expanding bull's eye effect.

The coefficient on the energy dissipation term is .29 indicating that a doubling of the tornado energy leads to a 22% increase in the number of casualties.

Is the model significant? The drop in deviance is
```{r}
pchisq(1975.49 - 917.79, 871-868, lower.tail = FALSE)
```

Yes. The model is significant since the $p$-value is less than .05. The null hypothesis is that the model IS NOT significant.

Is the model adequate? The null deviance is 917.79 with 869 degrees of freedom. So.
```{r}
pchisq(917.79, 869, lower.tail = FALSE)
```

Yes. The model is adequate since the $p$-value is greater than .05. The null hypothesis is that the model IS adequate.

Note the model is at the tornado level and is conditional on at least one casualty. We could fit a model at the individual person level. Consider each individual in the path of the tornado. Interaction term. Tornado clusters.

# Local Regression (L17)

In a regression model the response variable (or some function of the response) is linked to a set of explanatory variables. In a quantile regression model the strength of this linkage depends on quantiles of the response. In a geographic regression, the strength of this linkage depends on location.

The model coefficients (slopes) in a geographic regression vary over space and can be mapped. Geographic regression shows where the explanatory variable contributes most strongly to the relationship. Geographic regression is a series of local regressions. 

Local regression is more general and applies when the response variable is modeled using neighborhood values in attribute (feature) space. Compare for example a linear regression of ozone on temperature with a local regression of ozone on temperature.
```{r}
ggplot(airquality, aes(x = Temp, y = Ozone)) +
  geom_point() + 
  geom_smooth(se = FALSE) +
  geom_smooth(method = lm, se = FALSE, color = "red")
```

The linear regression is the red line. The local regression is the blue curve. The linear regression describing the change in mean ozone with temperature over estimates the relationship at low temperature and under estimates the relationship at higher temperature. Careful: It is the relationship we are describing not the observations.

The red line has a single slope value and a single intercept value that we can obtain with the `lm()` function. The local regression has a slope and intercept for each observation. For example, with `Temp = 70`, only observations with similar temperatures are used. This is done by weighting the observations inversely based on their distance to `Temp = 70`. Observations with temperature near 70F are given more weight than those much warmer and much colder.

The change in ozone per degree change in temperature is small for temperatures less than 75F, but this change increases as temperatures rise through the 80's.

*Weights matrix*

This inverse-distance weighting is applied to each observation so we have a matrix of weights. Elements of the weights matrix W are labeled $w_{ij}$, where $i$ indexes the location and $j$ indexes the other nearby values. Values in W are determined by a function called a kernel.

What does a kernel look like?
```{r}
p = ggplot(data.frame(x = c(-3, 3)), aes(x)) + 
    stat_function(fun = function(x) 1/sqrt(2 * pi) * exp(-.5 * x^2), lwd = 2) +
    xlab("spatial distance") + 
    ylab("weight") +  
    ggtitle("kernel")
p
# Location of interest
p = p + geom_point(aes(x = 0, y = 0), col = "red", size = 3) + 
    geom_text(aes(x = 0, y = -.02, label = "x1"))
p
# The data at that location gets the most weight
p = p + geom_segment(aes(y = 1/sqrt(2 * pi) * exp(-.5 * 0^2), 
                         yend = 0, x = 0, xend = 0), 
                     linetype = 2) +
    geom_text(aes(x = .3, y = .2, label = "w11"))
p
# Neighboring locations
p = p + geom_point(aes(x = 1, y = 0), col = "black", size = 3) + 
    geom_text(aes(x = 1, y = -.02, label = "x2"))
p
#
p = p + geom_segment(aes(y = 1/sqrt(2 * pi) * exp(-.5 * 1^2), 
                         yend = 0, x = 1, xend = 1), 
                     linetype = 2) +
    geom_text(aes(x = .7, y = .1, label = "w12"))
p
#
p = p + geom_point(aes(x = -2, y = 0), col = "black", size = 3) + 
    geom_text(aes(x = -2, y = -.02, label = "x3"))
p
#
p = p + geom_segment(aes(y = 1/sqrt(2 * pi) * exp(-.5 * (-2)^2), 
                         yend = 0, x = -2, xend = -2), 
                     linetype = 2) +
    geom_text(aes(x = -1.7, y = .025, label = "w13"))
p
```

Shapes other than the Gaussian can be used but they tend to be symmetric reflecting the type of dependency found in many spatial processes.

At each location i for which the local regression model is estimated, the kernel is given by 
$$
w_{ij} = \left[-(d_{ij}/h)^2\right], 
$$
where $d_{ij}$ is the distance between locations $i$ and $j$, and $h$ is the bandwidth. As $h$ increases, the gradient of the kernel becomes less steep and more data points are included in the local estimation.

```{r}
p + geom_segment(aes(y = .23, yend = .23, x = -1, xend = 1), col = "blue") +
    geom_text(x = 0, y = .25, label = "bandwidth")
```

The choice of bandwidth is a trade-off between variance and bias. A bandwidth that is narrow results in large variation in model parameters across the domain.  A bandwidth that is wide leads to a large bias locally as the model coefficients are influenced by processes not representative of the local conditions.

# Geographic Regression (L17)

Geographic regression (geographically weighted regression--GWR) is local regressions done spatially. A separate model is fit at each observation location. Data for each fit are nearby observations weighted by their distance to the location. The result is a set of regression coefficients at each observation location.

Example: Burglaries in Columbus, OH. The data frame `columbus`  is available in the **spgwr** package.
```{r}
data(columbus)
columbus[1:3, c(7:9, 13, 14)]
```

Interest is the relationship between the amount of crime and income and housing prices. Crime is the response variable (`CRIME`) as residential burglaries and vehicle thefts per 1000 households. Income (`INC`) and housing values (`HOVAL`) are explanatory variables in units of 1000 dollars. The `X` and `Y` values define the centroids of the census tracts.

*Start with linear regression (always)*

Are income and housing prices statistically related to crime in Columbus, OH?  How much of the variation in crime can be explained by housing prices and income? 
```{r}
model.lm = lm(CRIME ~ INC + HOVAL, data = columbus)
summary(model.lm)
```

For every one unit ($1000) increase in income there is a 1.6/10,000 households decrease in crime, holding housing price constant. This value of 1.6 is based on all the census tracts across the city and it represents the global (overall) relationship between crime and income in Columbus at the census tract level of analysis.

Can we do better with a simpler model?
```{r}
drop1(model.lm)
```

No, the model can't be simplified.

*Plot the data*

Before applying a geographic regression, let's look at some plots. To examine the possibility that the relationship between crime and these two explanatory variables varies by location, use a conditioning plot (`coplot()` function).
```{r}
coplot(CRIME ~ INC | X + Y, number = 3, 
       panel = panel.smooth, span = 1, data = columbus)
```

The plot is described by the formula (`CRIME ~ INC`) with the response variable on the left side of the tilde and the explanatory variable on the right side. The vertical bar is read "conditional on." The relationship between crime and income conditional on location.

Panels are ordered as you would see them on a map with the lower-left panel being the southwest side of Columbus and the upper-right the northeast side. 

The number of conditioning intervals is given by the argument number. It is used if the conditioning variable is not a factor. The default is 6. The `X` and `Y` ranges vary across the panels with overlap as shown in the margin plots.

The red lines a local regression model through the points. There appears to be some variation in the slope of the red lines.

Where is crime most prevalent?
```{r}
coplot(Y ~ X | CRIME, data = columbus)
```

Here the conditioning is reversed. We condition location based on crime. The margin plot shows the level of crime from low to high (left to right) and what range of crime values are used in each panel starting in the lower left and finishing in the upper right.

More familiar is a bubble plot where the symbol size is proportional to crime.
```{r, eval = FALSE}
test <- columbus
ggplot(data = test, aes(x = X, y = Y)) + 
  geom_point(aes(size = CRIME))
```

How far apart are the centroids? First create a 2-column matrix of location coordinates using the `cbind()` function (bind the columns).
```{r}
xy = cbind(x = columbus$X, y = columbus$X)
head(xy)
```

Next use the `dist()` function to compute the pairwise distances between each location (assuming planar coordinates).
```{r}
dij = dist(xy)
```

The distances are stored as a `dist` object. To examine the distances type the following:
```{r}
max(dij)
min(dij)
hist(dij)
length(dij) # should equal n*(n-1)/2
```

*Select a bandwidth*

The first step is to determine the kernel bandwidth. The default kernel shape is Gaussian. The kernel shape together with the bandwidth specifying the weights matrix.

Selection uses leave-one-out (LOO) cross validation. For a given bandwidth, cross validation determines the sum of the squared differences between the model predicted value and the observed response at each observation location (CV score), where the prediction is made on the observation that is left out of the model. The bandwidth that provides the smallest CV score is chosen.

Here we use the `gwr.sel()` function from the **spgwr** package. We specify the model `CRIME ~ INC + HOVAL, the data, and which columns contain the spatial coordinates. To determine the bandwidth and save it, type:
```{r}
model.bw = gwr.sel(CRIME ~ INC + HOVAL,
                   data = columbus, 
                   coords = cbind(columbus$X, columbus$Y))
```

The procedure is an iterative optimization with improvements made based on the residuals from each of the geographic regressions using different bandwidths. Values for the bandwidth and the CV score are printed. After several iterations there is no improvement in the CV score so the corresponding bandwidth is selected.

The bandwidth is 2.27 spatial units.

Fit a geographic regression

Next fit a geographic regression. This is done with the `gwr()` function. The implementation the same as the `gwr.sel()` function except for the bandwidth argument.
```{r}
model.gwr = gwr(CRIME ~ INC + HOVAL, 
                data = columbus, 
                coords = cbind(columbus$X, columbus$Y), 
                bandwidth = model.bw)
model.gwr
```

The output repeats the call function, which includes the functional form of the model (crime is statistically related to income and housing), the geographic coordinates, and the bandwidth. The kernel is Gaussian (`gwr.Gauss`) and the bandwidth is fixed.

An aspatial summary of the coefficients is given. In general, crime is negatively related to income, but the maximum value for the regression coefficient indicates that for at least one location the relationship is positive. Similar for housing. The median values can be compared to the values from a global regression (right-most column). 

For the linear regression model, for every one unit increase in income there is a 1.6 decrease in crime, holding housing constant. This value of 1.6 is based on the data across the entire city of Columbus and it represents the 'global' linear relationship between crime and income.  

To see what is stored in the model object `model.gwr`, type:
```{r}
names(model.gwr)
```

The first item in the list of twelve is a spatial points data frame (`SDF`). To see what is stored in this data frame, type:
```{r}
names(model.gwr$SDF)
class(model.gwr$SDF)
```

A spatial points data frame is an S4 class. The 4th version of S. S is a language with two implementations, S-Plus (commercial) and R (free).

The spatial points data frame has five slots with names
```{r}
slotNames(model.gwr$SDF)
```

`@data`, `@coords.nrs`, `@coords`, `@bbox`, `@proj4string`.

The attribute table is a data frame in the slot called `data`.
  	
Plot a histogram of the coefficient values for the income term. First extract the attribute table as a data frame using the `slot()` function. Then use `ggplot()`.
```{r}
df = slot(model.gwr$SDF, "data")
ggplot(df, aes(INC)) + 
  geom_histogram(binwidth = .25)
```

Note: These are not the values for the variable income. They are the values of the coefficient on the income term in the geographic regression.

The histogram indicates that across most of Columbus the relationship between crime and housing values is negative. But there are a few places where it is positive. As housing values increase, so does crime at certain locations. This information can be quite important.

What percentage of locations show a positive relationship between crime and income?
```{r}
sum(df$INC > 0)/length(df$INC)
```

Map the coefficients to see how the relationship varies across the city.

*Map the output*

The easiest way to make a map from an S4 spatial object is with the `spplot()` method.
```{r}
spplot(model.gwr$SDF, "INC")
```

Orange circles indicate locations where there is a near-zero relationship between crime and income given these data and the model. Blue and black circles indicate an inverse relationship with crime lower in areas of greater wealth. The yellow dots indicate a direct relationship in the neighborhoods surrounding OSU.

The variance explained by the model for crime using both income and housing values is given by the local R squared values. We map the amount of variance explained in a similar why but use the better color ramps available in the **RColorBrewer** package.
```{r}
cls = brewer.pal(5, "Greens")
cls
spplot(model.gwr$SDF, "localR2", 
       col.regions = cls)
```

The model explains the least amount of the variance in crime in the center part of the city. This is due to other factors not related income and housing and greater random variation. 

Prediction errors are given in the column labeled `gwr.e`. These are saved and plotted by typing:
```{r}
cls = brewer.pal(5, "RdYlGn")
spplot(model.gwr$SDF, "gwr.e", 
       col.regions = cls)
```

Yellow circles show where the prediction errors are smallest. Green circles are where the model under estimates (residual = observed minus predicted) crime given income and housing values. Red circles are where the model over estimates crime.

On average prediction errors using a GWR are smaller than the prediction errors using a global regression. The better fit results from more model coefficients and sometimes from a better representation of the underlying process.

We extract the residuals from the global linear regression and from the geographic regression.
```{r}
pe.lm = residuals(model.lm)
pe.gwr = model.gwr$SDF$gwr.e
```

The mean squared residuals from the linear regression model and from the geographic regression models are
```{r}
mean(pe.lm^2)
mean(pe.gwr^2)
```

*Example: Geographic Weighted Regression with Southern homicides*

Recall the data set containing homicide rates in counties of the southern U.S. (Lesson 11). Download the shapefiles from my website and unzip it in your working directory.
```{r}
download.file(url = "http://myweb.fsu.edu/jelsner/temp/data/south.zip",
              destfile = "south.zip", mode = "wb")
unzip("south.zip")
```

Read the shapefile using the `readOGR()` from the **rgdal** package.
```{r}
homicides.spdf = readOGR(dsn = "south", 
                         layer = "south", 
                         stringsAsFactors = FALSE)
```

Spatial data objects have slots identified by the `@` symbol similar to the way columns are identified with the `$` symbol in data frames.

With the spatial polygons data frame the slots contain the attribute tables as a data frame, the polygons, the plot order, coordinates for the bounding box, and the coordinate reference system.

To get a list of the names of the columns in the attribute table type
```{r}
names(homicides.spdf)
```

We will look at predicting homicide rates (`HR`) given in number per 100,000 people. We will initially consider five explanatory variables including:

`RD` | resource deprivation index
`PS` | population structure index
`MA` | marriage age
`DV` | divorce rate
`UE` | unemployment rate

The two digit number appended to the column names is the census year.

Recall the `plot()` method plots the county borders.
```{r}
plot(homicides.spdf)
```

We use the `spplot()` method to create a choropleth map of the homicide rates during based on the 1990 census (`HR90`). Break the rates into seven equal range bands. Select a color ramp from the *RColorBrewer* package. 
```{r}
range(homicides.spdf$HR90)
brks = seq(0, 70, 10)
cr = brewer.pal(7, "Oranges")

spplot(homicides.spdf, "HR90", 
       col.regions = cr, 
       at = brks)
```

Move the color ramp to the bottom and include labels.
```{r}
spplot(homicides.spdf, "HR90", 
       col.regions = cr, 
       at = brks, 
       colorkey = list(space = "bottom"),
       sub = "County Homicide Rate [/100,000] (1990)")
```

Start with a global linear regression of homicide rates for 1990.
```{r}
model.lm = lm(HR90 ~ RD90 + PS90 + MA90 + DV90 + UE90, 
              data = homicides.spdf)
summary(model.lm)
```

Can you simplify it?
```{r}
drop1(model.lm)
```

Final model.
```{r}
model.lm2 = lm(HR90 ~ RD90 + PS90 + DV90 + UE90, 
               data = homicides.spdf)
step(model.lm2, direction = "both")
```

The new model can't be simplified.

Map the predicted values from the regression model. First add the predictions to the spatial polygons data frame.
```{r}
homicides.spdf$predLM = predict(model.lm2)
range(homicides.spdf$predLM)
brks = seq(-10, 30, 5)
cr = brewer.pal(8, "Oranges")
spplot(homicides.spdf, "predLM", col.regions = cr, at = brks, 
       colorkey = list(space = "bottom"),
       sub = "LM Predicted County Homicide Rate [/100,000]")
```

Linear regression captures the spatial pattern of homicides across the south but the predicted values have a smaller range. Also since rates are non-negative transforming them to logarithms might be a good choice.

Create a new column in the `homicides.spdf` data slot called `logHR90`.
```{r}
homicides.spdf$logHR90 = log(homicides.spdf$HR90)
x = homicides.spdf$HR90 == 0
mn = min(homicides.spdf$logHR90[!x])
homicides.spdf$logHR90[x] = mn

model.lm3 = lm(logHR90 ~ RD90 + PS90 + DV90 + UE90, 
               data = homicides.spdf)
summary(model.lm3)
```

```{r}
homicides.spdf$predLM = exp(predict(model.lm3))
range(homicides.spdf$predLM)
brks = seq(0, 70, 10)
cr = brewer.pal(7, "Oranges")
spplot(homicides.spdf, "predLM", col.regions = cr, at = brks, 
       colorkey = list(space = "bottom"),
       sub = "LM Predicted County Homicide Rate [/100,000]")
```

Model the same homicide rates using geographic regression with the same set of explanatory variables. First select the bandwidth.
```{r}
bw = gwr.sel(logHR90 ~ RD90 + PS90 + DV90 + UE90, 
             data = homicides.spdf)
```

Then fit the models. The model output is saved as a spatial polygons data frame called "model.gwr$SDF"
```{r}
model.gwr = gwr(logHR90 ~ RD90 + PS90 + DV90 + UE90, 
                data = homicides.spdf, 
                bandwidth = bw)
```

Add a column to the output spatial polygons data frame.
```{r}
model.gwr$SDF$pred2 = exp(model.gwr$SDF$pred)
```

Map the predicted values.
```{r}
spplot(model.gwr$SDF, "pred2", col.regions = cr, at = brks, 
       colorkey = list(space = "bottom"),
       sub = "GWR Predicted County Homicide Rate [/100,000]")
```

Geographic regression similarly captures the spatial pattern of homicides across the south. The spread of predicted values matches the observed spread better than the linear model. The pattern is also a smoother.

Map the coefficient on resource deprivation. From the linear model we saw that as RD increases so does HR.
```{r}
range(model.gwr$SDF$RD90)
brks = seq(0, 1.2, .2)
cr = brewer.pal(6, "Blues")
spplot(model.gwr$SDF, "RD90", 
       col.regions = cr, at = brks, 
       colorkey = list(space = "bottom"),
       sub = "Resource Deprivation Coefficient")
```

All values are above zero, but the areas in dark blue indicate where RD plays a stronger role in explaining HR relative to the other variables.

Where are homicide rates best predicted by the model? This is answered with a choropleth map of `localR2` column in the spatial polygons data frame.
```{r}
brks = seq(0, .8, .1)
cr = brewer.pal(8, "Purples")
spplot(model.gwr$SDF, "localR2", 
       col.regions = cr,  at = brks, 
       colorkey = list(space = "bottom"),
       sub = "Local Regression Fit")
```

#Spatial data: 

spatial data frames are s4 class objects 


`polygons`: list of the polygon boundaries. Each entry in the list is a separate polygon corresponding to a row in the data frame.
`plotOrder`: vector of integers listing the order of the polygons for plotting.
`bbox`: numeric 2 x 2 matrix listing the bounding coordinates (lower left and upper right range of the geographic domain)
`proj4string`: class of CRS (coordinate reference system) GDAL standard.

# Spatial autocorrelation (L18)

Spatial autocorrelation measures the degree to which data values tend to cluster across space. A data value is given at each location of a spatial feature. Spatial features can be points, lines, polygons, etc. 

For example, yesterday Dr. Wong showed the spatial autocorrelation of Supplemental Security Income (SSI) values. An SSI was measured at each polygon representing a county in the United States.

How spatial autocorrelation is estimated depends on the spatial feature. Estimating spatial autocorrelation using point data is different than estimating spatial autocorrelation with field data.

We begin by considering polygons. Data are often available as attributes corresponding to a set of fixed polygons. Polygons may be defined by the researcher (e.g., regular grids) or may be administrative boundaries (counties).

Attributes are usually data aggregated within each polygon (population, voting results, etc). Areas are a tessellation (completely cover with no overlap) of the study domain but they may contain holes (e.g., lakes).

Spatial dependency (autocorrelation) describes the fact that the attribute values in neighboring polygons are all similar. There are at least three possible explanations for this.

One is a spatial association: whatever is causing an attribute to have a value in one area also causes the attribute to have a similar value in nearby areas. Crime rates in nearby areas tend to be similar due to factors such as economic status, amount of policing and the built environment: conditions that attract one criminal will attract others. Or non-infectious diseases such as lung cancer have similar rates in neighbors with similar socio-economic conditions.

Another is spatial causality: something within a given area directly influences outcomes within nearby areas. The broken window theory of crime suggests that poverty, lack of maintenance, and petty crime tends to breed more crime due to a perceived breakdown in civil order.

Another explanation is spatial interaction: the movement of people, goods or information creates apparent relationships between areas. Infectious diseases spread from a source region thus increasing the disease rates in surrounding areas as the direct result of contact or movement of people between regions.

Spatial statistical models exploit spatial autocorrelation but the models are silent on the reason for the autocorrelation.

# Quantifying spatial autocorrelation

Spatial autocorrelation is quantified by calculating how similar a value in region (polygon feature) $i$ is to the value in region $j$ and weighting this similarity by how close region $i$ is to region $j$. The closer the regions are together the greater the weight.

High similarities with high weight (similar values close together) yield high values of the index. Low similarities with high weight (dissimilar values close together) yield low values of the index. 

Let $\hbox{sim}_{ij}$ denote the similarity between values $Y_i$ and $Y_j$, and let $w_{ij}$ denote a weight describing the proximity between regions $i$ and $j$, for $i$, $j$ = 1, ..., $N$. Then a spatial autocorrelation index (sai) is given by
$$
\hbox{sai}_{ij} = \frac{\sum_{i=1}^N \sum_{j=1}^N w_{ij}\hbox{sim}_{ij}}{\sum_{i=1}^N \sum_{j=1}^N w_{ij}}
$$
which represents the weighted average similarity between regions. The collection of weights is called a spatial weights matrix and defines the neighborhoods for each region.

For regions on a rectangular grid under the *rook contiguity* criterion, $w_{ij}$ = 1 if regions $i$ and $j$ share a boundary, and 0 otherwise. This choice results in a symmetric matrix of weights since $w_{ij}$ = $w_{ji}$. Also a region is not a neighbor to itself so $w_{ii}$ = 0.

Alternatively we can define a centroid for each region and let $w_{ij}$ = 1 if the centroid of region $i$ is near the centroid of region $j$ and 0 otherwise. In this case we decide on the number of nearest neighbors to define neighborhoods. The k-nearest neighbor method can result in a non-symmetric weight matrix because the distances to the kth nearest neighbor may be different for each centroid.

We can also define the neighbors by distance. For example, if $d_{ij}$ is the distance between centroids of regions $i$ and $j$, we could let $w_{ij}$ =  1 if $d_{ij}$ < $\delta$ and 0 otherwise. This approach produces a symmetric weights matrix.

Let's return to the Columbus, OH crime data but this time as polygon data (census tracts).

### Columbus crime

```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip", mode = "wb")
unzip("columbus.zip")
spdf = readOGR(dsn = "columbus", 
               layer = "columbus")
```

Create a choropleth map of the crime rate by census tract. Residential burglaries and vehicle thefts per 1000 households.
```{r}
brks = round(quantile(spdf$CRIME, 
                      probs = seq(0, 1, .2)))
cr = rev(brewer.pal(5, "PiYG"))
spplot(spdf, "CRIME", col.regions = cr, at = brks, 
            colorkey = list(space = "bottom", 
                            labels = paste(brks)),
            sub = "Burglary & Vehicle Theft/1000 Households")
```

High crime areas tend to cluster. Spatial autocorrelation quantifies this clustering. The first step towards quantification is to define a list of neighbors for each polygon.

### List of neighbors

Given a spatial polygons data frame we create a list of neighbors using the `poly2nb()` function from the **spdep** package.
```{r}
neighbors = poly2nb(spdf)
neighbors
```

The `nb` stands for neighbor list object. The function builds the list from regions based on contiguity. That is neighbors must share one or more boundary points. Caution: With a large number of polygons and detailed boundaries it can take some time to generate the list.

There are 49 tracts. Each tract is bordered by at least one other tract (each region has at least one neighbor), which may not always be the case. The average number of neighbors is 4.8. The total number of neighbors over all tracts is 236. This represents 9.8% of all possible connections (if every tract is a neighbor of itself and every other tract 49*49).

A graph of the links (network) is obtained as follows.
```{r}
plot(neighbors, coordinates(spdf))
```

The arguments include the neighbor list object and the location of the polygon centroids. Location of the centroids are extracted from the spatial object with the `coordinates()` function.

The graph is a network displaying the contiguity pattern (topology). Tracts in the center of the city have more neighbors than those on the outskirts.

The number of links per node (tract)--link distribution--is obtained by typing
```{r}
summary(neighbors)
``` 

The list of neighboring tracts for the first two tracts.
```{r}
neighbors[[1]]; neighbors[[2]]
```

The first tract has two neighbors, tracts 2 and 3. The neighbor numbers are stored as an integer vector. Tract 2 has three neighbors, tracts 1, 3, and 4. 

Tract 5 has 8 neighbors and so on.
```{r}
neighbors[[5]]
```

The function `card()` tallies the number of neighbors by tract.
```{r}
card(neighbors)
```

The tracts are identified by region id from the spatial polygons data frame. 
```{r}
attr(neighbors, "region.id")
```

The region id is a character vector with elements from "0" to "48".

By default the contiguity is defined as having at least one point in common. This can be changed to more than one point by using the argument `queen = FALSE` in the `poly2nb()` function.

### Weights

Before spatial autocorrelation can be quantified the neighbor list object needs to be appended with weights specifying how much weight to give to each link.

The function `nb2listw()` function turns the neighbors list object into a spatial weights object.
```{r}
wtsnbs = nb2listw(neighbors)
class(wtsnbs)
```

The `wtsnbs` object is a list with two elements. The first element (`listw`) is the weights matrix and the second element (`nb`) is the neighbor list object.
```{r}
summary(wtsnbs)
```

The network statistics are given along with information about the weights. By default all neighboring tracts are assigned equal weights (`style = "W"`). For a tract with 5 neighbors each neighbor gets a weight of 1/5. The sum over all weights (`S0`) is the number of tracts.

To see the weights for the first two tracts type
```{r}
wtsnbs$weights[1:2]
```

This is called a sparse representation of the spatial weights matrix. The full matrix would be 49 x 49.

To see the neighbors of the first two tracts type
```{r}
wtsnbs$neighbours[1:2]
```

Tract 1 has two neighbors (tract 2 & 3) so each are given a weight of 1/2. Tract 2 has three neighbors (tract 1, 3, & 4) so each are given a weight of 1/3.

With the neighbors and weights matrix specified and saved as a single object we are ready to compute a metric of spatial autocorrelation.

Caution: Contiguity can result in areas having no neighbors; islands for example. By default the `nb2listw()` function assumes each area has at least one neighbor. If not we need to specify how these areas are handled with the argument `zero.policy = TRUE`. This assigns a zero to the lagged values computed using the `listw` object for areas without neighbors.

# Moran's I (L18)

igh crime areas tend to cluster. Use the `moran()` function from the **spdep** package to quantify the amount of clustering. First create a list of neighbors using the `poly2nb()` function and then specify the weights. By default the neighbors are determined by "queen" contiguity and each neighbor is given a weight equal to the inverse of the number of neighbors (row standardized).
```{r}
nbs = poly2nb(spdf)
wts = nb2listw(nbs)
```

queen contiguity: refers to touchinng at atleast onne point. 

Next save the number of polygons and the summation over all the weights. The `moran()` function requires these along with the vector of values and the `listw` object.
```{r}
m = length(spdf$CRIME)
s = Szero(wts)
moran(spdf$CRIME, 
      listw = wts, 
      n = m, S0 = s)
```

The function returns the Moran's I statistic and the kurtosis (K). Moran's I ranges from -1 to +1. A value of .5 indicates a high level of spatial autocorrelation. This was anticipated based on our visual inspection of the crime showing pockets of higher and lower rates. Positive values of Moran's I indicate clustering and negative values indicate 'inhibition'.

totally random is a 0
checkerboard would be negative correlation because it is seperated by one unit. 

Kurtosis is a statistic measuring the peakedness of the distribution of the attribute values. A normal distribution has a kurtosis of 3. If the kurtosis is too large or too small relative to a normal distribution then the inferences we make with Moran's I may be suspect.

normal distribution has a kurtosis of 3



There is less clustering of housing values and somewhat less clustering in household income.
```{r}
moran(spdf$HOVAL, 
      listw = wts, 
      n = m, S0 = s)
moran(spdf$INC, 
      listw = wts, 
      n = m, S0 = s)
```

Another way to quantify spatial autocorrelation is with the Geary's C statistic. The equation is 
$$
\hbox{C} = \frac{(N-1) \sum_{i} \sum_{j} w_{ij} (Y_i-Y_j)^2}{2 W \sum_{i}(Y_i-\bar Y)^2} 
$$

We instantiate the `geary()` function in a way similar to the `moran()` function except here we also specify `n1` to be the number of polygons minus one.
```{r}
geary(spdf$CRIME, 
      listw = wts,
      n = m, S0 = s, 
      n1 = m - 1)
```

Values for C range from 0 to 2 with 1 indicating no spatial autocorrelation. Values less than 1 indicate positive autocorrelation. Both I and C are global measures of autocorrelation, but C is more sensitive to local variations in autocorrelation. 

If the interpretation of C is much different than the interpretation of I then consider local measures of spatial autocorrelation.

# Spatial lag (L18)

Insight into spatial autocorrelation is obtained by noting that Moran's I is the slope coefficient from a regression of the weighted average of the neighborhood values onto the observed values. The weighted average of neighborhood values is called the spatial lag.

Let crime be the set of crime values in each region. We create a spatial lag variable using the `lag.listw()` function. The first argument is the `listw` object and the second is the vector of crime values.
```{r}
crime = spdf$CRIME
Wcrime = lag.listw(wtsnbs, crime)
```

For each value in the vector crime there is a corresponding value in the vector object `Wcrime` representing the average crime over the neighboring regions.

Recall tract 1 had tract 2 and 3 as neighbors. So the follow should return a TRUE.
```{r}
Wcrime[1] == (crime[2] + crime[3])/2
```

A scatter plot of the neighborhood average crime versus the crime in each region shows there is a relationship.
```{r}
data.frame(crime, Wcrime) %>%
ggplot(., aes(x = crime, y = Wcrime)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 70)) +
  scale_y_continuous(limits = c(0, 70)) +
  xlab("Crime") + ylab("Spatial Lag of Crime")
```

Tracts with low values of crime tend to be surrounded by tracts with low values of crime on average and tracts with high values of crime tend be surrounded by tracts with high values of crime. The range of neighborhood averages is smaller. There are a few outliers.

The slope of the regression line is the value of Moran's I. To check this type
```{r}
lm(Wcrime ~ crime)
```

# Statistical significance of spatial autocorrelation (L19)

Let's consider another data set. Sudden infant death syndrome rates in North Carolina counties. The data are from: Cressie, Noel (1993). Statistics for Spatial Data. New York, Wiley, pp. 386-389.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/sids2.zip",
              "sids2.zip", mode = "wb")
unzip("sids2.zip")
spdf = readOGR(dsn = "sids2", layer = "sids2")
head(spdf@data)
```

The column `SIDR79` contains the death rate per 1000 live births (1979-84) from sudden infant death syndrome. Create a choropleth map of the SIDS rates.
```{r}
range(spdf$SIDR79)
brks = seq(0, 7, 1)
cr = brewer.pal(7, "PuBu")
spplot(spdf, "SIDR79", col.regions = cr, at = brks, 
            colorkey = list(space = "bottom"),
            sub = "SIDS Rates 1979-84 [per 1000]")

```

Create a neighborhood list (`nb`) and a `listw` object (`wts`). Graph the neighborhood network.
```{r}
nb = poly2nb(spdf)
wts = nb2listw(nb)
plot(nb, coordinates(spdf))
plot(spdf, add = TRUE)
```

Next compute Moran's I
```{r}
m = length(spdf$SIDR79)
s = Szero(wts)
moran(spdf$SIDR79, 
      listw = wts, 
      n = m, S0 = s)
```

I is .14 and K is 4.4. A normal distribution has a kurtosis of 3. Values less than about 2 or greater than about 4 indicate that inferences about autocorrelation based on the assumption of normality are suspect.

Weights are specified using the `style =` argument in the `nb2listw()` function. The default "W" is row standardized (sum of the weights over all links equals the number of polygons). "B" is binary (each neighbor gets a weight of one). "S" is a variance stabilizing scheme (Tiefelsdorf et al. 1999). 

Each style gives a somewhat different value for I.
```{r}
x = spdf$SIDR79
moran.test(x, nb2listw(nb, style = "W"))$estimate[1]
moran.test(x, nb2listw(nb, style = "B"))$estimate[1]  # binary
moran.test(x, nb2listw(nb, style = "S"))$estimate[1]  # variance-stabilizing
```

Thus when we report a Moran's I we need to state what weighting scheme was used.

Let `sids` be the SIDS rate in each region. We create a spatial lag variable using the `lag.listw()` function. The first argument is the `listw` object and the second is the vector of crime values.
```{r}
sids = spdf$SIDR79
Wsids = lag.listw(wts, sids)
```

For each value in the vector `spdf$SIDR79` there is a corresponding value in the object `Wsids` representing the neighborhood average SIDS rate. 
```{r}
Wsids[1]
j = wts$neighbours[[1]]
j
sum(spdf$SIDR79[j])/length(j)
```

The weight for county one is `Wsids[1]` = 2.659. The neighbor indices for this county are in the vector `wts$neighbours[[1]]` of length 3. Add the SIDS rates from those counties and divide by the length.

A scatter plot of the neighborhood average SIDS rate versus the actual SIDS rate in each region.
```{r}
data.frame(sids, Wsids) %>%
ggplot(., aes(x = sids, y = Wsids)) + 
  geom_point() + 
  geom_smooth(method = lm) +
  scale_x_continuous(limits = c(0, 7)) +
  scale_y_continuous(limits = c(0, 7)) +
  xlab("SIDS") + ylab("Spatial Lag of SIDS")
```
The poinnts in y is always going to be less than the length of poinnts through x


The slope of the regression line is upward indicating positive autocorrelation. The value of the slope is I. To check this type
```{r}
lm(Wsids ~ sids)
```

Is the value of Moran's I significant with respect to the null hypothesis of no spatial autocorrelation (I = 0)? One way to answer this question is to draw an uncertainty band on the regression line and see if a horizontal line can be put through the band. If not, then I is statistically different from zero.

More formally the question is answered by comparing the standard deviate ($z$-value) of the I statistic with a normal deviate. This is done using the `moran.test()` function, where the $z$-value is the difference between I and the expected value of I divided by the square root of the variance of I. The function takes a variable name or numeric vector and a spatial weights list object in that order as required arguments.
```{r}
moran.test(spdf$SIDR79, wts)
```

I is .143 with a variance of .0042. The $z$-value for the I statistic is 2.3438 giving a small $p$-value under the assumption of no spatial autocorrelation.  

Thus we conclude there is weak but significant spatial autocorrelation in SIDS rates across North Carolina at the county level.
```{r}
pnorm(2.3438, lower.tail = FALSE)
```

Under the assumption of normal distributed and uncorrelated data, the expected value for Moran's I is -1/(n-1) where n is the number of counties. A check on the distribution of SIDS rates indicates that normality is somewhat suspect. Recall a good way to check the normality assumption is to use the `sm.density()` function/
```{r}
sm::sm.density(spdf$SIDR79, model = "Normal",
           xlab = "SIDS Rates 1979-84 [per 1000]")
```

Note that the SIDS rates are more "peaked" (higher kurtosis) than a normal distribution. In this case it is best to use a permutation approach to assess significance.

# Monte Carlo approach

The `moran.mc()` function allows inference on I without the assumption of normally distributed data values. MC stands for Monte Carlo which refers to the city of Monte Carlo in Monaco famous for its gambling casinos. The MC procedure refers to random sampling.

The name of the data vector and the weights list object (`listw`) are required arguments as is the number of permutations (`nsim`). Each permutation is a random rearrangement of the SIDS rates across the counties. This removes the spatial autocorrelation but keeps the non-spatial distribution of the SIDS rates. The neighbor topology and weights remain the same.

For each permutation (random shuffle), I is computed and saved. The $p$-value is obtained as the ratio of the number of permuted I values greater or exceeding the observed I over the number of permutation plus one. In the case where there are 5 permuted I values greater or equal to the observed value based on 99 simulations, the $p$-value is 5/(99 + 1) = .05.

For example, if you want inference on I using 99 permutations type
```{r}
set.seed(4102)
mP = moran.mc(spdf$SIDR79, listw = wts, nsim = 99)
mP
```

Two of the permutations yield a Moran's I greater than 0.1428, hence the $p$-value as evidence in support of the null hypothesis (the true value for Moran's I is zero) is 0.02.

Note: Here we initiate the random number generator to a specific seed value so that the set of random permutations of the values across the domain will be the same each time we knit this Rmd. This is important for reproducibility. The default random number generator seed value is determined from the current time (internal clock) and so no random permutations will be identical. To control the seed use the `set.seed()` function.

The values of I computed for each permutation are saved in the vector `mP$res`.
```{r}
head(mP$res)
tail(mP$res)
```

The last value in the vector is I computed using the data in the correct counties. The $p$-value as evidence in support of the null hypothesis that I is zero is given as
```{r}
sum(mP$res > mP$res[100])/99
```

A density graph displays the distribution of permuted I's. First, rerun using 999 simulations. Then plot a density curve and add a vertical line at the value of Moran's I computed from the data at the actual locations.
```{r}
mP = moran.mc(spdf$SIDR79, wts, nsim = 999)
df = data.frame(mp = mP$res[-1000])
ggplot(df, aes(mp)) + 
  geom_density() + 
  geom_rug() + 
  geom_vline(xintercept = mP$res[1000], 
             color = "red", size = 2)
```

The density curve is centered just to the left of zero consistent with the theoretical expectation (mean) of -.01. Also note that the right tail is fatter than the left tail. This is due to the skewness of the rates used in the calculation of I.

What do you do with the knowledge that the SIDS rates have significant spatial autocorrelation? By itself not much but it can provide notice that something might be going on in certain regions (hot spot analysis).

More typically the knowledge is useful after other known factors are considered. Or in the language of statistics, knowledge of significant spatial autocorrelation in the model residuals can help you build a better model.

# Spatial autocorrelation of model residuals

A spatial regression model is needed whenever the residuals from a non-spatial regression model exhibit significant spatial autocorrelation. So a common way to proceed is to first regress the response variable onto the explanatory variables and check for spatial autocorrelation.

Even if the response variable indicates a high level of spatial autocorrelation it might not be necessary to use a spatial regression model if the explanatory variables remove this correlation.

Let's return once again to the Columbus crime data.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/columbus.zip",
              "columbus.zip", mode = "wb")
unzip("columbus.zip")
spdf = readOGR(dsn = "columbus", 
               layer = "columbus")
```

Next model the data with linear regression.
```{r}
model = lm(CRIME ~ INC + HOVAL, 
           data = spdf)
summary(model)
```

The model explains 55% of the variation in crime. As income and housing values increase crime goes down. The model is statistically significant.

A check on the model residuals reveals no issue with normality.
```{r}
sm::sm.density(residuals(model), model = "Normal")
```

positive underpredicts negative overpredicts
yes, amount of clustering appears to be less


The next step is to create a choropleth map of the model residuals. Are the residuals clustered?
```{r}
spdf$resids = residuals(model)
range(spdf$resids)
brks = seq(-40, 40, 10)
cr = brewer.pal(8, "PuOr")
spplot(spdf, "resids", col.regions = cr, at = brks, 
            colorkey = list(space = "bottom"),
            sub = "Linear Model Residuals\n Burglary & Vehicle Theft/1000 Households")

```
dependent of the model not actual values so in nnext chunk you MUST do morantest

Yes. There are clustered regions where the model over predicts crime based on household income and housing values (orange regions, southwest) and where it under predicts crime (purple regions).

The amount of clustering appears to be less than before. That is, after accounting for regional factors related to crime the spatial autocorrelation is reduced.

positive underpredicts negative overpredicts
yes, amount of clustering appears to be less

To determine I on the residuals we use the `lm.morantest()` function and pass the regression model object and the weights object to it.
```{r}
nbs = poly2nb(spdf)
wts = nb2listw(nbs)
lm.morantest(model, wts)
```

Moran's I on the model residuals is .222.  This compares with the value of .5 on crime alone. Part of the spatial autocorrelation is absorbed by the explanatory factors.

Do we need a spatial regression model?  The output gives a $p$-value on I of .002, thus we reject the null hypothesis of no spatial autocorrelation in the residuals and conclude that a spatial regression model would improve the fit.  

The $z$-value takes into account the fact that these are residuals so the variance is adjusted accordingly.

The next step is to choose a spatial regression model.

# Local indicators of spatial autocorrelation (L19)

The Moran's I statistic was first used in the 1950s. Localization of this statistic was first presented by Luc Anselin in 1995 (Anselin, L. 1995. Local indicators of spatial association, Geographical Analysis, 27, 93–115).

Local I is a deconstruction of global I where the neighborhood defined by contiguity is used in two ways. (1) defines and weights neighbor and (2) determines the spatial scale over which I is computed.

The file *police.zip* contains shapefiles in a folder called *police* on my website. The variables include police expenditures (`POLICE`), crime (`CRIME`), income (`INC`), unemployment (`UNEMP`) and other socio-economic characteristics for counties in Mississippi.

The police expenditures are per capita, 1982 (dollars per person). The personal income per county resident, 1982 (dollars per person). The crime is the number of serious crimes per 100,000 residents, 1981. The unemployment is percent unemployed in 1980.

Read the data as a spatial polygons data frame.
```{r}
download.file("http://myweb.fsu.edu/jelsner/temp/data/police.zip",
                    "police.zip")
unzip("police.zip")
spdf = readOGR(dsn = "police", layer = "police")
```

Using queen's contiguity determine the neighborhood topology and the weights.
```{r}
nbs = poly2nb(spdf)
wts = nb2listw(nbs)
round(listw2mat(wts)[1:5, 1:10], 2)
```

The weights in full matrix form show that the first county ("0") has three neighbors 2, 3, and 9 and each get a weight of 1/3. County "2" has four neighbors 1, 4, 9 and 10 and each gets a weight of 1/4.

Compute Moran's I on the percent white.
```{r}
moran(spdf$WHITE, listw = wts, 
      n = length(nbs), S0 = Szero(wts))
```

Compute local Moran's I.
```{r}
Ii_stats = localmoran(spdf$WHITE, wts)
str(Ii_stats)
```

The local I is given in the first column of a matrix where the rows are the counties. The other columns are the expected value for I, the variance of I, the z-value and the p-value. For example, the local I statistics from the first county are given by typing
```{r}
Ii_stats[1, 1:5]
```

Because these local values must average to the global value, they can take on values outside the range between -1 and 1. A summary() method on the first column of the Li  object gives statistics from the non-spatial distribution of I's.
```{r}
summary(Ii_stats[, 1])
```

To map the values we first attach the matrix columns of interest to the spatial polygons data frame. Here we attach Ii, Zi, and Pi.
```{r}
spdf$Ii = Ii_stats[, 1]
spdf$Zi = Ii_stats[, 4]
spdf$Pi = Ii_stats[, 5]
```

```{r}
rng = seq(-3, 3, .75)
cls = brewer.pal(9, "PiYG")
spplot(spdf, "Ii", col.regions = cls, at = rng)
```
local I: segregation largest in NE and the MS delta

Compare with a map of percent white. 
```{r}
range(spdf$WHITE)
rng = seq(10, 100, 10)
cls = rev(brewer.pal(9, "YlOrBr"))
spplot(spdf, "WHITE", col.regions = cls, at = rng)
```

Areas where percent white is high over the northeast are areas with the largest spatial correlation. Other areas of high spatial correlation include the Mississippi Valley and in the south. Note the county with the most negative spatial correlation is the county in the northwest with a fairly high percentage of whites neighbored by counties with much lower percentages.

# Spatial classes

We've worked with spatial polygons as one type of spatial data but the **sp** package provides classes and methods for working with other types of spatial data. We finish the semester with more about the various spatial data types in R. This material is from Pebesma and Bivand (2005).

Spatial data classes include points, grids, lines, rings and polygons. The **sp** package provides classes for the spatial-only information (topology), e.g. `SpatialPoints` object, `SpatialPolygons` object, etc. It also has extensions to these classes in cases where there is attribute information stored in a data frame, e.g. `SpatialPointsDataFrame`. 

That is a `SpatialPointsDataFrame` object extends a `SpatialPoints` object by adding an attribute table as a data frame to the data slot. In the same way a `SpatialLinesDataFrame` object extends a `SpatialLines` object.

### Manipulating spatial objects

Entries in spatial objects are accessible through a slot name. For example, `x@coords` contains the coordinates of the object `x` of class `SpatialPoints` and its extensions. Entries are best accessed using functions. In this case `coordinates()` to retrieve the coordinates.

Subsetting the spatial object is done with the subset operator `[]`. For example, to subset only those polygons where percent white exceeds 80 use
```{r}
plot(spdf[spdf$WHITE > 80, ])
```

The subset operator works on the attribute table and on the geometry.

To subset only the first two polygons use
```{r}
spdf2 = spdf[1:2, ]
plot(spdf2)
```

To subset only the first four polygons and save only the data column `WHITE` use
```{r, eval=FALSE}
spdf3 = spdf[1:4, "WHITE"]
head(spdf3)
```

The subset on list operator `[[]]` selects a column from the attribute table. It is also used to assign or replace values to a column in the table. For example
```{r}
hist(spdf[["WHITE"]])
```

Other methods include `plot()`, `summary()`, `print()`, `dim()`, and `names()`, `as.data.frame()`, `as.matrix()` and `image()` (for gridded data), and `length()` (number of features).

Additional *spatial* methods are available for the classes in **sp**. For example: `dimensions()` returns the number of spatial dimensions, `y = spTransform(x, CRS("+proj=longlat +datum=WGS84"))` transform from one coordinate reference system (geographic projection) to another (requires package **rgdal**), `bbox(x)` returns a matrix with the coordinates bounding box; the dimensions form rows, min/max form the columns, `coordinates()` returns a matrix with the spatial coordinates, `gridded()` tells whether `x` derives from `SpatialPixels()`.

* The `spplot()` method as we've used makes a map of an attribute. It can do this in combination with other types of data (points, lines, grids, polygons).

* The `over()` method combines two spatial layers of differing types, e.g. retrieve the polygon or grid indexes on a set of points.

* The `spsample()` function samples geometries. For example, it can be used to create a sample of points within a polygon, a gridded area, or on a spatial line.

### Spatial points

Most of the time there is no need to create a spatial object as the data will be input as such. However it is helpful to understand a bit about how spatial data are constructed and stored. 

We start by generating a set of 10 points on a unit square [0, 1] x [0, 1] by
```{r}
xc = round(runif(10), digits = 2)
yc = round(runif(10), digits = 2)
xy = cbind(xc, yc)
xy
```

This 10 x 2 matrix is converted to a `SpatialPoints` object by
```{r}
xy.sp = SpatialPoints(xy)
xy.sp
plot(xy.sp)
```

We retrieve the coordinates from `xy.sp` as a matrix by
```{r}
xy.cc = coordinates(xy.sp)
class(xy.cc)
dim(xy.cc)
```

Other methods include: retrieve the bounding box or the dimensions.
```{r}
bbox(xy.sp)
dimensions(xy.sp)
```

Subset points and coerce to a data frame.
```{r}
xy.sp[1:2]
xy.df = as.data.frame(xy.sp)
class(xy.df)
dim(xy.df)
summary(xy.sp)
```

#### Attributes

You can create a `SpatialPointsDataFrame` by building it from a `SpatialPoints` object together with a data frame containing the attributes.
```{r}
df = data.frame(z1 = round(5 + rnorm(10), 2), 
                z2 = 20:29)
df
```

Create a `SpatialPointsDataFrame` by combining the data frame with the `SpatialPoints` object.
```{r}
xy.spdf = SpatialPointsDataFrame(xy.sp, df)
xy.spdf
summary(xy.spdf)
```

Select row 1 and 2.
```{r}
xy.spdf[1:2, ]
```

Select attribute in column 1 along with the coordinates.
```{r}
xy.spdf[1]
```

Select rows 1 and 2 of attribute `z2`.
```{r}
xy.spdf[1:2, "z2"]
```

Coerce to data frame.
```{r}
xy.df = as.data.frame(xy.spdf)
xy.df[1:2, ]
```

Extract the coordinates.
```{r}
xy.cc = coordinates(xy.spdf)
class(xy.cc)
```

The subset behavior is copied from that of data frames, but coordinates are always sticky (stay with attribute rows) and a `SpatialPointsDataFrame` is always returned. If coordinates need to be dropped, use the `as.data.frame()` method.

If data are stored in a spreadsheet where the spatial coordinates are given alongside the attributes as columns (very common), a spatial points data frame object is created by specifying the columns containing the coordinates.
```{r}
df1 = data.frame(xy, df)
head(df1)
coordinates(df1) = c("xc", "yc")
class(df1)
df1[1:6, ]
```

Or
```{r}
df2 = data.frame(xy, df)
coordinates(df2) = ~ xc + yc
df2[1:2, ]
```

Here an object of class data frame is promoted to an object of class `SpatialPointsDataFrame` using the `coordinates()` function.

Elements (columns) in the data frame part of a spatial object can be manipulated (retrieved, assigned) directly.
```{r}
df2$z2
df2$z2[10] = 20
df2$z3 = 1:10
summary(df2)
```

We can plot attribute data using either `spplot()` or `bubble()` which uses symbol size.
```{r}
bubble(df2, "z1", key.space = "bottom")
```

```{r}
spplot(df2, "z1", col.regions = rainbow(5), 
       key.space = "bottom")
```

# ggplot method

We can use `ggplot()` to make choropleth maps but some work is needed. Let's return to the police expenditure data from counties in Mississippi. Read the data as a spatial polygons data frame.
```{r, eval=FALSE}
download.file("http://myweb.fsu.edu/jelsner/temp/data/police.zip",
                    "police.zip")
unzip("police.zip")
spdf = readOGR(dsn = "police", layer = "police")
```

First add row names as an attribute to the spatial polygons data frame. Here the row names are character strings from "0" to "81"
```{r}
rownames(spdf@data)
spdf$id = rownames(spdf@data)
```

Next, create a data frame of the county boundaries and join them to the attribute data frame. The join is made by `id` with the `inner_join()` function from the **dplyr** package.
```{r}
library(rgeos)
library(ggplot2)
df = as.data.frame(spdf)
fort = fortify(spdf)
line.df = dplyr::inner_join(fort, df, by = "id")
```

Create map. The `line.df` is the first argument and the aesthetics include the x, y coordinates, the group, and fill arguments.
```{r}
ggplot(line.df, 
       aes(x = long, y = lat, group = group, fill = INC)) +
  geom_polygon() +
  coord_equal()
```

The longitudes and latitudes define the polygons, the `group` indicates the order in which the counties are plotted (as a single continuous overlapping, self-intersecting polygon), and the fill is the color. Here the color is based on the value of income (`INC`). 

Note that the names given to the spatial coordinates from the `fortify()` function are longitude and latitude even though they represent projected coordinates with units of kilometers.

Make it better.
```{r}
ggplot(line.df, 
       aes(x = long, y = lat, group = group, 
           fill = INC)) +
  geom_polygon() +
  coord_equal() +
  geom_path(color = "gray75") + 
  theme(panel.grid.minor = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.background = element_blank(),
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
#        labs(title = "Income") + 
        xlab("") + ylab("") +
  scale_fill_gradient2("Median Income [$]") 
```

## Rasters

A raster is a spatial data structure that divides a region into rectangles called 'cells' (or 'pixels') that can store one or more values for each of these cells. 

Such a data structure is also referred to as a 'grid' and is often contrasted with 'vector' data that is used to represent points, lines, and polygons.

The **raster** package developed by Robert Hijmans has functions for creating, reading, manipulating, and writing raster data. This tutorial is based on the package vignette.
```{r}
library(raster)
```

Functions in **raster** depend on functions in **sp**. The package implements raster algebra and most functions for raster data manipulation that are common in Geographic Information Systems (GIS). These functions are similar to those in GIS programs such as Idrisi, the raster functions of GRASS, and the 'grid' module of ArcInfo ('workstation'). It has the ability to work with raster data from a file without importing the file to R. 

### Classes

The package is built with S4 classes with `RasterLayer`, `RasterBrick`, and `RasterStack` classes the most important. A `RasterLayer` object represents single-layer (variable) raster data. A `RasterLayer` object stores the number of columns and rows, the coordinates of its spatial extent ('bounding box'), and the coordinate reference system (the 'map projection').

In addition, a `RasterLayer` can store information about the file in which the raster cell values are stored (if there is such a file). A `RasterLayer` can also hold the raster cell values in memory.

It is common to analyze raster data using single-layer objects. However, in many cases multi-variable raster data sets are used. The **raster** package has two classes for multi-layer data the `RasterStack` and the `RasterBrick`. 

The main difference is that a `RasterBrick` can only be linked to a single (multi-layer) file. In contrast, a `RasterStack` can be formed from separate files and/or from a few layers ('bands') from a single file.

### Creating a raster object

The default settings will create a global raster data structure with a longitude/latitude coordinate reference system and 1 by 1 degree cells. 

We can change these by providing additional arguments such as `xmin`, `nrow`, `ncol`, and/or `crs`, to the function. We can also change these after creating the object. If you set the projection. Note that setting the projection is simply to define it not to change it. To transform a RasterLayer to another coordinate reference system (projection) we can use the function `projectRaster()`.

Let's create a raster.
```{r}
x = raster()
x
```

With other parameters.
```{r}
x = raster(ncol = 36, nrow = 18, 
           xmn = -100, xmx = 0, 
           ymn = 0, ymx = 50)
res(x)
res(x) = 3
x
ncol(x)
ncol(x) = 18
res(x)
```

Define the map projection by setting the coordinate reference system (CRS). 
```{r}
projection(x) = "+proj=utm +zone=48 +datum=WGS84"
x
```

UTM is the Universal Transverse Mercator projection system.

The object `x` is a skeleton (scaffold) raster with no values associated with the pixels. To add values to the raster
```{r}
r = raster(ncol = 10, nrow = 10)
ncell(r)
hasValues(r)
values(r) = runif(ncell(r))
inMemory(r)
plot(r)
projection(r) = "+proj=utm +zone=48 +datum=WGS84"
plot(r)
```


##GRAPHING

Facet_wrap: 
```{r, eval = FALSE}
ggplot(kid.weights, aes(x = height, y = weight)) + 
  geom_point() + 
  facet_grid(~ gender)
```


Make a fully detailed figure: 
```{r}

```


Lesson 7 -- Quantitative Geography for boxplots by month. 























