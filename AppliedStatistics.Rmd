---
title: "Comprehensive Exams -- Applied Statistics"
author: "Zoe Schroder"
date: "9/16/2019"
output: html_document
---

Load libraries: 
```{r}
suppressMessages(library(ggplot2))
suppressMessages(library(dplyr))
suppressMessages(library(reshape2))
suppressMessages(library(ISwR))
```

# Variance:

Variance is the measures of how far a set of numbers are spread out from their average value.
The formula is: 
$$
\hbox{var}(x) = \frac{(x_1 - \bar x)^2 + (x_2 - \bar x)^2 + \cdots + (x_n - \bar x)^2}{n-1}
$$

# Mean: 

The mean is the average calue of a set of numbers defined as: 
$$
\bar x = \frac{x_1 + x_2 + \cdots + x_n}{n}
$$
The sample mean is a measure of the central tendency of a set of values. Typically there are more values near the mean. This is similar with the median, but the median is more resistent to outliers. The mediann is a `resistant statistic` because it is not greatly influenced by only a few values. 

# Quantiles: 

Quantiles cut a set of ordered data into equal-sized data bins. The ordering comes from rearranging the data from lowest to highest. The first, or lower, quartile corresponding to the .25 quantile (25th percentile), indicates that 25% of the data have a value less than this value. The third, or upper, quartile corresponding to the .75 quantile (75th percentile), indicates that 75% of the data have a smaller value than this value.

The difference between the first and third quartile values is called the interquartile range (IQR). Fifty percent of all values lie within the IQR.

```{r}
x <- rnorm(n = 10000, 
           mean = 10, 
           sd = 100)
quantile(x, 
         probs = c(0.05, .25, .5, .75, .95))

IQR(x)
```

Find location of min and max values in a data frame: 
```{r}
Volume = c(20.233, 19.659, 18.597, 18.948, 17.820, 16.736, 16.648, 17.068, 15.916, 14.455, 14.569)
Year = 2002:2012
Ice.df = data.frame(Year, Volume)

which.min(Ice.df$Volume)
which.max(Ice.df$Volume)
```

# Dplyr: 

Verb        | Description
-----------:|:-----------
`select()`    | selects columns; pick variables by their names
`filter()`    | filters rows; pick observations by their values
Try this on your thesis stuff lo
`arrange()`   | re-orders the rows
`mutate()`    | creates new columns; create new variables with functions of existing variables
`summarize()` | summarizes values; collapse many values down to a single summary
`group_by()`  | allows operations to be grouped

For other types of combinations, we need to use Boolean operators: `&` is "and", `|` is "or", and `!` is "not".

```{r}
filter(airquality, Month == 5 | Month == 9)

df = airquality %>%
  filter((Month == 8 | Month == 9) & Wind < 5) %>%
  select(Month, Day, Ozone, Wind, Temp)
```

# Arrange: 

Arrange the data by a specific column
```{r}
# Ascending: 
airquality %>%
  arrange((Solar.R))

# Descending
airquality %>%
  arrange(desc(Solar.R))
#### OR ####
airquality %>%
  arrange(-Solar.R)
```

# Melt: 

Sometimes you need to reshape the data. You use the `melt()` function. The ID variable is a vector of a non-measured quantity. 

```{r}
L = "http://myweb.fsu.edu/jelsner/temp/data/FLprecip.txt"
FLp = read.table(L, na.string = "-9.900", 
                 header = TRUE)

FLpL = melt(FLp, id.vars = "Year")
head(FLpL)
ggplot(FLpL, aes(x = variable, y = value)) + 
  geom_boxplot() +
  xlab("Month") + 
  ylab("Precipitation (in)")
```

# Correlation

A scatter plot is a graphical representation of the correlation. The corrlation describes the linear relationship between two variables witha  single number. The correlation computed on a set of data values a statistic. It describes whether larger- and smaller-than-average values of one variable are related to larger- or smaller-than-average values of the other variable.  If larger than average values of one variable tend to be associated with larger than average values of the other variable then the correlation is positive. These values range betweenn -1 and +1 

Pearson correlation
$$
r = \frac{1}{n-1} \sum \left(\frac{x_i - \bar x}{s_x}\right)\left(\frac{y_i - \bar y}{s_y}\right)
$$
where $n$ is the sample size, $\bar x$ and $\bar y$ are the average of the x and y variables, respectively and $s_x$ and $s_y$ are the standard deviation of the x and y variables.

```{r}
cor(FLp$Jan, FLp$Feb)
```


geom_smooth(method = lm, se = FALSE) + #linear trend line
geom_smooth(se = FALSE, color = "red") #nonlinear trend line

Spearman Rank Correlation: 

Spearman rank correlation is a nonparametric measure of the rank correlation (statistical dependence between the rankings of two variables.) This correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples (Spearman's rho is limited to the value of the rank). High correlation between two variables indicates a similar rank. 

```{r}
cor(FLp$Jan, FLp$Feb, method = "spearman")
```

# One-sample test: 

A one-sample test of whether the mean of a population has a value specified in a null hypothesis. This is a test of location: are the data consistent with being centered on a specific value?

*Example: Given a random sample of FSU student heights (to make things simple, let's assume we are considering only the guys) in feet, test the hypothesis that the mean height of the population of guys at FSU is 183 cm (~ 6 feet).*

Start by plotting the data together with the hypothesis.
```{r}
ht = c(177, 180, 179, 174, 192, 186, 165, 183)
ht.df = data.frame(Student = 1:length(ht), 
                   Height = ht)

library(ggplot2)
ggplot(ht.df, aes(x = "", y = Height)) + 
  geom_boxplot() +
  ylab("Height (cm)") +
  geom_hline(aes(yintercept = 183), color = "red") +
  theme_minimal()
mean(ht)
```

We formally test the hypothesis that the mean height in the population is 183 cm with the `t.test()` function. The first argument is the data (as a vector!) and the second argument is the suggested population mean (`mu =`). Note that the mean is a statistic so we  
```{r}
t.test(ht, mu = 183)

#The output begins with the $t$-value (-1.2239).  The $t$-value is a statistic computed as

(mean(ht) - 183)/(sd(ht)/sqrt(length(ht)))

```

The 'degrees of freedom' (dof, df) is a term that indicates the number of values in the calculation of a statistic with a known value that are 'free' to vary. Suppose you know the mean of a set of numbers (say it's 24) and how many numbers were used to calculate the mean (say there are five). What values could the five numbers have? Four of them could be any value, but the fifth one is constrained to make the mean work out to be 24. This is true for any mean. Thus the mean is a statistic with degrees of freedom equal to n - 1.

# p-values

A $p$-value is an estimate of the probability that your data, or data more extreme than observed, could have occurred by chance IF THE HULL HYPOTHESIS IS TRUE. Said slightly differently this is how unusual your data are ASSUMING THE NULL HYPOTHESIS IS TRUE. A small $p$-value tells you that your data is unusual with respect to this particular hypothesis.

The $p$-value summarizes the evidence in support of the null hypothesis. The smaller the $p$-value, the less evidence there is in support of the null hypothesis. But, the interpretation of the $p$-value is typically stated as evidence AGAINST the null hypothesis:

$p$-value        | Statement of evidence against the null
---------------- | ---------------------
less than  .01   | convincing
.01 - .05        | moderate 
.05 - .15        | suggestive, but inconclusive
greater than .15 | no

The $p$-value is the area under the tails of the $t$ distribution. The distribution describes how the $t$ statistic varies. The $t$ statistic is given by
$$
t = \frac{\bar x - \mu_o}{s/\sqrt{n}}
$$
where $s$ is the standard deviation of the sample values and $n$ is the sample size. The denominator is the standard error (standard deviation divided by square root of sample size) of the mean.

The $p$-value comes from the `pt()` function, which determines the area under the $t$ distribution curve to the left of a particular value. The curve is obtained using the `dt()` function (density function).  

The area under the curve to the left of -1.2239 is 0.13. So 13% of the area lies to the left of the first red line. The distribution is symmetric so 13% of the area lies to the right of the second red line. With a two-sided test you add these two fractions (or multiply by 2) to get the $p$-value.
```{r}
curve(dt(x, 7), from = -3, to = 3, lwd = 2)
abline(v = -1.2239, col = 'red')
abline(v = 1.2239, col = 'red')

#To check the p-value of a t statistic: t-value = -1.2239
pt(-1.2239, df = 7)
#0.13
pt(-1.2239, 7) * 2
#0.26
```

# two-sample test 

With two samples of data the hypothesis is that they both come from distributions having the same mean.

For example, data are from two groups which we assume are sampled from the normal distributions. We test the null hypothesis that the two samples have the same population mean by computing the t statistic. 

In this case, the $t$ statistic is the difference in sample means divided by the standard error of the difference in means (SEDM).

There are two ways to calculate SEDM. (1) Assume equal variance: use the pooled standard deviation (s). Under the null hypothesis, the t statistic will follow a t distribution with n1 + n2 - 2 degrees of freedom (df). (2) Don't assume equal variances (this is the default assumption). Under the null hypothesis, the $t$ statistic approximates a $t$ distribution. In this case it is called the Welch procedure. In this case the df is not an integer.

Usually the two methods give similar results unless group sizes and variances differ widely among the two samples.

Note that in this case the df is a whole number, namely 13 + 9 - 2 = 20.  The $p$-value has dropped slightly and the confidence interval is a little narrower, but the changes are slight.

Start with side-by-side boxplots.
```{r}
ggplot(tlc, aes(x = factor(sex), y = tlc)) +
  geom_boxplot() +
  xlab("Sex (1: female)") +
  ylab("Total lung capacity (liters)") +
  theme_minimal()
```

You see a difference in lung expenditure between the two groups. Is this difference statistically significant?

With this data format (long) we use model syntax for specifying the test. Model syntax is response variable ~ explanatory variable. The `~` is the tilde on your keyboard.

Thus `tlc` is your response variable and `sex` is your explanatory variable.
```{r}
t.test(tlc ~ sex, data = tlc)

# t = -3.6693, df = 29.703, p-value = 0.0009493
# alternative hypothesis: true difference in means is not equal to 0
# 95 percent confidence interval:
# -2.7691965 -0.7883035
# sample estimates:
# mean in group 1 mean in group 2 
#       5.198125        6.976875 
```

The output is a bit different this time. The small $p$-value allows you to conclude that there is convincing evidence against the null hypothesis of no difference. Therefore, they are significantly different. The wider the uncertainty (confidence) interval the more uncertain we are about the population difference. It does not contain the value 0, so we can conclude there is a statistical difference.

# F Test

To test the assumption of equal variances you use `var.test()` function.
```{r}
var.test(energy$expend ~ energy$stature)
```

The variance test is based on the assumption that the groups are independent.  Don't apply this test to paired data. if the variances are equal, the ratio of the variances will be 1.

# Wilcoxon Test aka Mann-Whitney U test

Best to use when the data is not normally distributed. 

We can avoid the distributional assumption by using a non-parametric test. The non-parametric alternative is the Wilcoxon test. Also known as the Mann-Whitney U test.

The test statistic W is the sum of the ranks in the first group minus the sum of the ranks in the second. It is invoked with the `wilcox.test()` function. Try it on the energy data.
```{r}
wilcox.test(energy$expend ~ energy$stature)
```

# Paired observations

Paired tests are used when there are two measurements on the same experimental unit. The theory is based on taking the differences in paired values thus reducing the problem to that of a one-sample test. To invoke the paired $t$ test, use the argument `paired = TRUE`.

Example: Shoe material. A manufacturer makes two different shoe materials (A and B).  A sample of 10 kids try *both* materials and recorded are the wear times (months) for each material. Compare whether or not there is any difference in shoe material wear times.

Only used when it is the same people in the test. The same 10 kids are in both A and B
```{r}
matA = c(14, 8.8, 11.2, 14.2, 11.8, 6.4, 9.8, 11.3, 9.3, 13.6)
matB = c(13.2, 8.2, 10.9, 14.3, 10.7, 6.6, 9.5, 10.8, 8.8, 13.3)
```

```{r}
t.test(matA, matB, paired = TRUE)
```

# Bayesian data analysis

The Bayesian approach to inference is now everywhere. Solving problems like predicting elections and directing self-driving cars. Problems where large uncertainty needs to be quantified and where efficient integration of many sources of information is required.

The problem is that the approach is still not taught in statistics. R is made for Bayesian data analysis. But if you google "Bayesian" you get philosophy. Subjective vs objective, frequentism vs Bayesianism, $p$-values vs subjective probabilities.

`In brief:`

The building blocks of the Bayesian approach are distributions. We've already talked about the Gaussian (normal) distribution as a model for the mean value over a set of measurements. The Gaussian distribution has two parameters $\mu$ and $\sigma$ so we call a Gaussian model a family of Gaussian distributions.

A parametric model is a family of distributions that can be described using a finite (usually small) number of parameters. Bayesian models are made up of parametric models.

Bayesian models are generative. If we know the parameters then we can generate data: Parameters ($\mu$, $\theta$, $\sigma$) --> generative model --> data (5, 3, 4, 0, 1, ...): Monte Carlo simulation. If we know the data then we can estimate the parameters: data (5, 3, 4, 0, 1, ...) --> generative model --> Parameters ($\mu$, $\theta$, $\sigma$): Bayesian data analysis

What Bayesian data analysis is not

* A category of models
* Subjective
* Not the most efficient method of fitting a model
* Anything new

Why use Bayesian data analysis? *(Lesson 10 -- Quant Geo)*

We can build more flexible models. We can use models to answer specific questions in a natural way. 

Bayes Theorem is used to determine a posterior distribution on the unknown number of fish in a lake. It was not determined analytically but rather by repeated sampling. The distribution itself was a vector of values representing possible numbers of fish in the lake.

We can build models to answer natural questions. We don't need to express our questions in terms of a null hypothesis. 

We also have greater flexibility in building models. 

Bayes Theorem. prior x likelihood / posterior

What Bayesian data analysis is not

A category of models (e.g., regression models, decision trees). Rather it is a way of thinking about and constructing models. You can do regression or machine learning with a Bayesian framework (e.g., Bayesian decision analysis).

Not subjective. All statistics require you make assumptions. Results have to be interpreted in light of those assumptions.

Nothing new. Thomas Bayes, Simon LaPlace, Ronald Fischer (first to use the term Bayesian statistics).

# Linear Regression 

Recall that a $t$ test is used to examine whether there is evidence that population means from two distinct groups are statistically different. Linear regression extends the test to an arbitrary number of groups.

The data frame contains total lung capacity (`tlc`) in liters for 32 patients waiting for a heart or lung transplant. It also contains the patients sex (`sex`) coded as 1 for female and 2 for male.We test the null hypothesis of no difference in lung capacity between males and females. 
```{r}
t.test(tlc ~ sex, 
       data = tlc, 
       var.equal = TRUE)
```

The $t$-statistic, computed as the difference in means divided by the standard error of the difference, is -3.6693. With 30 degrees of freedom (sample size = 32), the $p$-value is less than .01 so we reject the null hypothesis of no difference in lung capacities. 

The same test is done with a linear regression model. The function is `lm()`. The syntax is identical to that of the `t.test()` function, except the assumption of equal variance is the default.
```{r}
summary(lm(tlc ~ sex,
           data = tlc))
```
We see that the $p$-value (last line) is identical to that from the `t.test()` and state that we reject the null hypothesis that lung capacity is not significantly related to `sex`. In the table of coefficients we see the value of `1.7788` next to `sex` in the column labeled `Estimate`. This is the difference in average lung capacity grouped by sex. Linear regression generalizes this type of comparison across any number of groups (explicit or not).

The mean value is a model for these data. Some points are above the line and others below. No individual value fits the model precisely (no points are on the line), but the model represents the 'best' guess at what a new value might be.

The closer the points are to the line, the more precise the model is.  Closeness is defined as the distance along the y axis between the point and the line. Distances above the line indicate values that are larger than the mean, so `y - mean(y)` is positive for these values. Distances below the line indicate values that are smaller than the mean, so `y - mean(y)` is negative for these values.
```{r}
y = c(2.9, -2.1, -0.5, 2.9, 4.2)
x = 1:5
df = data.frame(x, y)

p = ggplot(df, aes(x, y)) +
      geom_point(size = 4) +
      geom_hline(yintercept = mean(y)) +
                 scale_y_continuous(limits = c(-6, 6))

p = p + geom_segment(aes(y = mean(y), yend = y, 
                         x = x, xend = x))
p
```
Instead we add the squared distances.
```{r}
sum((y - mean(y))^2)
```

The sum of the squared distances is a measure of how well the model fits the data. The smaller the sum, the better the fit. It is called the sum of the squared errors (SSE). The individual distances are called "errors." Statisticians call them "residuals." The SSE equal to 0 indicates that all points fall exactly on the line.

Keep in mind: A residual is the observed value minus the modeled value.

We see that the regression line (conditional mean) provides a more precise model since the SSE is smaller. Rule: When choosing a regression model from a set of competing models we choose the model that minimizes the SSE.

Recall that a line is uniquely determined by its slope and the value at which it intersects the vertical axis (y-intercept). Mathematically the slope and y-intercept are determined by
$$
\hat \beta_1 = \frac{\sum (x_i-\bar x)(y_i-\bar y)}{\sum (x_i-\bar x)^2}\\
\hat \beta_0 = \bar y - \hat \beta_1 \bar x
$$
`beta1` ($\beta_1$) and `beta0` ($\beta_0$) are called regression model coefficients. $\beta_1$ is the slope coefficient and $\beta_0$ is the y-intercept coefficient.


The notation is `lm(y ~ x)`. The `~` (tilde) in this notation is read "is modeled by" or "is conditional on". So the model formula `y ~ x` is read "y is modeled by x". If we use `lm(y ~ x)` then we say "y is modeled by x" in a linear way.

```{r}
lm(y ~ x)
```

Example: Age is in years and the heart rate is in beats per minute. Heart Rate is response variable. Age is explanatory variable 
```{r}
Age = c(18, 23, 25, 35, 65, 54, 34, 56, 72, 19, 23, 42, 18, 39, 37)
HR = c(202, 186, 187, 180, 156, 169, 174, 172, 153, 199, 193, 174, 198, 183, 178)
heart.df = data.frame(HR = HR, Age = Age)

model <- lm(HR ~ Age, data = heart.df)

# Make predictions for these ages: 
predict(model, newdata = data.frame(Age = 50))
predict(model, newdata = data.frame(Age = seq(40, 60, 10)))
```

We interpret the model as follows: On average a person's maximum HR *decreases* by .8 bpm every year. Or more easily understood as a decrease of 8 bpm every 10 years.

```{r}
summary(model)
```

estimate / std error == t value 
null hypothesis: age does not effect heart rate 
reject the null

The `summary()` method returns quite a bit of output. The first is the code we used to create the model. The second are the summary statistics for the model residuals: observed value minus the modeled value. The most important output is the table of coefficients. The table shows the slope and intercept coefficients in the column labeled `Estimate`. The adjacent column labeled `Std. Error` lists the standard errors on these coefficients. The standard error (or margin of error) is a measure of the uncertainty surrounding the coefficient estimate. For the slope coefficient it is computed as
$$
s_{\beta_1} = \sqrt{ \frac{\frac{1}{n - 2}\sum_{i=1}^n \varepsilon_i^{\,2}} {\sum_{i=1}^n (x_i -\bar{x})^2} },
$$
where
$$
\varepsilon_i  = y_i - \beta_0 - \beta_1 x_i
$$

The best estimate for the (population) slope is -.798 with a margin of error of +/- .07. The uncertainty about the model coefficient is used to test hypotheses and compute confidence intervals. 

Interest typically centers on the null hypothesis that the slope = 0. A zero slope implies the line is horizontal and thus, in this case, that maximum heart rate is independent of how old you are.

The $t$-value ($t$-statistic) for the zero-slope hypothesis is the slope divided by its standard error. The $t$-value has a $t$ distribution with $n-2$ degrees of freedom if the slope is zero.

Here the $t$-value is -11.4 which gives a $p$-value (Pr(>|t|)) of .0000000385 (3.85e-08). It is written this way, because the $p$-value is the probability of observing a more extreme $t$-value (positive or negative) assuming the null hypothesis is true (slope is zero).

The symbols to the right indicate a category of confidence in the inference. The line below the table shows the definitions which we can interpret using our definition. Three asterisks: overwhelming, Two asterisks: convincing, One asterisk: moderate, Point: suggestive but inconclusive.

So we have overwhelming evidence that on average maximum heart rate depends on age given these data.

The next line of output from the `summary()` function gives the residual standard error. This tells us how close (on average) the observations are from the regression line. The degrees of freedom are again $n-2$.
```{r}
sqrt(sum(resid(model)^2)/13)
```

Note the `resid()` [or `residual()`] outputs the residuals from the model object.
```{r}
resid(model)
```

The next line of output gives the multiple R-squared value. Also called the coefficient of determination. And the adjusted R-squared value. The multiple R-squared is equal to the square of the Pearson correlation coefficient. 

The multiple R-squared = 1 - SSE/SSY, where SSE is the sum of the squared residuals (residual sum of squares--RSS) and SSY is the total variation about a constant mean response.

It is useful to see how these are computed. The SSE is the sum of the squared residuals. This is computed with the `deviance()` function.
```{r}
SSE = sum(resid(model)^2)
SSE
deviance(model)
```

To compute the multiple R-squared we need SSY. SSY is the deviance from the simpler constant mean model. This simpler model is estimated using the `lm()` function as
```{r}
model0 = lm(HR ~ 1, data = heart.df)
SSY = deviance(model0)
1 - SSE/SSY
```

One minus the ratio of the explained variation to the total variation.

SSE is less than SSY so SSE/SSY will be less that 1. If SSE is much less than SSY then, SSE/SSY is close to zero so R squared is close to 1.

The R-squared multiplied by 100% is the variance of the response variable explained (statistically) by the explanatory variable.

The adjusted R-squared value is a reduction from the R-squared value. The amount of reduction depends on how many variables are in the model.
$$
1 - \frac{n - 1}{n - p} (1 - R^2 )
$$

where $n$ is the sample size and $p$ is the number of coefficients.

The adjusted R-squared is always smaller than the multiple R-squared, can decrease as new explanatory variables are added, and can even be negative for really poorly fitting models. It is important in the context of multiple regression.

The final line of output is the F-value, degrees of freedom, and associated $p$-value.
$$
F_\hbox{statistic} = \frac{(SSY - SSE)/(p - 1)}{SSE/(n - p)}
$$

Under the null hypothesis that the regression is no better than the unconditional mean as a model for the data, the $F$-statistic comes from an $F$ distribution with ($p-1$) and $n$ degrees of freedom.

```{r}
pf(130, 1, 13, lower.tail = FALSE)
```

The $p$-value is very small so we reject the null hypothesis that the mean model (`lm(HR ~ 1, data = heart.df)`) is better than the linear regression model.

To get the uncertainty intervals on the predicted values:  Use the `level =` and `interval = "confidence"` arguments.
```{r}
predict(model, newdata = data.frame(Age = c(50, 60)), 
        level = .95, interval = "confidence")
ggplot(heart.df, aes(x = Age, y = HR)) +
  geom_point() +
  geom_smooth(method = lm)
```

The lower (`lwr`) and upper (`upr`) bounds represent the 95% uncertainty interval about the location of the line for the particular value of the explanatory variable `Age`.

We state that the best prediction for average maximum heart rate for a set of random 50-yr olds is 170 bpm with a 95% uncertainty interval between 167 and 173 bpm. Thus if we repeat the sampling 100 times and make the same prediction, our CI on the prediction will cover the true predicted value 95 times.

With `interval = "prediction"` we get a 95% *prediction* interval. That interval is wider than the confidence interval as it represents two sources of uncertainty. 
The uncertainty associated with the mean value GIVEN the person's age AND the uncertainty associated with a particular maximum heart rate GIVEN the conditional mean.  
```{r}
predict(model, data.frame(Age = c(50, 60)), 
        level = .95, interval = "prediction")
```




























Facet_wrap: 
```{r}
ggplot(kid.weights, aes(x = height, y = weight)) + 
  geom_point() + 
  facet_grid(~ gender)
```


Make a fully detailed figure: 
```{r}

```


Lesson 7 -- Quantitative Geography for boxplots by month. 







